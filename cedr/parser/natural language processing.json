[{"id":["http://arxiv.org/abs/1608.04434v1"],"updated":["2016-08-15T23:09:21Z"],"published":["2016-08-15T23:09:21Z"],"title":["Natural Language Processing using Hadoop and KOSHIK"],"summary":["  Natural language processing, as a data analytics related technology, is used\nwidely in many research areas such as artificial intelligence, human language\nprocessing, and translation. At present, due to explosive growth of data, there\nare many challenges for natural language processing. Hadoop is one of the\nplatforms that can process the large amount of data required for natural\nlanguage processing. KOSHIK is one of the natural language processing\narchitectures, and utilizes Hadoop and contains language processing components\nsuch as Stanford CoreNLP and OpenNLP. This study describes how to build a\nKOSHIK platform with the relevant tools, and provides the steps to analyze wiki\ndata. Finally, it evaluates and discusses the advantages and disadvantages of\nthe KOSHIK architecture, and gives recommendations on improving the processing\nperformance.\n"],"author":[{"name":["Emre Erturk"]},{"name":["Hong Shi"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1608.04434v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1608.04434v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1906.11608v2"],"updated":["2019-07-26T08:16:08Z"],"published":["2019-06-27T13:15:12Z"],"title":["Simple Natural Language Processing Tools for Danish"],"summary":["  This technical note describes a set of baseline tools for automatic\nprocessing of Danish text. The tools are machine-learning based, using natural\nlanguage processing models trained over previously annotated documents. They\nare maintained at ITU Copenhagen and will always be freely available.\n"],"author":[{"name":["Leon Derczynski"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1906.11608v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1906.11608v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2006.16212v1"],"updated":["2020-06-29T17:24:09Z"],"published":["2020-06-29T17:24:09Z"],"title":["Towards the Study of Morphological Processing of the Tangkhul Language"],"summary":["  There is no or little work on natural language processing of Tangkhul\nlanguage. The current work is a humble beginning of morphological processing of\nthis language using an unsupervised approach. We use a small corpus collected\nfrom different sources of text books, short stories and articles of other\ntopics. Based on the experiments carried out, the morpheme identification task\nusing morphessor gives reasonable and interesting output despite using a small\ncorpus.\n"],"author":[{"name":["Mirinso Shadang"]},{"name":["Navanath Saharia"]},{"name":["Thoudam Doren Singh"]}],"arxiv:comment":[{"_":"In proceeding of Regional International Conference on Natural\n  Language Processing (regICON) 2017, 3rd and 4th November 2017, Imphal, India","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"In proceeding of Regional International Conference on Natural\n  Language Processing (regICON) 2017, 3rd and 4th November 2017, IIIT Senapati,\n  Manipur, India","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2006.16212v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2006.16212v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1511.07916v1"],"updated":["2015-11-24T23:23:13Z"],"published":["2015-11-24T23:23:13Z"],"title":["Natural Language Understanding with Distributed Representation"],"summary":["  This is a lecture note for the course DS-GA 3001 <Natural Language\nUnderstanding with Distributed Representation> at the Center for Data Science ,\nNew York University in Fall, 2015. As the name of the course suggests, this\nlecture note introduces readers to a neural network based approach to natural\nlanguage understanding/processing. In order to make it as self-contained as\npossible, I spend much time on describing basics of machine learning and neural\nnetworks, only after which how they are used for natural languages is\nintroduced. On the language front, I almost solely focus on language modelling\nand machine translation, two of which I personally find most fascinating and\nmost fundamental to natural language understanding.\n"],"author":[{"name":["Kyunghyun Cho"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1511.07916v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1511.07916v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"stat.ML","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1908.08971v2"],"updated":["2019-09-01T02:39:09Z"],"published":["2019-08-23T18:31:35Z"],"title":["Deploying Technology to Save Endangered Languages"],"summary":["  Computer scientists working on natural language processing, native speakers\nof endangered languages, and field linguists to discuss ways to harness\nAutomatic Speech Recognition, especially neural networks, to automate\nannotation, speech tagging, and text parsing on endangered languages.\n"],"author":[{"name":["Hilaria Cruz"]},{"name":["Joseph Waring"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1908.08971v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1908.08971v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1510.00726v1"],"updated":["2015-10-02T20:17:33Z"],"published":["2015-10-02T20:17:33Z"],"title":["A Primer on Neural Network Models for Natural Language Processing"],"summary":["  Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.\n"],"author":[{"name":["Yoav Goldberg"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1510.00726v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1510.00726v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.01705v1"],"updated":["2021-12-03T04:26:49Z"],"published":["2021-12-03T04:26:49Z"],"title":["Multilingual Text Classification for Dravidian Languages"],"summary":["  As the fourth largest language family in the world, the Dravidian languages\nhave become a research hotspot in natural language processing (NLP). Although\nthe Dravidian languages contain a large number of languages, there are\nrelatively few public available resources. Besides, text classification task,\nas a basic task of natural language processing, how to combine it to multiple\nlanguages in the Dravidian languages, is still a major difficulty in Dravidian\nNatural Language Processing. Hence, to address these problems, we proposed a\nmultilingual text classification framework for the Dravidian languages. On the\none hand, the framework used the LaBSE pre-trained model as the base model.\nAiming at the problem of text information bias in multi-task learning, we\npropose to use the MLM strategy to select language-specific words, and used\nadversarial training to perturb them. On the other hand, in view of the problem\nthat the model cannot well recognize and utilize the correlation among\nlanguages, we further proposed a language-specific representation module to\nenrich semantic information for the model. The experimental results\ndemonstrated that the framework we proposed has a significant performance in\nmultilingual text classification tasks with each strategy achieving certain\nimprovements.\n"],"author":[{"name":["Xiaotian Lin"]},{"name":["Nankai Lin"]},{"name":["Kanoksak Wattanachote"]},{"name":["Shengyi Jiang"]},{"name":["Lianxi Wang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.01705v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.01705v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1107.4687v2"],"updated":["2011-10-07T09:50:12Z"],"published":["2011-07-23T12:56:02Z"],"title":["Fence - An Efficient Parser with Ambiguity Support for Model-Driven\n  Language Specification"],"summary":["  Model-based language specification has applications in the implementation of\nlanguage processors, the design of domain-specific languages, model-driven\nsoftware development, data integration, text mining, natural language\nprocessing, and corpus-based induction of models. Model-based language\nspecification decouples language design from language processing and, unlike\ntraditional grammar-driven approaches, which constrain language designers to\nspecific kinds of grammars, it needs general parser generators able to deal\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\nparsing algorithm with lexical and syntactic ambiguity support that enables the\nuse of model-based language specification in practice.\n"],"author":[{"name":["Luis Quesada"]},{"name":["Fernando Berzal"]},{"name":["Francisco J. Cortijo"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1107.4687v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1107.4687v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1612.07486v2"],"updated":["2017-03-19T18:52:15Z"],"published":["2016-12-22T08:29:25Z"],"title":["Continuous multilinguality with language vectors"],"summary":["  Most existing models for multilingual natural language processing (NLP) treat\nlanguage as a discrete category, and make predictions for either one language\nor the other. In contrast, we propose using continuous vector representations\nof language. We show that these can be learned efficiently with a\ncharacter-based neural language model, and used to improve inference about\nlanguage varieties not seen during training. In experiments with 1303 Bible\ntranslations into 990 different languages, we empirically explore the capacity\nof multilingual language models, and also show that the language vectors\ncapture genetic relationships between languages.\n"],"author":[{"name":["Robert Östling"]},{"name":["Jörg Tiedemann"]}],"arxiv:comment":[{"_":"In Proceedings of the 15th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL), Valencia, Spain, April,\n  2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1612.07486v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1612.07486v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2111.09791v1"],"updated":["2021-11-18T16:47:56Z"],"published":["2021-11-18T16:47:56Z"],"title":["Supporting Undotted Arabic with Pre-trained Language Models"],"summary":["  We observe a recent behaviour on social media, in which users intentionally\nremove consonantal dots from Arabic letters, in order to bypass\ncontent-classification algorithms. Content classification is typically done by\nfine-tuning pre-trained language models, which have been recently employed by\nmany natural-language-processing applications. In this work we study the effect\nof applying pre-trained Arabic language models on \"undotted\" Arabic texts. We\nsuggest several ways of supporting undotted texts with pre-trained models,\nwithout additional training, and measure their performance on two Arabic\nnatural-language-processing downstream tasks. The results are encouraging; in\none of the tasks our method shows nearly perfect performance.\n"],"author":[{"name":["Aviad Rom"]},{"name":["Kfir Bar"]}],"arxiv:comment":[{"_":"Paper accepted to 4th International Conference on Natural Language\n  and Speech Processing (ICNLSP 2021)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2111.09791v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2111.09791v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.05222v2"],"updated":["2021-07-22T19:12:46Z"],"published":["2021-05-11T17:37:55Z"],"title":["Including Signed Languages in Natural Language Processing"],"summary":["  Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.\n"],"author":[{"name":["Kayo Yin"]},{"name":["Amit Moryossef"]},{"name":["Julie Hochgesang"]},{"name":["Yoav Goldberg"]},{"name":["Malihe Alikhani"]}],"arxiv:comment":[{"_":"ACL 2021 Best Theme Paper","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.05222v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.05222v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1312.0175v2"],"updated":["2014-08-23T23:15:21Z"],"published":["2013-12-01T03:16:22Z"],"title":["On Even Linear Indexed Languages with a Reduction to the Learning of\n  Context-Free Languages"],"summary":["  This paper presents a restricted form of linear indexed grammars, called even\nlinear indexed grammars, which yield the even linear indexed languages. These\nlanguages properly contain the context-free languages and are contained in the\nset of linear indexed languages. We show that several patterns found in natural\nlanguages are also generated by these grammars, including crossing\ndependencies, copying, and multiple agreements. We discuss the learning problem\nfor even linear indexed languages and show that it is reducible to that of the\ncontext-free languages. The closure properties for this class of languages are\nalso presented.\n"],"author":[{"name":["Benjamin Caulfield"]}],"arxiv:comment":[{"_":"Submitted to Information Processing Letters Certain proofs were not\n  considered rigorous enough for publication","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1312.0175v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1312.0175v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.09712v1"],"updated":["2021-04-20T01:35:16Z"],"published":["2021-04-20T01:35:16Z"],"title":["Problems and Countermeasures in Natural Language Processing Evaluation"],"summary":["  Evaluation in natural language processing guides and promotes research on\nmodels and methods. In recent years, new evalua-tion data sets and evaluation\ntasks have been continuously proposed. At the same time, a series of problems\nexposed by ex-isting evaluation have also restricted the progress of natural\nlanguage processing technology. Starting from the concept, com-position,\ndevelopment and meaning of natural language evaluation, this article classifies\nand summarizes the tasks and char-acteristics of mainstream natural language\nevaluation, and then summarizes the problems and causes of natural language\npro-cessing evaluation. Finally, this article refers to the human language\nability evaluation standard, puts forward the concept of human-like machine\nlanguage ability evaluation, and proposes a series of basic principles and\nimplementation ideas for hu-man-like machine language ability evaluation from\nthe three aspects of reliability, difficulty and validity.\n"],"author":[{"name":["Qingxiu Dong"]},{"name":["Zhifang Sui"]},{"name":["Weidong Zhan"]},{"name":["Baobao Chang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.09712v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.09712v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2202.07138v1"],"updated":["2022-02-15T02:19:09Z"],"published":["2022-02-15T02:19:09Z"],"title":["Integrating AI Planning with Natural Language Processing: A Combination\n  of Explicit and Tacit Knowledge"],"summary":["  Automated planning focuses on strategies, building domain models and\nsynthesizing plans to transit initial states to goals. Natural language\nprocessing concerns with the interactions between agents and human language,\nespecially processing and analyzing large amounts of natural language data.\nThese two fields have abilities to generate explicit knowledge, e.g.,\npreconditions and effects of action models, and learn from tacit knowledge,\ne.g., neural models, respectively. Integrating AI planning and natural language\nprocessing effectively improves the communication between human and intelligent\nagents. This paper outlines the commons and relations between AI planning and\nnatural language processing, argues that each of them can effectively impact on\nthe other one by four areas: (1) planning-based text understanding, (2)\nplanning-based text generation, (3) text-based human-robot interaction, and (4)\ntext-based explainable planning. We also explore some potential future issues\nbetween AI planning and natural language processing.\n"],"author":[{"name":["Kebing Jin"]},{"name":["Hankz Hankui Zhuo"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2202.07138v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2202.07138v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1908.10747v1"],"updated":["2019-08-28T14:29:13Z"],"published":["2019-08-28T14:29:13Z"],"title":["Language Tasks and Language Games: On Methodology in Current Natural\n  Language Processing Research"],"summary":["  \"This paper introduces a new task and a new dataset\", \"we improve the state\nof the art in X by Y\" -- it is rare to find a current natural language\nprocessing paper (or AI paper more generally) that does not contain such\nstatements. What is mostly left implicit, however, is the assumption that this\nnecessarily constitutes progress, and what it constitutes progress towards.\nHere, we make more precise the normally impressionistically used notions of\nlanguage task and language game and ask how a research programme built on these\nmight make progress towards the goal of modelling general language competence.\n"],"author":[{"name":["David Schlangen"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1908.10747v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1908.10747v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1612.03231v1"],"updated":["2016-12-10T00:32:28Z"],"published":["2016-12-10T00:32:28Z"],"title":["A natural language interface to a graph-based bibliographic information\n  retrieval system"],"summary":["  With the ever-increasing scientific literature, there is a need on a natural\nlanguage interface to bibliographic information retrieval systems to retrieve\nrelated information effectively. In this paper, we propose a natural language\ninterface, NLI-GIBIR, to a graph-based bibliographic information retrieval\nsystem. In designing NLI-GIBIR, we developed a novel framework that can be\napplicable to graph-based bibliographic information retrieval systems. Our\nframework integrates algorithms/heuristics for interpreting and analyzing\nnatural language bibliographic queries. NLI-GIBIR allows users to search for a\nvariety of bibliographic data through natural language. A series of text- and\nlinguistic-based techniques are used to analyze and answer natural language\nqueries, including tokenization, named entity recognition, and syntactic\nanalysis. We find that our framework can effectively represents and addresses\ncomplex bibliographic information needs. Thus, the contributions of this paper\nare as follows: First, to our knowledge, it is the first attempt to propose a\nnatural language interface to graph-based bibliographic information retrieval.\nSecond, we propose a novel customized natural language processing framework\nthat integrates a few original algorithms/heuristics for interpreting and\nanalyzing natural language bibliographic queries. Third, we show that the\nproposed framework and natural language interface provide a practical solution\nin building real-world natural language interface-based bibliographic\ninformation retrieval systems. Our experimental results show that the presented\nsystem can correctly answer 39 out of 40 example natural language queries with\nvarying lengths and complexities.\n"],"author":[{"name":["Yongjun Zhu"]},{"name":["Erjia Yan"]},{"name":["Il-Yeol Song"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1612.03231v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1612.03231v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9507009v1"],"updated":["1995-07-21T17:44:05Z"],"published":["1995-07-21T17:44:05Z"],"title":["Specifying Logic Programs in Controlled Natural Language"],"summary":["  Writing specifications for computer programs is not easy since one has to\ntake into account the disparate conceptual worlds of the application domain and\nof software development. To bridge this conceptual gap we propose controlled\nnatural language as a declarative and application-specific specification\nlanguage. Controlled natural language is a subset of natural language that can\nbe accurately and efficiently processed by a computer, but is expressive enough\nto allow natural usage by non-specialists. Specifications in controlled natural\nlanguage are automatically translated into Prolog clauses, hence become formal\nand executable. The translation uses a definite clause grammar (DCG) enhanced\nby feature structures. Inter-text references of the specification, e.g.\nanaphora, are resolved with the help of discourse representation theory (DRT).\nThe generated Prolog clauses are added to a knowledge base. We have implemented\na prototypical specification system that successfully processes the\nspecification of a simple automated teller machine.\n"],"author":[{"name":["Norbert E. Fuchs"],"arxiv:affiliation":[{"_":"Department of Computer Science, University of Zurich","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Rolf Schwitter"],"arxiv:affiliation":[{"_":"Department of Computer Science, University of Zurich","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"16 pages, compressed, uuencoded Postscript, published in Proceedings\n  CLNLP 95, COMPULOGNET/ELSNET/EAGLES Workshop on Computational Logic for\n  Natural Language Processing, Edinburgh, April 3-5, 1995","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings CLNLP 95, COMPULOGNET/ELSNET/EAGLES","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9507009v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9507009v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.00830v1"],"updated":["2021-04-19T06:16:07Z"],"published":["2021-04-19T06:16:07Z"],"title":["Natural Language Generation Using Link Grammar for General\n  Conversational Intelligence"],"summary":["  Many current artificial general intelligence (AGI) and natural language\nprocessing (NLP) architectures do not possess general conversational\nintelligence--that is, they either do not deal with language or are unable to\nconvey knowledge in a form similar to the human language without manual,\nlabor-intensive methods such as template-based customization. In this paper, we\npropose a new technique to automatically generate grammatically valid sentences\nusing the Link Grammar database. This natural language generation method far\noutperforms current state-of-the-art baselines and may serve as the final\ncomponent in a proto-AGI question answering pipeline that understandably\nhandles natural language material.\n"],"author":[{"name":["Vignav Ramesh"]},{"name":["Anton Kolonin"]}],"arxiv:comment":[{"_":"17 pages, 5 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.00830v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.00830v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2108.02170v1"],"updated":["2021-08-04T16:53:43Z"],"published":["2021-08-04T16:53:43Z"],"title":["Curriculum learning for language modeling"],"summary":["  Language Models like ELMo and BERT have provided robust representations of\nnatural language, which serve as the language understanding component for a\ndiverse range of downstream tasks.Curriculum learning is a method that employs\na structured training regime instead, which has been leveraged in computer\nvision and machine translation to improve model training speed and model\nperformance. While language models have proven transformational for the natural\nlanguage processing community, these models have proven expensive,\nenergy-intensive, and challenging to train. In this work, we explore the effect\nof curriculum learning on language model pretraining using various\nlinguistically motivated curricula and evaluate transfer performance on the\nGLUE Benchmark. Despite a broad variety of training methodologies and\nexperiments we do not find compelling evidence that curriculum learning methods\nimprove language model training.\n"],"author":[{"name":["Daniel Campos"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2108.02170v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2108.02170v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1811.03273v1"],"updated":["2018-11-08T05:10:34Z"],"published":["2018-11-08T05:10:34Z"],"title":["Information Flow in Pregroup Models of Natural Language"],"summary":["  This paper is about pregroup models of natural languages, and how they relate\nto the explicitly categorical use of pregroups in Compositional Distributional\nSemantics and Natural Language Processing. These categorical interpretations\nmake certain assumptions about the nature of natural languages that, when\nstated formally, may be seen to impose strong restrictions on pregroup grammars\nfor natural languages.\n  We formalize this as a hypothesis about the form that pregroup models of\nnatural languages must take, and demonstrate by an artificial language example\nthat these restrictions are not imposed by the pregroup axioms themselves. We\ncompare and contrast the artificial language examples with natural languages\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\nan illustrative example).\n  The hypothesis is simply that there must exist a causal connection, or\ninformation flow, between the words of a sentence in a language whose purpose\nis to communicate information. This is not necessarily the case with formal\nlanguages that are simply generated by a series of 'meaning-free' rules. This\nimposes restrictions on the types of pregroup grammars that we expect to find\nin natural languages; we formalize this in algebraic, categorical, and\ngraphical terms.\n  We take some preliminary steps in providing conditions that ensure pregroup\nmodels satisfy these conjectured properties, and discuss the more general forms\nthis hypothesis may take.\n"],"author":[{"name":["Peter M. Hines"],"arxiv:affiliation":[{"_":"University of York","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:doi":[{"_":"10.4204/EPTCS.283.2","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.4204/EPTCS.283.2","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1811.03273v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1811.03273v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"In Proceedings CAPNS 2018, arXiv:1811.02701","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"EPTCS 283, 2018, pp. 13-27","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1006.2835v1"],"updated":["2010-06-14T20:07:32Z"],"published":["2010-06-14T20:07:32Z"],"title":["Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit\n  Grammar"],"summary":["  Indian languages have long history in World Natural languages. Panini was the\nfirst to define Grammar for Sanskrit language with about 4000 rules in fifth\ncentury. These rules contain uncertainty information. It is not possible to\nComputer processing of Sanskrit language with uncertain information. In this\npaper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate\nuncertain information for reasoning with Sanskrit grammar. The Sanskrit\nlanguage processing is also discussed in this paper.\n"],"author":[{"name":["P. Venkata Subba Reddy"]}],"arxiv:comment":[{"_":"Submitted to Journal of Computer Science and Engineering, see\n  http://sites.google.com/site/jcseuk/volume-1-issue-1-may-2010","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Journal of Computer Science and Engineering, Volume 1, Issue 1,\n  p99-101, May 2010","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1006.2835v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1006.2835v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1212.4674v1"],"updated":["2012-12-19T14:40:38Z"],"published":["2012-12-19T14:40:38Z"],"title":["Natural Language Understanding Based on Semantic Relations between\n  Sentences"],"summary":["  In this paper, we define event expression over sentences of natural language\nand semantic relations between events. Based on this definition, we formally\nconsider text understanding process having events as basic unit.\n"],"author":[{"name":["Hyeok Kong"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1212.4674v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1212.4674v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1908.01699v1"],"updated":["2019-08-05T15:45:39Z"],"published":["2019-08-05T15:45:39Z"],"title":["Thoth: Improved Rapid Serial Visual Presentation using Natural Language\n  Processing"],"summary":["  Thoth is a tool designed to combine many different types of speed reading\ntechnology. The largest insight is using natural language parsing for more\noptimal rapid serial visual presentation and more effective reading\ninformation.\n"],"author":[{"name":["David Awad"]}],"arxiv:comment":[{"_":"10 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1908.01699v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1908.01699v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.HC","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2203.10326v2"],"updated":["2022-03-22T06:01:39Z"],"published":["2022-03-19T13:29:48Z"],"title":["Pretraining with Artificial Language: Studying Transferable Knowledge in\n  Language Models"],"summary":["  We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design artificial\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with an artificial language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.\n"],"author":[{"name":["Ryokan Ri"]},{"name":["Yoshimasa Tsuruoka"]}],"arxiv:comment":[{"_":"ACL 2022","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2203.10326v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2203.10326v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1202.4883v3"],"updated":["2012-12-12T12:38:14Z"],"published":["2012-02-22T11:16:47Z"],"title":["The Dissecting Power of Regular Languages"],"summary":["  A recent study on structural properties of regular and context-free languages\nhas greatly promoted our basic understandings of the complex behaviors of those\nlanguages. We continue the study to examine how regular languages behave when\nthey need to cut numerous infinite languages. A particular interest rests on a\nsituation in which a regular language needs to \"dissect\" a given infinite\nlanguage into two subsets of infinite size. Every context-free language is\ndissected by carefully chosen regular languages (or it is REG-dissectible). In\na larger picture, we show that constantly-growing languages and semi-linear\nlanguages are REG-dissectible. Under certain natural conditions, complements\nand finite intersections of semi-linear languages also become REG-dissectible.\nRestricted to bounded languages, the intersections of finitely many\ncontext-free languages and, more surprisingly, the entire Boolean hierarchy\nover bounded context-free languages are REG-dissectible. As an immediate\napplication of the REG-dissectibility, we show another structural property, in\nwhich an appropriate bounded context-free language can \"separate with infinite\nmargins\" two given nested infinite bounded context-free languages.\n"],"author":[{"name":["Tomoyuki Yamakami"]},{"name":["Yuichi Kato"]}],"arxiv:comment":[{"_":"A4, 10pt, 9 pages, 2 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Information Processing Letters, Vol.113, pp.116-122, 2013","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1202.4883v3","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1202.4883v3","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CC","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.14396v1"],"updated":["2021-09-29T12:59:38Z"],"published":["2021-09-29T12:59:38Z"],"title":["StoryDB: Broad Multi-language Narrative Dataset"],"summary":["  This paper presents StoryDB - a broad multi-language dataset of narratives.\nStoryDB is a corpus of texts that includes stories in 42 different languages.\nEvery language includes 500+ stories. Some of the languages include more than\n20 000 stories. Every story is indexed across languages and labeled with tags\nsuch as a genre or a topic. The corpus shows rich topical and language\nvariation and can serve as a resource for the study of the role of narrative in\nnatural language processing across various languages including low resource\nones. We also demonstrate how the dataset could be used to benchmark three\nmodern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.\n"],"author":[{"name":["Alexey Tikhonov"]},{"name":["Igor Samenko"]},{"name":["Ivan P. Yamshchikov"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.14396v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.14396v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2202.03371v1"],"updated":["2022-02-07T17:40:43Z"],"published":["2022-02-07T17:40:43Z"],"title":["Cedille: A large autoregressive French language model"],"summary":["  Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.\n"],"author":[{"name":["Martin Müller"]},{"name":["Florian Laurent"]}],"arxiv:comment":[{"_":"8 pages, 1 figure, 7 tables","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2202.03371v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2202.03371v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"68T50","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1203.3227v1"],"updated":["2012-03-14T22:06:42Z"],"published":["2012-03-14T22:06:42Z"],"title":["Generalisation of language and knowledge models for corpus analysis"],"summary":["  This paper takes new look on language and knowledge modelling for corpus\nlinguistics. Using ideas of Chaitin, a line of argument is made against\nlanguage/knowledge separation in Natural Language Processing. A simplistic\nmodel, that generalises approaches to language and knowledge, is proposed. One\nof hypothetical consequences of this model is Strong AI.\n"],"author":[{"name":["Anton Loss"]}],"arxiv:comment":[{"_":"13 pages, 2 figures, slightly unconventional","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1203.3227v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1203.3227v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2101.11436v1"],"updated":["2021-01-21T08:30:33Z"],"published":["2021-01-21T08:30:33Z"],"title":["Challenges Encountered in Turkish Natural Language Processing Studies"],"summary":["  Natural language processing is a branch of computer science that combines\nartificial intelligence with linguistics. It aims to analyze a language element\nsuch as writing or speaking with software and convert it into information.\nConsidering that each language has its own grammatical rules and vocabulary\ndiversity, the complexity of the studies in this field is somewhat\nunderstandable. For instance, Turkish is a very interesting language in many\nways. Examples of this are agglutinative word structure, consonant/vowel\nharmony, a large number of productive derivational morphemes (practically\ninfinite vocabulary), derivation and syntactic relations, a complex emphasis on\nvocabulary and phonological rules. In this study, the interesting features of\nTurkish in terms of natural language processing are mentioned. In addition,\nsummary info about natural language processing techniques, systems and various\nsources developed for Turkish are given.\n"],"author":[{"name":["Kadir Tohma"]},{"name":["Yakup Kutlu"]}],"arxiv:comment":[{"_":"8 pages, Natural and Engineering Sciences","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Natural and Engineering Sciences, 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2101.11436v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2101.11436v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2004.13645v1"],"updated":["2020-04-28T16:41:00Z"],"published":["2020-04-28T16:41:00Z"],"title":["Unnatural Language Processing: Bridging the Gap Between Synthetic and\n  Natural Language Data"],"summary":["  Large, human-annotated datasets are central to the development of natural\nlanguage processing models. Collecting these datasets can be the most\nchallenging part of the development process. We address this problem by\nintroducing a general purpose technique for ``simulation-to-real'' transfer in\nlanguage understanding problems with a delimited set of target behaviors,\nmaking it possible to develop models that can interpret natural utterances\nwithout natural training data. We begin with a synthetic data generation\nprocedure, and train a model that can accurately interpret utterances produced\nby the data generator. To generalize to natural utterances, we automatically\nfind projections of natural language utterances onto the support of the\nsynthetic language, using learned sentence embeddings to define a distance\nmetric. With only synthetic training data, our approach matches or outperforms\nstate-of-the-art models trained on natural language data in several domains.\nThese results suggest that simulation-to-real transfer is a practical framework\nfor developing NLP applications, and that improved models for transfer might\nprovide wide-ranging improvements in downstream tasks.\n"],"author":[{"name":["Alana Marzoev"]},{"name":["Samuel Madden"]},{"name":["M. Frans Kaashoek"]},{"name":["Michael Cafarella"]},{"name":["Jacob Andreas"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2004.13645v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2004.13645v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1905.05699v1"],"updated":["2019-05-07T21:09:49Z"],"published":["2019-05-07T21:09:49Z"],"title":["Development of Deep Learning Based Natural Language Processing Model for\n  Turkish"],"summary":["  Natural language is one of the most fundamental features that distinguish\npeople from other living things and enable people to communicate each other.\nLanguage is a tool that enables people to express their feelings and thoughts\nand to transfers cultures through generations. Texts and audio are examples of\nnatural language in daily life. In the natural language, many words disappear\nin time, on the other hand new words are derived. Therefore, while the process\nof natural language processing (NLP) is complex even for human, it is difficult\nto process in computer system. The area of linguistics examines how people use\nlanguage. NLP, which requires the collaboration of linguists and computer\nscientists, plays an important role in human computer interaction. Studies in\nNLP have increased with the use of artificial intelligence technologies in the\nfield of linguistics. With the deep learning methods which are one of the\nartificial intelligence study areas, platforms close to natural language are\nbeing developed. Developed platforms for language comprehension, machine\ntranslation and part of speech (POS) tagging benefit from deep learning\nmethods. Recurrent Neural Network (RNN), one of the deep learning\narchitectures, is preferred for processing sequential data such as text or\naudio data. In this study, Turkish POS tagging model has been proposed by using\nBidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed\nPOS tagging model is provided to natural language researchers with a platform\nthat allows them to perform and use their own analysis. In the development\nphase of the platform developed by using BLSTM, the error rate of the POS\ntagger has been reduced by taking feedback with expert opinion.\n"],"author":[{"name":["Baris Baburoglu"]},{"name":["Adem Tekerek"]},{"name":["Mehmet Tekerek"]}],"arxiv:comment":[{"_":"8 pages, in Turkish, 5 figures, ICITS 2019 02-04 May 2019\n  K{\\i}r\\c{s}ehir T\\\"urkiye","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1905.05699v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1905.05699v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1912.00609v1"],"updated":["2019-12-02T07:41:25Z"],"published":["2019-12-02T07:41:25Z"],"title":["GANCoder: An Automatic Natural Language-to-Programming Language\n  Translation Approach based on GAN"],"summary":["  We propose GANCoder, an automatic programming approach based on Generative\nAdversarial Networks (GAN), which can generate the same functional and logical\nprogramming language codes conditioned on the given natural language\nutterances. The adversarial training between generator and discriminator helps\ngenerator learn distribution of dataset and improve code generation quality.\nOur experimental results show that GANCoder can achieve comparable accuracy\nwith the state-of-the-art methods and is more stable when programming\nlanguages.\n"],"author":[{"name":["Yabing Zhu"]},{"name":["Yanfeng Zhang"]},{"name":["Huili Yang"]},{"name":["Fangjing Wang"]}],"arxiv:comment":[{"_":"10pages, 4 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"NLPCC 2019: Natural Language Processing and Chinese Computing","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1912.00609v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1912.00609v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2111.05805v1"],"updated":["2021-11-10T16:53:50Z"],"published":["2021-11-10T16:53:50Z"],"title":["Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language\n  Understanding"],"summary":["  Meta learning with auxiliary languages has demonstrated promising\nimprovements for cross-lingual natural language processing. However, previous\nstudies sample the meta-training and meta-testing data from the same language,\nwhich limits the ability of the model for cross-lingual transfer. In this\npaper, we propose XLA-MAML, which performs direct cross-lingual adaption in the\nmeta-learning stage. We conduct zero-shot and few-shot experiments on Natural\nLanguage Inference and Question Answering. The experimental results demonstrate\nthe effectiveness of our method across different languages, tasks, and\npretrained models. We also give analysis on various cross-lingual specific\nsettings for meta-learning including sampling strategy and parallelism.\n"],"author":[{"name":["Qianying Liu"]},{"name":["Fei Cheng"]},{"name":["Sadao Kurohashi"]}],"arxiv:comment":[{"_":"11 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2111.05805v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2111.05805v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1709.02076v1"],"updated":["2017-09-07T05:39:00Z"],"published":["2017-09-07T05:39:00Z"],"title":["Composition by Conversation"],"summary":["  Most musical programming languages are developed purely for coding virtual\ninstruments or algorithmic compositions. Although there has been some work in\nthe domain of musical query languages for music information retrieval, there\nhas been little attempt to unify the principles of musical programming and\nquery languages with cognitive and natural language processing models that\nwould facilitate the activity of composition by conversation. We present a\nprototype framework, called MusECI, that merges these domains, permitting\nscore-level algorithmic composition in a text editor while also supporting\nconnectivity to existing natural language processing frameworks.\n"],"author":[{"name":["Donya Quick"]},{"name":["Clayton T. Morrison"]}],"arxiv:comment":[{"_":"6 pages, 8 figures, accepted to ICMC 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1709.02076v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1709.02076v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.SD","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.SD","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.PL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"H.5.1; H.5.5; I.2.4; I.2.5; I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cs/0106016v1"],"updated":["2001-06-10T14:56:51Z"],"published":["2001-06-10T14:56:51Z"],"title":["File mapping Rule-based DBMS and Natural Language Processing"],"summary":["  This paper describes the system of storage, extract and processing of\ninformation structured similarly to the natural language. For recursive\ninference the system uses the rules having the same representation, as the\ndata. The environment of storage of information is provided with the File\nMapping (SHM) mechanism of operating system. In the paper the main principles\nof construction of dynamic data structure and language for record of the\ninference rules are stated; the features of available implementation are\nconsidered and the description of the application realizing semantic\ninformation retrieval on the natural language is given.\n"],"author":[{"name":["Vjacheslav M. Novikov"]}],"arxiv:comment":[{"_":"17 pages, 3 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cs/0106016v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cs/0106016v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.DB","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.PL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"D.3.2; H.2.4","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1905.04422v1"],"updated":["2019-05-11T02:02:55Z"],"published":["2019-05-11T02:02:55Z"],"title":["Controlled Natural Languages and Default Reasoning"],"summary":["  Controlled natural languages (CNLs) are effective languages for knowledge\nrepresentation and reasoning. They are designed based on certain natural\nlanguages with restricted lexicon and grammar. CNLs are unambiguous and simple\nas opposed to their base languages. They preserve the expressiveness and\ncoherence of natural languages. In this report, we focus on a class of CNLs,\ncalled machine-oriented CNLs, which have well-defined semantics that can be\ndeterministically translated into formal languages, such as Prolog, to do\nlogical reasoning. Over the past 20 years, a number of machine-oriented CNLs\nemerged and have been used in many application domains for problem solving and\nquestion answering. However, few of them support non-monotonic inference. In\nour work, we propose non-monotonic extensions of CNL to support defeasible\nreasoning.\n  In the first part of this report, we survey CNLs and compare three\ninfluential systems: Attempto Controlled English (ACE), Processable English\n(PENG), and Computer-processable English (CPL). We compare their language\ndesign, semantic interpretations, and reasoning services. In the second part of\nthis report, we first identify typical non-monotonicity in natural languages,\nsuch as defaults, exceptions and conversational implicatures. Then, we propose\ntheir representation in CNL and the corresponding formalizations in a form of\ndefeasible reasoning known as Logic Programming with Defaults and Argumentation\nTheory (LPDA).\n"],"author":[{"name":["Tiantian Gao"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1905.04422v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1905.04422v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1604.03249v1"],"updated":["2016-04-12T05:23:26Z"],"published":["2016-04-12T05:23:26Z"],"title":["Attributes as Semantic Units between Natural Language and Visual\n  Recognition"],"summary":["  Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images.\n"],"author":[{"name":["Marcus Rohrbach"]}],"arxiv:comment":[{"_":"book chapter","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1604.03249v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1604.03249v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CV","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CV","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2007.09774v1"],"updated":["2020-07-19T20:15:55Z"],"published":["2020-07-19T20:15:55Z"],"title":["An Overview of Natural Language State Representation for Reinforcement\n  Learning"],"summary":["  A suitable state representation is a fundamental part of the learning process\nin Reinforcement Learning. In various tasks, the state can either be described\nby natural language or be natural language itself. This survey outlines the\nstrategies used in the literature to build natural language state\nrepresentations. We appeal for more linguistically interpretable and grounded\nrepresentations, careful justification of design decisions and evaluation of\nthe effectiveness of different approaches.\n"],"author":[{"name":["Brielen Madureira"]},{"name":["David Schlangen"]}],"arxiv:comment":[{"_":"Accepted to the ICML 2020 Workshop on Language in Reinforcement\n  Learning (LaReL). 4 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2007.09774v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2007.09774v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1906.09379v1"],"updated":["2019-06-22T03:24:32Z"],"published":["2019-06-22T03:24:32Z"],"title":["Evaluating Computational Language Models with Scaling Properties of\n  Natural Language"],"summary":["  In this article, we evaluate computational models of natural language with\nrespect to the universal statistical behaviors of natural language. Statistical\nmechanical analyses have revealed that natural language text is characterized\nby scaling properties, which quantify the global structure in the vocabulary\npopulation and the long memory of a text. We study whether five scaling\nproperties (given by Zipf's law, Heaps' law, Ebeling's method, Taylor's law,\nand long-range correlation analysis) can serve for evaluation of computational\nmodels. Specifically, we test $n$-gram language models, a probabilistic\ncontext-free grammar (PCFG), language models based on Simon/Pitman-Yor\nprocesses, neural language models, and generative adversarial networks (GANs)\nfor text generation. Our analysis reveals that language models based on\nrecurrent neural networks (RNNs) with a gating mechanism (i.e., long short-term\nmemory, LSTM; a gated recurrent unit, GRU; and quasi-recurrent neural networks,\nQRNNs) are the only computational models that can reproduce the long memory\nbehavior of natural language. Furthermore, through comparison with recently\nproposed model-based evaluation methods, we find that the exponent of Taylor's\nlaw is a good indicator of model quality.\n"],"author":[{"name":["Shuntaro Takahashi"]},{"name":["Kumiko Tanaka-Ishii"]}],"arxiv:comment":[{"_":"32 pages, accepted by Computational Linguistics","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1906.09379v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1906.09379v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9511005v1"],"updated":["1995-11-29T05:19:16Z"],"published":["1995-11-29T05:19:16Z"],"title":["Chart-driven Connectionist Categorial Parsing of Spoken Korean"],"summary":["  While most of the speech and natural language systems which were developed\nfor English and other Indo-European languages neglect the morphological\nprocessing and integrate speech and natural language at the word level, for the\nagglutinative languages such as Korean and Japanese, the morphological\nprocessing plays a major role in the language processing since these languages\nhave very complex morphological phenomena and relatively simple syntactic\nfunctionality. Obviously degenerated morphological processing limits the usable\nvocabulary size for the system and word-level dictionary results in exponential\nexplosion in the number of dictionary entries. For the agglutinative languages,\nwe need sub-word level integration which leaves rooms for general morphological\nprocessing. In this paper, we developed a phoneme-level integration model of\nspeech and linguistic processings through general morphological analysis for\nagglutinative languages and a efficient parsing scheme for that integration.\nKorean is modeled lexically based on the categorial grammar formalism with\nunordered argument and suppressed category extensions, and chart-driven\nconnectionist parsing method is introduced.\n"],"author":[{"name":["WonIl Lee"],"arxiv:affiliation":[{"_":"POSTECH, Korea","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Geunbae Lee"],"arxiv:affiliation":[{"_":"POSTECH, Korea","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Jong-Hyeok Lee"],"arxiv:affiliation":[{"_":"POSTECH, Korea","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"6 pages, Postscript file, Proceedings of ICCPOL'95","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9511005v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9511005v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cs/0205028v1"],"updated":["2002-05-17T12:51:00Z"],"published":["2002-05-17T12:51:00Z"],"title":["NLTK: The Natural Language Toolkit"],"summary":["  NLTK, the Natural Language Toolkit, is a suite of open source program\nmodules, tutorials and problem sets, providing ready-to-use computational\nlinguistics courseware. NLTK covers symbolic and statistical natural language\nprocessing, and is interfaced to annotated corpora. Students augment and\nreplace existing components, learn structured programming by example, and\nmanipulate sophisticated models from the outset.\n"],"author":[{"name":["Edward Loper"]},{"name":["Steven Bird"]}],"arxiv:comment":[{"_":"8 pages, 1 figure, Proceedings of the ACL Workshop on Effective Tools\n  and Methodologies for Teaching Natural Language Processing and Computational\n  Linguistics, Philadelphia, July 2002, Association for Computational\n  Linguistics","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cs/0205028v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cs/0205028v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"D.2.6; I.2.7; J.5; K.3.2","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1907.05403v1"],"updated":["2019-07-11T17:35:20Z"],"published":["2019-07-11T17:35:20Z"],"title":["Incrementalizing RASA's Open-Source Natural Language Understanding\n  Pipeline"],"summary":["  As spoken dialogue systems and chatbots are gaining more widespread adoption,\ncommercial and open-sourced services for natural language understanding are\nemerging. In this paper, we explain how we altered the open-source RASA natural\nlanguage understanding pipeline to process incrementally (i.e., word-by-word),\nfollowing the incremental unit framework proposed by Schlangen and Skantze. To\ndo so, we altered existing RASA components to process incrementally, and added\nan update-incremental intent recognition model as a component to RASA. Our\nevaluations on the Snips dataset show that our changes allow RASA to function\nas an effective incremental natural language understanding service.\n"],"author":[{"name":["Andrew Rafla"]},{"name":["Casey Kennington"]}],"arxiv:comment":[{"_":"5 pages, 1 figure","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1907.05403v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1907.05403v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2108.04674v1"],"updated":["2021-08-10T13:25:29Z"],"published":["2021-08-10T13:25:29Z"],"title":["How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of\n  Recent Resources and Methodologies"],"summary":["  In this paper, we give an overview of commonsense reasoning in natural\nlanguage processing, which requires a deeper understanding of the contexts and\nusually involves inference over implicit external knowledge. We first review\nsome popular commonsense knowledge bases and commonsense reasoning benchmarks,\nbut give more emphasis on the methodologies, including recent approaches that\naim at solving some general natural language problems that take advantage of\nexternal knowledge bases. Finally, we discuss some future directions in pushing\nthe boundary of commonsense reasoning in natural language processing.\n"],"author":[{"name":["Yubo Xie"]},{"name":["Pearl Pu"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2108.04674v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2108.04674v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.07055v1"],"updated":["2021-12-13T22:39:46Z"],"published":["2021-12-13T22:39:46Z"],"title":["Language Models are not Models of Language"],"summary":["  Natural Language Processing (NLP) has become one of the leading application\nareas in the current Artificial Intelligence boom. Transfer learning has\nenabled large deep learning neural networks trained on the language modeling\ntask to vastly improve performance in almost all language tasks. Interestingly,\nwhen the models are trained with data that includes software code, they\ndemonstrate remarkable abilities in generating functioning computer code from\nnatural language specifications. We argue that this creates a conundrum for\nclaims that neural models provide an alternative theory to generative phrase\nstructure grammars in explaining how language works. Since the syntax of\nprogramming languages is determined by phrase structure grammars, successful\nneural models are apparently uninformative about the theoretical foundations of\nprogramming languages, and by extension, natural languages. We argue that the\nterm language model is misleading because deep learning models are not\ntheoretical models of language and propose the adoption of corpus model\ninstead, which better reflects the genesis and contents of the model.\n"],"author":[{"name":["Csaba Veres"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.07055v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.07055v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cs/0208020v1"],"updated":["2002-08-13T03:39:20Z"],"published":["2002-08-13T03:39:20Z"],"title":["Using the DIFF Command for Natural Language Processing"],"summary":["  Diff is a software program that detects differences between two data sets and\nis useful in natural language processing. This paper shows several examples of\nthe application of diff. They include the detection of differences between two\ndifferent datasets, extraction of rewriting rules, merging of two different\ndatasets, and the optimal matching of two different data sets. Since diff comes\nwith any standard UNIX system, it is readily available and very easy to use.\nOur studies showed that diff is a practical tool for research into natural\nlanguage processing.\n"],"author":[{"name":["Masaki Murata"]},{"name":["Hitoshi Isahara"]}],"arxiv:comment":[{"_":"10 pages. Computation and Language. This paper is the rough English\n  translation of our Japanese papar","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cs/0208020v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cs/0208020v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"H.3.3; I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1309.2471v1"],"updated":["2013-09-10T12:03:32Z"],"published":["2013-09-10T12:03:32Z"],"title":["Implementation of nlization framework for verbs, pronouns and\n  determiners with eugene"],"summary":["  UNL system is designed and implemented by a nonprofit organization, UNDL\nFoundation at Geneva in 1999. UNL applications are application softwares that\nallow end users to accomplish natural language tasks, such as translating,\nsummarizing, retrieving or extracting information, etc. Two major web based\napplication softwares are Interactive ANalyzer (IAN), which is a natural\nlanguage analysis system. It represents natural language sentences as semantic\nnetworks in the UNL format. Other application software is dEep-to-sUrface\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\nnatural language sentences out of semantic networks represented in the UNL\nformat. In this paper, NLization framework with EUGENE is focused, while using\nUNL system for accomplishing the task of machine translation. In whole\nNLization process, EUGENE takes a UNL input and delivers an output in natural\nlanguage without any human intervention. It is language-independent and has to\nbe parametrized to the natural language input through a dictionary and a\ngrammar, provided as separate interpretable files. In this paper, it is\nexplained that how UNL input is syntactically and semantically analyzed with\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\nand determiners for Punjabi natural language.\n"],"author":[{"name":["Harinder Singh"]},{"name":["Parteek Kumar"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1309.2471v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1309.2471v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9504008v2"],"updated":["1995-04-25T00:39:36Z"],"published":["1995-04-07T14:39:09Z"],"title":["SKOPE: A connectionist/symbolic architecture of spoken Korean processing"],"summary":["  Spoken language processing requires speech and natural language integration.\nMoreover, spoken Korean calls for unique processing methodology due to its\nlinguistic characteristics. This paper presents SKOPE, a connectionist/symbolic\nspoken Korean processing engine, which emphasizes that: 1) connectionist and\nsymbolic techniques must be selectively applied according to their relative\nstrength and weakness, and 2) the linguistic characteristics of Korean must be\nfully considered for phoneme recognition, speech and language integration, and\nmorphological/syntactic processing. The design and implementation of SKOPE\ndemonstrates how connectionist/symbolic hybrid architectures can be constructed\nfor spoken agglutinative language processing. Also SKOPE presents many novel\nideas for speech and language processing. The phoneme recognition,\nmorphological analysis, and syntactic analysis experiments show that SKOPE is a\nviable approach for the spoken Korean processing.\n"],"author":[{"name":["Geunbae Lee"],"arxiv:affiliation":[{"_":"Department of Computer Science & Engineering and Postech Information Research Laboratory Pohang University of Science & Technology, Hoja-Dong, Pohang, Korea","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Jong-Hyeok Lee"],"arxiv:affiliation":[{"_":"Department of Computer Science & Engineering and Postech Information Research Laboratory Pohang University of Science & Technology, Hoja-Dong, Pohang, Korea","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"8 pages, latex, use aaai.sty & aaai.bst, bibfile: nlpsp.bib, to be\n  presented at IJCAI95 workshops on new approaches to learning for natural\n  language processing","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9504008v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9504008v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1504.04716v1"],"updated":["2015-04-18T13:28:59Z"],"published":["2015-04-18T13:28:59Z"],"title":["Gap Analysis of Natural Language Processing Systems with respect to\n  Linguistic Modality"],"summary":["  Modality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality of\nstate of affairs. It implies different senses and thus has different\nperceptions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In this\npaper, the works and logical approaches employed by Natural Language Processing\nsystems dealing with modality are reviewed. It sees human cognition and\nintelligence as multi-layered approach that can be implemented by intelligent\nsystems for learning. Lastly, current flow of research going on within this\nfield is talked providing futurology.\n"],"author":[{"name":["Vishal Shukla"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1504.04716v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1504.04716v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2203.13344v1"],"updated":["2022-03-24T21:24:54Z"],"published":["2022-03-24T21:24:54Z"],"title":["Linking Emergent and Natural Languages via Corpus Transfer"],"summary":["  The study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational\napproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game\nframework. As a result, it remains unclear how the emergent languages from\nthese settings connect to natural languages or provide benefits in real-world\nlanguage processing tasks, where statistical models trained on large text\ncorpora dominate. In this work, we propose a novel way to establish such a link\nby corpus transfer, i.e. pretraining on a corpus of emergent language for\ndownstream natural language tasks, which is in contrast to prior work that\ndirectly transfers speaker and listener parameters. Our approach showcases\nnon-trivial transfer benefits for two different tasks -- language modeling and\nimage captioning. For example, in a low-resource setup (modeling 2 million\nnatural language tokens), pre-training on an emergent language corpus with just\n2 million tokens reduces model perplexity by $24.6\\%$ on average across ten\nnatural languages. We also introduce a novel metric to predict the\ntransferability of an emergent language by translating emergent messages to\nnatural language captions grounded on the same images. We find that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance $\\rho=0.83$ on Hebrew), while\ntopographic similarity, a popular metric in previous work, shows surprisingly\nlow correlation ($\\rho=0.003$), hinting that simple properties like attribute\ndisentanglement from synthetic domains might not capture the full complexities\nof natural language. Our findings also indicate potential benefits of moving\nlanguage emergence forward with natural language resources and models.\n"],"author":[{"name":["Shunyu Yao"]},{"name":["Mo Yu"]},{"name":["Yang Zhang"]},{"name":["Karthik R Narasimhan"]},{"name":["Joshua B. Tenenbaum"]},{"name":["Chuang Gan"]}],"arxiv:comment":[{"_":"ICLR 2022 Spotlight. Github repo: https://github.com/ysymyth/ec-nl","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2203.13344v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2203.13344v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2006.02633v1"],"updated":["2020-06-04T03:52:59Z"],"published":["2020-06-04T03:52:59Z"],"title":["Stopwords in Technical Language Processing"],"summary":["  There are increasingly applications of natural language processing techniques\nfor information retrieval, indexing and topic modelling in the engineering\ncontexts. A standard component of such tasks is the removal of stopwords, which\nare uninformative components of the data. While researchers use readily\navailable stopword lists which are derived for general English language, the\ntechnical jargon of engineering fields contains their own highly frequent and\nuninformative words and there exists no standard stopword list for technical\nlanguage processing applications. Here we address this gap by rigorously\nidentifying generic, insignificant, uninformative stopwords in engineering\ntexts beyond the stopwords in general texts, based on the synthesis of\nalternative data-driven approaches, and curating a stopword list ready for\ntechnical language processing applications.\n"],"author":[{"name":["Serhad Sarica"]},{"name":["Jianxi Luo"]}],"arxiv:doi":[{"_":"10.1371/journal.pone.0254937","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1371/journal.pone.0254937","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2006.02633v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2006.02633v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1406.3460v1"],"updated":["2014-06-13T09:23:53Z"],"published":["2014-06-13T09:23:53Z"],"title":["Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG"],"summary":["  Controlled natural languages for industrial application are often regarded as\na response to the challenges of translation and multilingual communication.\nThis paper presents a quite different approach taken by Koenig & Bauer AG,\nwhere the main goal was the improvement of the authoring process for technical\ndocumentation. Most importantly, this paper explores the notion of a controlled\nlanguage and demonstrates how style guides can emerge from non-linguistic\nconsiderations. Moreover, it shows the transition from loose language\nrecommendations into precise and prescriptive rules and investigates whether\nsuch rules can be regarded as a full-fledged controlled language.\n"],"author":[{"name":["Karolina Suchowolec"]}],"arxiv:comment":[{"_":"Fourth Workshop on Controlled Natural Language (CNL 2014)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1406.3460v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1406.3460v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1406.4057v1"],"updated":["2014-06-16T16:11:32Z"],"published":["2014-06-16T16:11:32Z"],"title":["Embedded Controlled Languages"],"summary":["  Inspired by embedded programming languages, an embedded CNL (controlled\nnatural language) is a proper fragment of an entire natural language (its host\nlanguage), but it has a parser that recognizes the entire host language. This\nmakes it possible to process out-of-CNL input and give useful feedback to\nusers, instead of just reporting syntax errors. This extended abstract explains\nthe main concepts of embedded CNL implementation in GF (Grammatical Framework),\nwith examples from machine translation and some other ongoing work.\n"],"author":[{"name":["Aarne Ranta"]}],"arxiv:comment":[{"_":"7 pages, extended abstract, preprint for CNL 2014 in Galway","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1406.4057v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1406.4057v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2010.01063v1"],"updated":["2020-10-02T15:44:58Z"],"published":["2020-10-02T15:44:58Z"],"title":["Syntax Representation in Word Embeddings and Neural Networks -- A Survey"],"summary":["  Neural networks trained on natural language processing tasks capture syntax\neven though it is not provided as a supervision signal. This indicates that\nsyntactic analysis is essential to the understating of language in artificial\nintelligence systems. This overview paper covers approaches of evaluating the\namount of syntactic information included in the representations of words for\ndifferent neural network architectures. We mainly summarize re-search on\nEnglish monolingual data on language modeling tasks and multilingual data for\nneural machine translation systems and multilingual language models. We\ndescribe which pre-trained models and representations of language are best\nsuited for transfer to syntactic tasks.\n"],"author":[{"name":["Tomasz Limisiewicz"]},{"name":["David Mareček"]}],"arxiv:journal_ref":[{"_":"Proceedings of the 20th Conference ITAT 2020: Automata, Formal and\n  Natural Languages Workshop","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2010.01063v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2010.01063v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2008.01548v1"],"updated":["2020-07-28T04:11:10Z"],"published":["2020-07-28T04:11:10Z"],"title":["Defining and Evaluating Fair Natural Language Generation"],"summary":["  Our work focuses on the biases that emerge in the natural language generation\n(NLG) task of sentence completion. In this paper, we introduce a framework of\nfairness for NLG followed by an evaluation of gender biases in two\nstate-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.\n"],"author":[{"name":["Catherine Yeo"]},{"name":["Alyssa Chen"]}],"arxiv:comment":[{"_":"7 pages, 2 figures, to be published in Proceedings of the The Fourth\n  Widening Natural Language Processing Workshop at ACL","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2008.01548v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2008.01548v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1108.3848v1"],"updated":["2011-08-18T20:12:50Z"],"published":["2011-08-18T20:12:50Z"],"title":["Language understanding as a step towards human level intelligence -\n  automatizing the construction of the initial dictionary from example\n  sentences"],"summary":["  For a system to understand natural language, it needs to be able to take\nnatural language text and answer questions given in natural language with\nrespect to that text; it also needs to be able to follow instructions given in\nnatural language. To achieve this, a system must be able to process natural\nlanguage and be able to capture the knowledge within that text. Thus it needs\nto be able to translate natural language text into a formal language. We\ndiscuss our approach to do this, where the translation is achieved by composing\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\nmethod that we developed (and other methods) to learn meaning of words from\nmeaning of sentences and an initial lexicon. We then present an improved method\nwhere the initial lexicon is also learned by analyzing the training sentence\nand meaning pairs. We evaluate our methods and compare them with other existing\nmethods on a corpora of database querying and robot command and control.\n"],"author":[{"name":["Chitta Baral"]},{"name":["Juraj Dzifcak"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1108.3848v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1108.3848v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/0908.4431v1"],"updated":["2009-08-30T23:20:41Z"],"published":["2009-08-30T23:20:41Z"],"title":["An OLAC Extension for Dravidian Languages"],"summary":["  OLAC was founded in 2000 for creating online databases of language resources.\nThis paper intends to review the bottom-up distributed character of the project\nand proposes an extension of the architecture for Dravidian languages. An\nontological structure is considered for effective natural language processing\n(NLP) and its advantages over statistical methods are reviewed\n"],"author":[{"name":["B Prabhulla Chandran Pillai"]}],"arxiv:comment":[{"_":"4 Pages, 2 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/0908.4431v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/0908.4431v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1310.1425v1"],"updated":["2013-10-05T00:33:46Z"],"published":["2013-10-05T00:33:46Z"],"title":["A State of the Art of Word Sense Induction: A Way Towards Word Sense\n  Disambiguation for Under-Resourced Languages"],"summary":["  Word Sense Disambiguation (WSD), the process of automatically identifying the\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\npromising developments in the field of NLP and its applications. Indeed,\nimprovement over current performance levels could allow us to take a first step\ntowards natural language understanding. Due to the lack of lexical resources it\nis sometimes difficult to perform WSD for under-resourced languages. This paper\nis an investigation on how to initiate research in WSD for under-resourced\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\ntopics to focus on.\n"],"author":[{"name":["Mohammad Nasiruddin"]}],"arxiv:comment":[{"_":"14 pages TALN/RECITAL 2013","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1310.1425v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1310.1425v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"68T50","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1707.00061v1"],"updated":["2017-06-30T22:57:50Z"],"published":["2017-06-30T22:57:50Z"],"title":["Racial Disparity in Natural Language Processing: A Case Study of Social\n  Media African-American English"],"summary":["  We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP.\n"],"author":[{"name":["Su Lin Blodgett"]},{"name":["Brendan O'Connor"]}],"arxiv:comment":[{"_":"Presented as a talk at the 2017 Workshop on Fairness, Accountability,\n  and Transparency in Machine Learning (FAT/ML 2017)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1707.00061v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1707.00061v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.10535v1"],"updated":["2021-02-21T07:21:26Z"],"published":["2021-02-21T07:21:26Z"],"title":["Automatic Code Generation using Pre-Trained Language Models"],"summary":["  Recent advancements in natural language processing \\cite{gpt2} \\cite{BERT}\nhave led to near-human performance in multiple natural language tasks. In this\npaper, we seek to understand whether similar techniques can be applied to a\nhighly structured environment with strict syntax rules. Specifically, we\npropose an end-to-end machine learning model for code generation in the Python\nlanguage built on-top of pre-trained language models. We demonstrate that a\nfine-tuned model can perform well in code generation tasks, achieving a BLEU\nscore of 0.22, an improvement of 46\\% over a reasonable sequence-to-sequence\nbaseline. All results and related code used for training and data processing\nare available on GitHub.\n"],"author":[{"name":["Luis Perez"]},{"name":["Lizi Ottens"]},{"name":["Sudharshan Viswanathan"]}],"arxiv:comment":[{"_":"9 pages, 11 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2102.10535v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.10535v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1604.08781v2"],"updated":["2016-06-28T05:29:24Z"],"published":["2016-04-29T11:36:25Z"],"title":["Teaching natural language to computers"],"summary":["  \"Natural Language,\" whether spoken and attended to by humans, or processed\nand generated by computers, requires networked structures that reflect creative\nprocesses in semantic, syntactic, phonetic, linguistic, social, emotional, and\ncultural modules. Being able to produce novel and useful behavior following\nrepeated practice gets to the root of both artificial intelligence and human\nlanguage. This paper investigates the modalities involved in language-like\napplications that computers -- and programmers -- engage with, and aims to fine\ntune the questions we ask to better account for context, self-awareness, and\nembodiment.\n"],"author":[{"name":["Joseph Corneli"]},{"name":["Miriam Corneli"]}],"arxiv:comment":[{"_":"6 pages, including 1 figure and 3 tables; accepted for presentation\n  at IJCAI2016 Workshop on Language Sense on Computers","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1604.08781v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1604.08781v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"H.5.2; D.1.2; J.5","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1510.01717v1"],"updated":["2015-10-06T19:35:23Z"],"published":["2015-10-06T19:35:23Z"],"title":["Language Segmentation"],"summary":["  Language segmentation consists in finding the boundaries where one language\nends and another language begins in a text written in more than one language.\nThis is important for all natural language processing tasks. The problem can be\nsolved by training language models on language data. However, in the case of\nlow- or no-resource languages, this is problematic. I therefore investigate\nwhether unsupervised methods perform better than supervised methods when it is\ndifficult or impossible to train supervised approaches. A special focus is\ngiven to difficult texts, i.e. texts that are rather short (one sentence),\ncontaining abbreviations, low-resource languages and non-standard language. I\ncompare three approaches: supervised n-gram language models, unsupervised\nclustering and weakly supervised n-gram language model induction. I devised the\nweakly supervised approach in order to deal with difficult text specifically.\nIn order to test the approach, I compiled a small corpus of different text\ntypes, ranging from one-sentence texts to texts of about 300 words. The weakly\nsupervised language model induction approach works well on short and difficult\ntexts, outperforming the clustering algorithm and reaching scores in the\nvicinity of the supervised approach. The results look promising, but there is\nroom for improvement and a more thorough investigation should be undertaken.\n"],"author":[{"name":["David Alfter"]}],"arxiv:comment":[{"_":"Master Thesis","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1510.01717v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1510.01717v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1311.3175v1"],"updated":["2013-11-13T15:36:12Z"],"published":["2013-11-13T15:36:12Z"],"title":["Architecture of an Ontology-Based Domain-Specific Natural Language\n  Question Answering System"],"summary":["  Question answering (QA) system aims at retrieving precise information from a\nlarge collection of documents against a query. This paper describes the\narchitecture of a Natural Language Question Answering (NLQA) system for a\nspecific domain based on the ontological information, a step towards semantic\nweb question answering. The proposed architecture defines four basic modules\nsuitable for enhancing current QA capabilities with the ability of processing\ncomplex questions. The first module was the question processing, which analyses\nand classifies the question and also reformulates the user query. The second\nmodule allows the process of retrieving the relevant documents. The next module\nprocesses the retrieved documents, and the last module performs the extraction\nand generation of a response. Natural language processing techniques are used\nfor processing the question and documents and also for answer extraction.\nOntology and domain knowledge are used for reformulating queries and\nidentifying the relations. The aim of the system is to generate short and\nspecific answer to the question that is asked in the natural language in a\nspecific domain. We have achieved 94 % accuracy of natural language question\nanswering in our implementation.\n"],"author":[{"name":["Athira P. M."]},{"name":["Sreeja M."]},{"name":["P. C. Reghu Raj"]}],"arxiv:journal_ref":[{"_":"International Journal of Web & Semantic Technology (IJWesT) Vol.4,\n  No.4, October 2013","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1311.3175v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1311.3175v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1806.05480v1"],"updated":["2018-06-14T11:38:24Z"],"published":["2018-06-14T11:38:24Z"],"title":["Automatic Language Identification for Romance Languages using Stop Words\n  and Diacritics"],"summary":["  Automatic language identification is a natural language processing problem\nthat tries to determine the natural language of a given content. In this paper\nwe present a statistical method for automatic language identification of\nwritten text using dictionaries containing stop words and diacritics. We\npropose different approaches that combine the two dictionaries to accurately\ndetermine the language of textual corpora. This method was chosen because stop\nwords and diacritics are very specific to a language, although some languages\nhave some similar words and special characters they are not all common. The\nlanguages taken into account were romance languages because they are very\nsimilar and usually it is hard to distinguish between them from a computational\npoint of view. We have tested our method using a Twitter corpus and a news\narticle corpus. Both corpora consists of UTF-8 encoded text, so the diacritics\ncould be taken into account, in the case that the text has no diacritics only\nthe stop words are used to determine the language of the text. The experimental\nresults show that the proposed method has an accuracy of over 90% for small\ntexts and over 99.8% for\n"],"author":[{"name":["Ciprian-Octavian Truică"]},{"name":["Julien Velcin"]},{"name":["Alexandru Boicea"]}],"arxiv:doi":[{"_":"10.1109/SYNASC.2015.45","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1109/SYNASC.2015.45","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1806.05480v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1806.05480v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9504005v1"],"updated":["1995-04-05T14:16:33Z"],"published":["1995-04-05T14:16:33Z"],"title":["Constraint Logic Programming for Natural Language Processing"],"summary":["  This paper proposes an evaluation of the adequacy of the constraint logic\nprogramming paradigm for natural language processing. Theoretical aspects of\nthis question have been discussed in several works. We adopt here a pragmatic\npoint of view and our argumentation relies on concrete solutions. Using actual\ncontraints (in the CLP sense) is neither easy nor direct. However, CLP can\nimprove parsing techniques in several aspects such as concision, control,\nefficiency or direct representation of linguistic formalism. This discussion is\nillustrated by several examples and the presentation of an HPSG parser.\n"],"author":[{"name":["Philippe Blache"],"arxiv:affiliation":[{"_":"2LC-CNRS","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Nabil Hathout"],"arxiv:affiliation":[{"_":"INaLF-CNRS","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"15 pages, uuencoded and compressed postscript to appear in\n  Proceedings of the 5th Int. Workshop on Natural Language Understanding and\n  Logic Programming. Lisbon, Portugal. 1995","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9504005v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9504005v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1408.0016v1"],"updated":["2014-06-27T01:00:59Z"],"published":["2014-06-27T01:00:59Z"],"title":["Architecture of a Web-based Predictive Editor for Controlled Natural\n  Language Processing"],"summary":["  In this paper, we describe the architecture of a web-based predictive text\neditor being developed for the controlled natural language PENG$^{ASP)$. This\ncontrolled language can be used to write non-monotonic specifications that have\nthe same expressive power as Answer Set Programs. In order to support the\nwriting process of these specifications, the predictive text editor\ncommunicates asynchronously with the controlled natural language processor that\ngenerates lookahead categories and additional auxiliary information for the\nauthor of a specification text. The text editor can display multiple sets of\nlookahead categories simultaneously for different possible sentence\ncompletions, anaphoric expressions, and supports the addition of new content\nwords to the lexicon.\n"],"author":[{"name":["Stephen Guy"]},{"name":["Rolf Schwitter"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1408.0016v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1408.0016v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1708.05148v1"],"updated":["2017-08-17T06:42:03Z"],"published":["2017-08-17T06:42:03Z"],"title":["Natural Language Processing: State of The Art, Current Trends and\n  Challenges"],"summary":["  Natural language processing (NLP) has recently gained much attention for\nrepresenting and analysing human language computationally. It has spread its\napplications in various fields such as machine translation, email spam\ndetection, information extraction, summarization, medical, and question\nanswering etc. The paper distinguishes four phases by discussing different\nlevels of NLP and components of Natural Language Generation (NLG) followed by\npresenting the history and evolution of NLP, state of the art presenting the\nvarious applications of NLP and current trends and challenges.\n"],"author":[{"name":["Diksha Khurana"]},{"name":["Aditya Koli"]},{"name":["Kiran Khatter"]},{"name":["Sukhdev Singh"]}],"arxiv:comment":[{"_":"25 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1708.05148v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1708.05148v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1412.1342v1"],"updated":["2014-12-03T14:37:36Z"],"published":["2014-12-03T14:37:36Z"],"title":["A perspective on the advancement of natural language processing tasks\n  via topological analysis of complex networks"],"summary":["  Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).\n"],"author":[{"name":["Diego R. Amancio"]}],"arxiv:doi":[{"_":"10.1016/j.plrev.2014.07.010","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1016/j.plrev.2014.07.010","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1412.1342v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1412.1342v1","rel":"related","type":"application/pdf"}}],"arxiv:journal_ref":[{"_":"Physics of Life Reviews, v. 11, p. 641-643, 2014","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1605.04122v1"],"updated":["2016-05-13T10:46:22Z"],"published":["2016-05-13T10:46:22Z"],"title":["Natural Language Semantics and Computability"],"summary":["  This paper is a reflexion on the computability of natural language semantics.\nIt does not contain a new model or new results in the formal semantics of\nnatural language: it is rather a computational analysis of the logical models\nand algorithms currently used in natural language semantics, defined as the\nmapping of a statement to logical formulas - formulas, because a statement can\nbe ambiguous. We argue that as long as possible world semantics is left out,\none can compute the semantic representation(s) of a given statement, including\naspects of lexical meaning. We also discuss the algorithmic complexity of this\nprocess.\n"],"author":[{"name":["Richard Moot"],"arxiv:affiliation":[{"_":"LaBRI","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Christian Retoré"],"arxiv:affiliation":[{"_":"TEXTE","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"link":[{"$":{"href":"http://arxiv.org/abs/1605.04122v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1605.04122v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CC","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2008.07138v1"],"updated":["2020-08-17T08:04:11Z"],"published":["2020-08-17T08:04:11Z"],"title":["Logical Semantics, Dialogical Argumentation, and Textual Entailment"],"summary":["  In this chapter, we introduce a new dialogical system for first order\nclassical logic which is close to natural language argumentation, and we prove\nits completeness with respect to usual classical validity. We combine our\ndialogical system with the Grail syntactic and semantic parser developed by the\nsecond author in order to address automated textual entailment, that is, we use\nit for deciding whether or not a sentence is a consequence of a short text.\nThis work-which connects natural language semantics and argumentation with\ndialogical logic-can be viewed as a step towards an inferentialist view of\nnatural language semantics.\n"],"author":[{"name":["Davide Catta"],"arxiv:affiliation":[{"_":"TEXTE","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Richard Moot"],"arxiv:affiliation":[{"_":"TEXTE, LIRMM, CNRS","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Christian Retoré"],"arxiv:affiliation":[{"_":"LaBRI","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"Natural Language Processing in Artificial Intelligence, In press","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2008.07138v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2008.07138v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2010.12433v1"],"updated":["2020-10-23T14:26:30Z"],"published":["2020-10-23T14:26:30Z"],"title":["Natural Language Processing Chains Inside a Cross-lingual Event-Centric\n  Knowledge Pipeline for European Union Under-resourced Languages"],"summary":["  This article presents the strategy for developing a platform containing\nLanguage Processing Chains for European Union languages, consisting of\nTokenization to Parsing, also including Named Entity recognition andwith\naddition ofSentiment Analysis. These chains are part of the first step of an\nevent-centric knowledge processing pipeline whose aim is to process\nmultilingual media information about major events that can cause an impactin\nEurope and the rest of the world. Due to the differences in terms of\navailability of language resources for each language, we have built this\nstrategy in three steps, starting with processing chains for the well-resourced\nlanguages and finishing with the development of new modules for the\nunder-resourced ones. In order to classify all European Union official\nlanguages in terms of resources, we have analysed the size of annotated corpora\nas well as the existence of pre-trained models in mainstream Language\nProcessing tools, and we have combined this information with the proposed\nclassification published at META-NETwhitepaper series.\n"],"author":[{"name":["Diego Alves"]},{"name":["Gaurish Thakkar"]},{"name":["Marko Tadić"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2010.12433v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2010.12433v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2009.14384v1"],"updated":["2020-09-30T01:52:00Z"],"published":["2020-09-30T01:52:00Z"],"title":["Development of Word Embeddings for Uzbek Language"],"summary":["  In this paper, we share the process of developing word embeddings for the\nCyrillic variant of the Uzbek language. The result of our work is the first\npublicly available set of word vectors trained on the word2vec, GloVe, and\nfastText algorithms using a high-quality web crawl corpus developed in-house.\nThe developed word embeddings can be used in many natural language processing\ndownstream tasks.\n"],"author":[{"name":["B. Mansurov"]},{"name":["A. Mansurov"]}],"arxiv:comment":[{"_":"7 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2009.14384v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2009.14384v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2009.11832v1"],"updated":["2020-09-24T17:28:59Z"],"published":["2020-09-24T17:28:59Z"],"title":["Novel Keyword Extraction and Language Detection Approaches"],"summary":["  Fuzzy string matching and language classification are important tools in\nNatural Language Processing pipelines, this paper provides advances in both\nareas. We propose a fast novel approach to string tokenisation for fuzzy\nlanguage matching and experimentally demonstrate an 83.6% decrease in\nprocessing time with an estimated improvement in recall of 3.1% at the cost of\na 2.6% decrease in precision. This approach is able to work even where keywords\nare subdivided into multiple words, without needing to scan\ncharacter-to-character. So far there has been little work considering using\nmetadata to enhance language classification algorithms. We provide\nobservational data and find the Accept-Language header is 14% more likely to\nmatch the classification than the IP Address.\n"],"author":[{"name":["Malgorzata Pikies"]},{"name":["Andronicus Riyono"]},{"name":["Junade Ali"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2009.11832v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2009.11832v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2011.05402v1"],"updated":["2020-11-10T21:21:08Z"],"published":["2020-11-10T21:21:08Z"],"title":["OCR Post Correction for Endangered Language Texts"],"summary":["  There is little to no data available to build natural language processing\nmodels for most endangered languages. However, textual data in these languages\noften exists in formats that are not machine-readable, such as paper books and\nscanned images. In this work, we address the task of extracting text from these\nresources. We create a benchmark dataset of transcriptions for scanned books in\nthree critically endangered languages and present a systematic analysis of how\ngeneral-purpose OCR tools are not robust to the data-scarce setting of\nendangered languages. We develop an OCR post-correction method tailored to ease\ntraining in this data-scarce setting, reducing the recognition error rate by\n34% on average across the three languages.\n"],"author":[{"name":["Shruti Rijhwani"]},{"name":["Antonios Anastasopoulos"]},{"name":["Graham Neubig"]}],"arxiv:comment":[{"_":"Accepted to EMNLP 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2011.05402v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2011.05402v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2202.00470v1"],"updated":["2022-01-26T21:56:14Z"],"published":["2022-01-26T21:56:14Z"],"title":["An Assessment of the Impact of OCR Noise on Language Models"],"summary":["  Neural language models are the backbone of modern-day natural language\nprocessing applications. Their use on textual heritage collections which have\nundergone Optical Character Recognition (OCR) is therefore also increasing.\nNevertheless, our understanding of the impact OCR noise could have on language\nmodels is still limited. We perform an assessment of the impact OCR noise has\non a variety of language models, using data in Dutch, English, French and\nGerman. We find that OCR noise poses a significant obstacle to language\nmodelling, with language models increasingly diverging from their noiseless\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\nthis respect.\n"],"author":[{"name":["Konstantin Todorov"]},{"name":["Giovanni Colavizza"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2202.00470v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2202.00470v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/0912.1820v1"],"updated":["2009-12-09T18:03:20Z"],"published":["2009-12-09T18:03:20Z"],"title":["Parsing of part-of-speech tagged Assamese Texts"],"summary":["  A natural language (or ordinary language) is a language that is spoken,\nwritten, or signed by humans for general-purpose communication, as\ndistinguished from formal languages (such as computer-programming languages or\nthe \"languages\" used in the study of formal logic). The computational\nactivities required for enabling a computer to carry out information processing\nusing natural language is called natural language processing. We have taken\nAssamese language to check the grammars of the input sentence. Our aim is to\nproduce a technique to check the grammatical structures of the sentences in\nAssamese text. We have made grammar rules by analyzing the structures of\nAssamese sentences. Our parsing program finds the grammatical errors, if any,\nin the Assamese sentence. If there is no error, the program will generate the\nparse tree for the Assamese sentence\n"],"author":[{"name":["Mirzanur Rahman"]},{"name":["Sufal Das"]},{"name":["Utpal Sharma"]}],"arxiv:comment":[{"_":"International Journal of Computer Science Issues, IJCSI Volume 6,\n  Issue 1, pp28-34, November 2009","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"M. Rahman, S. Das and U. Sharma, \"Parsing of part-of-speech tagged\n  Assamese Texts\", International Journal of Computer Science Issues, IJCSI,\n  Volume 6, Issue 1, pp28-34, November 2009","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/0912.1820v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/0912.1820v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2012.00187v1"],"updated":["2020-12-01T00:48:27Z"],"published":["2020-12-01T00:48:27Z"],"title":["Statistical patterns of word frequency suggesting the probabilistic\n  nature of human languages"],"summary":["  Traditional linguistic theories have largely regard language as a formal\nsystem composed of rigid rules. However, their failures in processing real\nlanguage, the recent successes in statistical natural language processing, and\nthe findings of many psychological experiments have suggested that language may\nbe more a probabilistic system than a formal system, and thus cannot be\nfaithfully modeled with the either/or rules of formal linguistic theory. The\npresent study, based on authentic language data, confirmed that those important\nlinguistic issues, such as linguistic universal, diachronic drift, and language\nvariations can be translated into probability and frequency patterns in parole.\nThese findings suggest that human language may well be probabilistic systems by\nnature, and that statistical may well make inherent properties of human\nlanguages.\n"],"author":[{"name":["Shuiyuan Yu"]},{"name":["Chunshan Xu"]},{"name":["Haitao Liu"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2012.00187v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2012.00187v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"physics.comp-ph","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2002.06053v1"],"updated":["2020-02-10T21:02:05Z"],"published":["2020-02-10T21:02:05Z"],"title":["Exploring Chemical Space using Natural Language Processing Methodologies\n  for Drug Discovery"],"summary":["  Text-based representations of chemicals and proteins can be thought of as\nunstructured languages codified by humans to describe domain-specific\nknowledge. Advances in natural language processing (NLP) methodologies in the\nprocessing of spoken languages accelerated the application of NLP to elucidate\nhidden knowledge in textual representations of these biochemical entities and\nthen use it to construct models to predict molecular properties or to design\nnovel molecules. This review outlines the impact made by these advances on drug\ndiscovery and aims to further the dialogue between medicinal chemists and\ncomputer scientists.\n"],"author":[{"name":["Hakime Öztürk"]},{"name":["Arzucan Özgür"]},{"name":["Philippe Schwaller"]},{"name":["Teodoro Laino"]},{"name":["Elif Ozkirimli"]}],"arxiv:doi":[{"_":"10.1016/j.drudis.2020.01.020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1016/j.drudis.2020.01.020","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2002.06053v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2002.06053v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"q-bio.BM","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"q-bio.BM","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"stat.ML","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.11115v1"],"updated":["2021-05-24T06:42:58Z"],"published":["2021-05-24T06:42:58Z"],"title":["Self-Attention Networks Can Process Bounded Hierarchical Languages"],"summary":["  Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n"],"author":[{"name":["Shunyu Yao"]},{"name":["Binghui Peng"]},{"name":["Christos Papadimitriou"]},{"name":["Karthik Narasimhan"]}],"arxiv:comment":[{"_":"ACL 2021. 19 pages with extended appendix. Code:\n  https://github.com/princeton-nlp/dyck-transformer","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.11115v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.11115v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1906.04068v1"],"updated":["2019-06-10T15:20:32Z"],"published":["2019-06-10T15:20:32Z"],"title":["Hierarchical Representation in Neural Language Models: Suppression and\n  Recovery of Expectations"],"summary":["  Deep learning sequence models have led to a marked increase in performance\nfor a range of Natural Language Processing tasks, but it remains an open\nquestion whether they are able to induce proper hierarchical generalizations\nfor representing natural language from linear input alone. Work using\nartificial languages as training input has shown that LSTMs are capable of\ninducing the stack-like data structures required to represent context-free and\ncertain mildly context-sensitive languages---formal language classes which\ncorrespond in theory to the hierarchical structures of natural language. Here\nwe present a suite of experiments probing whether neural language models\ntrained on linguistic data induce these stack-like data structures and deploy\nthem while incrementally predicting words. We study two natural language\nphenomena: center embedding sentences and syntactic island constraints on the\nfiller--gap dependency. In order to properly predict words in these structures,\na model must be able to temporarily suppress certain expectations and then\nrecover those expectations later, essentially pushing and popping these\nexpectations on a stack. Our results provide evidence that models can\nsuccessfully suppress and recover expectations in many cases, but do not fully\nrecover their previous grammatical state.\n"],"author":[{"name":["Ethan Wilcox"]},{"name":["Roger Levy"]},{"name":["Richard Futrell"]}],"arxiv:comment":[{"_":"Proceedings of BlackboxNLP 2019, ACL, Florence, Italy","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1906.04068v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1906.04068v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1302.5181v1"],"updated":["2013-02-21T04:58:48Z"],"published":["2013-02-21T04:58:48Z"],"title":["Basic Classes of Grammars with Prohibition"],"summary":["  A practical tool for natural language modeling and development of\nhuman-machine interaction is developed in the context of formal grammars and\nlanguages. A new type of formal grammars, called grammars with prohibition, is\nintroduced. Grammars with prohibition provide more powerful tools for natural\nlanguage generation and better describe processes of language learning than the\nconventional formal grammars. Here we study relations between languages\ngenerated by different grammars with prohibition based on conventional types of\nformal grammars such as context-free or context sensitive grammars. Besides, we\ncompare languages generated by different grammars with prohibition and\nlanguages generated by conventional formal grammars. In particular, it is\ndemonstrated that they have essentially higher computational power and\nexpressive possibilities in comparison with the conventional formal grammars.\nThus, while conventional formal grammars are recursive and subrecursive\nalgorithms, many classes of grammars with prohibition are superrecursive\nalgorithms. Results presented in this work are aimed at the development of\nhuman-machine interaction, modeling natural languages, empowerment of\nprogramming languages, computer simulation, better software systems, and theory\nof recursion.\n"],"author":[{"name":["Mark Burgin"]}],"arxiv:comment":[{"_":"2 tables","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1302.5181v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1302.5181v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.FL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1905.11471v1"],"updated":["2019-05-27T19:44:33Z"],"published":["2019-05-27T19:44:33Z"],"title":["XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and\n  Question Answering"],"summary":["  While natural language processing systems often focus on a single language,\nmultilingual transfer learning has the potential to improve performance,\nespecially for low-resource languages. We introduce XLDA, cross-lingual data\naugmentation, a method that replaces a segment of the input text with its\ntranslation in another language. XLDA enhances performance of all 14 tested\nlanguages of the cross-lingual natural language inference (XNLI) benchmark.\nWith improvements of up to $4.8\\%$, training with XLDA achieves\nstate-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast\nto, and performs markedly better than, a more naive approach that aggregates\nexamples in various languages in a way that each example is solely in one\nlanguage. On the SQuAD question answering task, we see that XLDA provides a\n$1.0\\%$ performance increase on the English evaluation set. Comprehensive\nexperiments suggest that most languages are effective as cross-lingual\naugmentors, that XLDA is robust to a wide range of translation quality, and\nthat XLDA is even more effective for randomly initialized models than for\npretrained models.\n"],"author":[{"name":["Jasdeep Singh"]},{"name":["Bryan McCann"]},{"name":["Nitish Shirish Keskar"]},{"name":["Caiming Xiong"]},{"name":["Richard Socher"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1905.11471v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1905.11471v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2204.03905v2"],"updated":["2022-04-22T07:47:42Z"],"published":["2022-04-08T08:07:42Z"],"title":["BioBART: Pretraining and Evaluation of A Biomedical Generative Language\n  Model"],"summary":["  Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.\n"],"author":[{"name":["Hongyi Yuan"]},{"name":["Zheng Yuan"]},{"name":["Ruyi Gan"]},{"name":["Jiaxing Zhang"]},{"name":["Yutao Xie"]},{"name":["Sheng Yu"]}],"arxiv:comment":[{"_":"Accepted by BioNLP 2022, Long Paper","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2204.03905v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2204.03905v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1903.04739v1"],"updated":["2019-03-12T05:52:41Z"],"published":["2019-03-12T05:52:41Z"],"title":["Syllable-based Neural Named Entity Recognition for Myanmar Language"],"summary":["  Named Entity Recognition (NER) for Myanmar Language is essential to Myanmar\nnatural language processing research work. In this work, NER for Myanmar\nlanguage is treated as a sequence tagging problem and the effectiveness of deep\nneural networks on NER for Myanmar language has been investigated. Experiments\nare performed by applying deep neural network architectures on syllable level\nMyanmar contexts. Very first manually annotated NER corpus for Myanmar language\nis also constructed and proposed. In developing our in-house NER corpus,\nsentences from online news website and also sentences supported from\nALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian\nLanguage Treebank (ALT) project under ASEAN IVO. This paper contributes the\nfirst evaluation of neural network models on NER task for Myanmar language. The\nexperimental results show that those neural sequence models can produce\npromising results compared to the baseline CRF model. Among those neural\narchitectures, bidirectional LSTM network added CRF layer above gives the\nhighest F-score value. This work also aims to discover the effectiveness of\nneural network approaches to Myanmar textual processing as well as to promote\nfurther researches on this understudied language.\n"],"author":[{"name":["Hsu Myat Mo"]},{"name":["Khin Mar Soe"]}],"arxiv:doi":[{"_":"10.5121/ijnlc.2019.8101","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.5121/ijnlc.2019.8101","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1903.04739v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1903.04739v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Myanmar NER","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"International Journal on Natural Language Computing (IJNLC) Vol.8,\n  No.1, February 2019","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2008.11785v1"],"updated":["2020-08-26T20:06:30Z"],"published":["2020-08-26T20:06:30Z"],"title":["Understanding scholarly Natural Language Processing system diagrams\n  through application of the Richards-Engelhardt framework"],"summary":["  We utilise Richards-Engelhardt framework as a tool for understanding Natural\nLanguage Processing systems diagrams. Through four examples from scholarly\nproceedings, we find that the application of the framework to this ecological\nand complex domain is effective for reflecting on these diagrams. We argue for\nvocabulary to describe multiple-codings, semiotic variability, and\ninconsistency or misuse of visual encoding principles in diagrams. Further, for\napplication to scholarly Natural Language Processing systems, and perhaps\nsystems diagrams more broadly, we propose the addition of \"Grouping by Object\"\nas a new visual encoding principle, and \"Emphasising\" as a new visual encoding\ntype.\n"],"author":[{"name":["Guy Clarke Marshall"]},{"name":["Caroline Jay"]},{"name":["André Freitas"]}],"arxiv:comment":[{"_":"16 pages, 5 figures, pre-print","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2008.11785v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2008.11785v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.HC","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.HC","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2107.06056v1"],"updated":["2021-07-13T13:10:10Z"],"published":["2021-07-13T13:10:10Z"],"title":["Indian Legal NLP Benchmarks : A Survey"],"summary":["  Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.\n"],"author":[{"name":["Prathamesh Kalamkar"]},{"name":["Janani Venugopalan Ph. D."]},{"name":["Vivek Raghavan Ph. D"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2107.06056v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2107.06056v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2101.06949v1"],"updated":["2021-01-18T09:23:35Z"],"published":["2021-01-18T09:23:35Z"],"title":["HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language"],"summary":["  Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.\n"],"author":[{"name":["Harsh Patel"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2101.06949v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2101.06949v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1701.03338v2"],"updated":["2017-07-29T15:52:00Z"],"published":["2017-01-12T13:41:08Z"],"title":["LanideNN: Multilingual Language Identification on Character Window"],"summary":["  In language identification, a common first step in natural language\nprocessing, we want to automatically determine the language of some input text.\nMonolingual language identification assumes that the given document is written\nin one language. In multilingual language identification, the document is\nusually in two or three languages and we just want their names. We aim one step\nfurther and propose a method for textual language identification where\nlanguages can change arbitrarily and the goal is to identify the spans of each\nof the languages. Our method is based on Bidirectional Recurrent Neural\nNetworks and it performs well in monolingual and multilingual language\nidentification tasks on six datasets covering 131 languages. The method keeps\nthe accuracy also for short documents and across domains, so it is ideal for\noff-the-shelf use without preparation of training data.\n"],"author":[{"name":["Tom Kocmi"]},{"name":["Ondřej Bojar"]}],"arxiv:comment":[{"_":"Accepted to EACL 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings of the 15th Conference of the European Chapter of the\n  Association for Computational Linguistics: Volume 1, Long Papers, 927-936\n  (2017)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1701.03338v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1701.03338v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1708.09417v1"],"updated":["2017-08-30T18:22:28Z"],"published":["2017-08-30T18:22:28Z"],"title":["LangPro: Natural Language Theorem Prover"],"summary":["  LangPro is an automated theorem prover for natural language\n(https://github.com/kovvalsky/LangPro). Given a set of premises and a\nhypothesis, it is able to prove semantic relations between them. The prover is\nbased on a version of analytic tableau method specially designed for natural\nlogic. The proof procedure operates on logical forms that preserve linguistic\nexpressions to a large extent. %This property makes the logical forms easily\nobtainable from syntactic trees. %, in particular, Combinatory Categorial\nGrammar derivation trees. The nature of proofs is deductive and transparent. On\nthe FraCaS and SICK textual entailment datasets, the prover achieves high\nresults comparable to state-of-the-art.\n"],"author":[{"name":["Lasha Abzianidze"]}],"arxiv:comment":[{"_":"6 pages, 8 figures, Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1708.09417v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1708.09417v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"68T50","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2005.03812v1"],"updated":["2020-05-08T01:16:03Z"],"published":["2020-05-08T01:16:03Z"],"title":["Comparative Analysis of Word Embeddings for Capturing Word Similarities"],"summary":["  Distributed language representation has become the most widely used technique\nfor language representation in various natural language processing tasks. Most\nof the natural language processing models that are based on deep learning\ntechniques use already pre-trained distributed word representations, commonly\ncalled word embeddings. Determining the most qualitative word embeddings is of\ncrucial importance for such models. However, selecting the appropriate word\nembeddings is a perplexing task since the projected embedding space is not\nintuitive to humans. In this paper, we explore different approaches for\ncreating distributed word representations. We perform an intrinsic evaluation\nof several state-of-the-art word embedding methods. Their performance on\ncapturing word similarities is analysed with existing benchmark datasets for\nword pairs similarities. The research in this paper conducts a correlation\nanalysis between ground truth word similarities and similarities obtained by\ndifferent word embedding methods.\n"],"author":[{"name":["Martina Toshevska"]},{"name":["Frosina Stojanovska"]},{"name":["Jovan Kalajdjieski"]}],"arxiv:doi":[{"_":"10.5121/csit.2020.100402","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.5121/csit.2020.100402","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2005.03812v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2005.03812v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Part of the 6th International Conference on Natural Language\n  Processing (NATP 2020)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"6th International Conference on Natural Language Processing (NATP\n  2020)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9709010v1"],"updated":["1997-09-23T12:43:09Z"],"published":["1997-09-23T12:43:09Z"],"title":["Message-Passing Protocols for Real-World Parsing -- An Object-Oriented\n  Model and its Preliminary Evaluation"],"summary":["  We argue for a performance-based design of natural language grammars and\ntheir associated parsers in order to meet the constraints imposed by real-world\nNLP. Our approach incorporates declarative and procedural knowledge about\nlanguage and language use within an object-oriented specification framework. We\ndiscuss several message-passing protocols for parsing and provide reasons for\nsacrificing completeness of the parse in favor of efficiency based on a\npreliminary empirical evaluation.\n"],"author":[{"name":["Udo Hahn"],"arxiv:affiliation":[{"_":"Computational Linguistics Lab, Freiburg University","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Peter Neuhaus"],"arxiv:affiliation":[{"_":"Computational Linguistics Lab, Freiburg University","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Norbert Broeker"],"arxiv:affiliation":[{"_":"Institute for Natural Language Processing, Stuttgart University","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"12 pages, uses epsfig.sty","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proc. Int'l Workshop on Parsing Technologies, 1997, Boston/MA:\n  MIT, pp 101-112","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9709010v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9709010v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1803.08793v1"],"updated":["2018-03-21T16:14:22Z"],"published":["2018-03-21T16:14:22Z"],"title":["Exploring the Naturalness of Buggy Code with Recurrent Neural Networks"],"summary":["  Statistical language models are powerful tools which have been used for many\ntasks within natural language processing. Recently, they have been used for\nother sequential data such as source code.(Ray et al., 2015) showed that it is\npossible train an n-gram source code language mode, and use it to predict buggy\nlines in code by determining \"unnatural\" lines via entropy with respect to the\nlanguage model. In this work, we propose using a more advanced language\nmodeling technique, Long Short-term Memory recurrent neural networks, to model\nsource code and classify buggy lines based on entropy. We show that our method\nslightly outperforms an n-gram model in the buggy line classification task\nusing AUC.\n"],"author":[{"name":["Jack Lanchantin"]},{"name":["Ji Gao"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1803.08793v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1803.08793v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2103.16712v1"],"updated":["2021-03-30T22:43:13Z"],"published":["2021-03-30T22:43:13Z"],"title":["Collaborative construction of lexicographic and parallel datasets for\n  African languages: first assessment"],"summary":["  Faced with a considerable lack of resources in African languages to carry out\nwork in Natural Language Processing (NLP), Natural Language Understanding (NLU)\nand artificial intelligence, the research teams of NTeALan association has set\nitself the objective of building open-source platforms for the collaborative\nconstruction of lexicographic data in African languages. In this article, we\npresent our first reports after 2 years of collaborative construction of\nlexicographic resources useful for African NLP tools.\n"],"author":[{"name":["Elvis Mboning Tchiaze"]}],"arxiv:comment":[{"_":"EACL 2021 - AfricaNLP workshop","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2103.16712v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2103.16712v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1711.01100v1"],"updated":["2017-11-03T10:53:05Z"],"published":["2017-11-03T10:53:05Z"],"title":["One Model to Rule them all: Multitask and Multilingual Modelling for\n  Lexical Analysis"],"summary":["  When learning a new skill, you take advantage of your preexisting skills and\nknowledge. For instance, if you are a skilled violinist, you will likely have\nan easier time learning to play cello. Similarly, when learning a new language\nyou take advantage of the languages you already speak. For instance, if your\nnative language is Norwegian and you decide to learn Dutch, the lexical overlap\nbetween these two languages will likely benefit your rate of language\nacquisition. This thesis deals with the intersection of learning multiple tasks\nand learning multiple languages in the context of Natural Language Processing\n(NLP), which can be defined as the study of computational processing of human\nlanguage. Although these two types of learning may seem different on the\nsurface, we will see that they share many similarities.\n  The traditional approach in NLP is to consider a single task for a single\nlanguage at a time. However, recent advances allow for broadening this\napproach, by considering data for multiple tasks and languages simultaneously.\nThis is an important approach to explore further as the key to improving the\nreliability of NLP, especially for low-resource languages, is to take advantage\nof all relevant data whenever possible. In doing so, the hope is that in the\nlong term, low-resource languages can benefit from the advances made in NLP\nwhich are currently to a large extent reserved for high-resource languages.\nThis, in turn, may then have positive consequences for, e.g., language\npreservation, as speakers of minority languages will have a lower degree of\npressure to using high-resource languages. In the short term, answering the\nspecific research questions posed should be of use to NLP researchers working\ntowards the same goal.\n"],"author":[{"name":["Johannes Bjerva"]}],"arxiv:comment":[{"_":"PhD thesis, University of Groningen","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1711.01100v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1711.01100v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1304.3265v1"],"updated":["2013-04-11T11:56:39Z"],"published":["2013-04-11T11:56:39Z"],"title":["Extension of hidden markov model for recognizing large vocabulary of\n  sign language"],"summary":["  Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.\n"],"author":[{"name":["Maher Jebali"]},{"name":["Patrice Dalle"]},{"name":["Mohamed Jemni"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1304.3265v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1304.3265v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1312.6948v1"],"updated":["2013-12-25T09:23:49Z"],"published":["2013-12-25T09:23:49Z"],"title":["Description Logics based Formalization of Wh-Queries"],"summary":["  The problem of Natural Language Query Formalization (NLQF) is to translate a\ngiven user query in natural language (NL) into a formal language so that the\nsemantic interpretation has equivalence with the NL interpretation.\nFormalization of NL queries enables logic based reasoning during information\nretrieval, database query, question-answering, etc. Formalization also helps in\nWeb query normalization and indexing, query intent analysis, etc. In this paper\nwe are proposing a Description Logics based formal methodology for wh-query\nintent (also called desire) identification and corresponding formal\ntranslation. We evaluated the scalability of our proposed formalism using\nMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\n"],"author":[{"name":["Sourish Dasgupta"]},{"name":["Rupali KaPatel"]},{"name":["Ankur Padia"]},{"name":["Kushal Shah"]}],"arxiv:comment":[{"_":"Natural Language Query Processing, Representation","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1312.6948v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1312.6948v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2111.07119v1"],"updated":["2021-11-13T14:06:37Z"],"published":["2021-11-13T14:06:37Z"],"title":["Extracting and filtering paraphrases by bridging natural language\n  inference and paraphrasing"],"summary":["  Paraphrasing is a useful natural language processing task that can contribute\nto more diverse generated or translated texts. Natural language inference (NLI)\nand paraphrasing share some similarities and can benefit from a joint approach.\nWe propose a novel methodology for the extraction of paraphrasing datasets from\nNLI datasets and cleaning existing paraphrasing datasets. Our approach is based\non bidirectional entailment; namely, if two sentences can be mutually entailed,\nthey are paraphrases. We evaluate our approach using several large pretrained\ntransformer language models in the monolingual and cross-lingual setting. The\nresults show high quality of extracted paraphrasing datasets and surprisingly\nhigh noise levels in two existing paraphrasing datasets.\n"],"author":[{"name":["Matej Klemen"]},{"name":["Marko Robnik-Šikonja"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2111.07119v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2111.07119v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2204.09593v1"],"updated":["2022-04-01T07:03:40Z"],"published":["2022-04-01T07:03:40Z"],"title":["COOL, a Context Outlooker, and its Application to Question Answering and\n  other Natural Language Processing Tasks"],"summary":["  Vision outlookers improve the performance of vision transformers, which\nimplement a self-attention mechanism by adding outlook attention, a form of\nlocal attention.\n  In natural language processing, as has been the case in computer vision and\nother domains, transformer-based models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many authors have argued and\ndemonstrated the importance of local context.\n  We present and evaluate an outlook attention mechanism, COOL, for natural\nlanguage processing. COOL adds, on top of the self-attention layers of a\ntransformer-based model, outlook attention layers that encode local syntactic\ncontext considering word proximity and consider more pair-wise constraints than\ndynamic convolution operations used by existing approaches.\n  A comparative empirical performance evaluation of an implementation of COOL\nwith different transformer-based approaches confirms the opportunity of\nimprovement over a baseline using the neural language models alone for various\nnatural language processing tasks, including question answering. The proposed\napproach is competitive with state-of-the-art methods.\n"],"author":[{"name":["Fangyi Zhu"]},{"name":["See-Kiong Ng"]},{"name":["Stéphane Bressan"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2204.09593v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2204.09593v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9512004v1"],"updated":["1995-12-21T16:38:12Z"],"published":["1995-12-21T16:38:12Z"],"title":["Natural language processing: she needs something old and something new\n  (maybe something borrowed and something blue, too)"],"summary":["  Given the present state of work in natural language processing, this address\nargues first, that advance in both science and applications requires a revival\nof concern about what language is about, broadly speaking the world; and\nsecond, that an attack on the summarising task, which is made ever more\nimportant by the growth of electronic text resources and requires an\nunderstanding of the role of large-scale discourse structure in marking\nimportant text content, is a good way forward.\n"],"author":[{"name":["Karen Sparck Jones"],"arxiv:affiliation":[{"_":"Computer Laboratory, University of Cambridge","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"Presidential Address, 1994, Association for Computational Linguistics","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9512004v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9512004v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/cmp-lg/9712008v1"],"updated":["1997-12-23T15:32:05Z"],"published":["1997-12-23T15:32:05Z"],"title":["What is word sense disambiguation good for?"],"summary":["  Word sense disambiguation has developed as a sub-area of natural language\nprocessing, as if, like parsing, it was a well-defined task which was a\npre-requisite to a wide range of language-understanding applications. First, I\nreview earlier work which shows that a set of senses for a word is only ever\ndefined relative to a particular human purpose, and that a view of word senses\nas part of the linguistic furniture lacks theoretical underpinnings. Then, I\ninvestigate whether and how word sense ambiguity is in fact a problem for\ndifferent varieties of NLP application.\n"],"author":[{"name":["Adam Kilgarriff"],"arxiv:affiliation":[{"_":"ITRI, University of Brighton","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:comment":[{"_":"6 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proc. Natural Language Processing Pacific Rim Symposium. Phuket,\n  Thailand. December 1997. Pp 209--214.","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/cmp-lg/9712008v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/cmp-lg/9712008v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cmp-lg","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1209.1301v1"],"updated":["2012-08-19T02:31:29Z"],"published":["2012-08-19T02:31:29Z"],"title":["Evaluation of Computational Grammar Formalisms for Indian Languages"],"summary":["  Natural Language Parsing has been the most prominent research area since the\ngenesis of Natural Language Processing. Probabilistic Parsers are being\ndeveloped to make the process of parser development much easier, accurate and\nfast. In Indian context, identification of which Computational Grammar\nFormalism is to be used is still a question which needs to be answered. In this\npaper we focus on this problem and try to analyze different formalisms for\nIndian languages.\n"],"author":[{"name":["Nisheeth Joshi"]},{"name":["Iti Mathur"]}],"arxiv:comment":[{"_":"Proc. of International Conference in Computer Engineering and\n  Technology, 2012, Organized by Jodhpur Institute of Engineering and\n  Technology, Jodhpur. Sponsored by IEEE, USA and Institution of Engineers\n  (India), Kolkatta","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1209.1301v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1209.1301v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]}]