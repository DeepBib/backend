[{"id":["http://arxiv.org/abs/2101.10848v1"],"updated":["2021-01-26T15:11:52Z"],"published":["2021-01-26T15:11:52Z"],"title":["Spark NLP: Natural Language Understanding at Scale"],"summary":["  Spark NLP is a Natural Language Processing (NLP) library built on top of\nApache Spark ML. It provides simple, performant and accurate NLP annotations\nfor machine learning pipelines that can scale easily in a distributed\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\nhealthcare organizations as the worlds most widely used NLP library in the\nenterprise.\n"],"author":[{"name":["Veysel Kocaman"]},{"name":["David Talby"]}],"arxiv:comment":[{"_":"=Accepted as a publication in Elsevier, Software Impacts Journal.\n  arXiv admin note: substantial text overlap with arXiv:2012.04005","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2101.10848v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2101.10848v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2010.03061v1"],"updated":["2020-10-06T22:23:00Z"],"published":["2020-10-06T22:23:00Z"],"title":["A Survey on Recognizing Textual Entailment as an NLP Evaluation"],"summary":["  Recognizing Textual Entailment (RTE) was proposed as a unified evaluation\nframework to compare semantic understanding of different NLP systems. In this\nsurvey paper, we provide an overview of different approaches for evaluating and\nunderstanding the reasoning capabilities of NLP systems. We then focus our\ndiscussion on RTE by highlighting prominent RTE datasets as well as advances in\nRTE dataset that focus on specific linguistic phenomena that can be used to\nevaluate NLP systems on a fine-grained level. We conclude by arguing that when\nevaluating NLP systems, the community should utilize newly introduced RTE\ndatasets that focus on specific linguistic phenomena.\n"],"author":[{"name":["Adam Poliak"]}],"arxiv:comment":[{"_":"1st Workshop on Evaluation and Comparison for NLP systems (Eval4NLP)\n  at EMNLP 2020; 18 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2010.03061v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2010.03061v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.11410v2"],"updated":["2021-07-15T20:57:12Z"],"published":["2021-06-21T20:59:06Z"],"title":["A Survey of Race, Racism, and Anti-Racism in NLP"],"summary":["  Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.\n"],"author":[{"name":["Anjalie Field"]},{"name":["Su Lin Blodgett"]},{"name":["Zeerak Waseem"]},{"name":["Yulia Tsvetkov"]}],"arxiv:comment":[{"_":"Accepted to ACL 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.11410v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.11410v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1808.09772v2"],"updated":["2018-08-30T17:44:54Z"],"published":["2018-08-29T12:58:45Z"],"title":["Notes on Deep Learning for NLP"],"summary":["  My notes on Deep Learning for NLP.\n"],"author":[{"name":["Antoine J. -P. Tixier"]}],"arxiv:comment":[{"_":"work in progress","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1808.09772v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1808.09772v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.07874v1"],"updated":["2021-04-16T03:46:10Z"],"published":["2021-04-16T03:46:10Z"],"title":["Translational NLP: A New Paradigm and General Principles for Natural\n  Language Processing Research"],"summary":["  Natural language processing (NLP) research combines the study of universal\nprinciples, through basic science, with applied science targeting specific use\ncases and settings. However, the process of exchange between basic NLP and\napplications is often assumed to emerge naturally, resulting in many\ninnovations going unapplied and many important questions left unstudied. We\ndescribe a new paradigm of Translational NLP, which aims to structure and\nfacilitate the processes by which basic and applied NLP research inform one\nanother. Translational NLP thus presents a third research paradigm, focused on\nunderstanding the challenges posed by application needs and how these\nchallenges can drive innovation in basic science and technology design. We show\nthat many significant advances in NLP research have emerged from the\nintersection of basic principles with application needs, and present a\nconceptual framework outlining the stakeholders and key questions in\ntranslational research. Our framework provides a roadmap for developing\nTranslational NLP as a dedicated research area, and identifies general\ntranslational principles to facilitate exchange between basic and applied\nresearch.\n"],"author":[{"name":["Denis Newman-Griffis"]},{"name":["Jill Fain Lehman"]},{"name":["Carolyn Rosé"]},{"name":["Harry Hochheiser"]}],"arxiv:comment":[{"_":"Accepted to NAACL-HLT 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.07874v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.07874v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.13704v1"],"updated":["2021-05-28T09:57:22Z"],"published":["2021-05-28T09:57:22Z"],"title":["Natural Language Processing 4 All (NLP4All): A New Online Platform for\n  Teaching and Learning NLP Concepts"],"summary":["  Natural Language Processing offers new insights into language data across\nalmost all disciplines and domains, and allows us to corroborate and/or\nchallenge existing knowledge. The primary hurdles to widening participation in\nand use of these new research tools are, first, a lack of coding skills in\nstudents across K-16, and in the population at large, and second, a lack of\nknowledge of how NLP-methods can be used to answer questions of disciplinary\ninterest outside of linguistics and/or computer science. To broaden\nparticipation in NLP and improve NLP-literacy, we introduced a new tool\nweb-based tool called Natural Language Processing 4 All (NLP4All). The intended\npurpose of NLP4All is to help teachers facilitate learning with and about NLP,\nby providing easy-to-use interfaces to NLP-methods, data, and analyses, making\nit possible for non- and novice-programmers to learn NLP concepts\ninteractively.\n"],"author":[{"name":["Rebekah Baglini"]},{"name":["Arthur Hjorth"]}],"arxiv:comment":[{"_":"Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.13704v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.13704v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2204.04282v1"],"updated":["2022-04-08T20:28:00Z"],"published":["2022-04-08T20:28:00Z"],"title":["Classification of Natural Language Processing Techniques for\n  Requirements Engineering"],"summary":["  Research in applying natural language processing (NLP) techniques to\nrequirements engineering (RE) tasks spans more than 40 years, from initial\nefforts carried out in the 1980s to more recent attempts with machine learning\n(ML) and deep learning (DL) techniques. However, in spite of the progress, our\nrecent survey shows that there is still a lack of systematic understanding and\norganization of commonly used NLP techniques in RE. We believe one hurdle\nfacing the industry is lack of shared knowledge of NLP techniques and their\nusage in RE tasks. In this paper, we present our effort to synthesize and\norganize 57 most frequently used NLP techniques in RE. We classify these NLP\ntechniques in two ways: first, by their NLP tasks in typical pipelines and\nsecond, by their linguist analysis levels. We believe these two ways of\nclassification are complementary, contributing to a better understanding of the\nNLP techniques in RE and such understanding is crucial to the development of\nbetter NLP tools for RE.\n"],"author":[{"name":["Liping Zhao"]},{"name":["Waad Alhoshan"]},{"name":["Alessio Ferrari"]},{"name":["Keletso J. Letsholo"]}],"arxiv:comment":[{"_":"10 pages, 4 tables, 1 figure","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2204.04282v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2204.04282v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"68-02","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"A.1; D.m; I.7.m","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1409.2073v1"],"updated":["2014-09-07T02:31:03Z"],"published":["2014-09-07T02:31:03Z"],"title":["An NLP Assistant for Clide"],"summary":["  This report describes an NLP assistant for the collaborative development\nenvironment Clide, that supports the development of NLP applications by\nproviding easy access to some common NLP data structures. The assistant\nvisualizes text fragments and their dependencies by displaying the semantic\ngraph of a sentence, the coreference chain of a paragraph and mined triples\nthat are extracted from a paragraph's semantic graphs and linked using its\ncoreference chain. Using this information and a logic programming library, we\ncreate an NLP database which is used by a series of queries to mine the\ntriples. The algorithm is tested by translating a natural language text\ndescribing a graph to an actual graph that is shown as an annotation in the\ntext editor.\n"],"author":[{"name":["Tobias Kortkamp"]}],"arxiv:comment":[{"_":"Bachelor Report","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1409.2073v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1409.2073v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.11326v1"],"updated":["2021-09-23T12:19:56Z"],"published":["2021-09-23T12:19:56Z"],"title":["The Current State of Finnish NLP"],"summary":["  There are a lot of tools and resources available for processing Finnish. In\nthis paper, we survey recent papers focusing on Finnish NLP related to many\ndifferent subcategories of NLP such as parsing, generation, semantics and\nspeech. NLP research is conducted in many different research groups in Finland,\nand it is frequently the case that NLP tools and models resulting from academic\nresearch are made available for others to use on platforms such as Github.\n"],"author":[{"name":["Mika Hämäläinen"]},{"name":["Khalid Alnajjar"]}],"arxiv:comment":[{"_":"Seventh international workshop on computational linguistics of Uralic\n  languages (IWCLUL)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.11326v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.11326v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2202.07105v1"],"updated":["2022-02-15T00:18:47Z"],"published":["2022-02-15T00:18:47Z"],"title":["A Survey on Model Compression for Natural Language Processing"],"summary":["  With recent developments in new architectures like Transformer and\npretraining techniques, significant progress has been made in applications of\nnatural language processing (NLP). However, the high energy cost and long\ninference delay of Transformer is preventing NLP from entering broader\nscenarios including edge and mobile computing. Efficient NLP research aims to\ncomprehensively consider computation, time and carbon emission for the entire\nlife-cycle of NLP, including data preparation, model training and inference. In\nthis survey, we focus on the inference stage and review the current state of\nmodel compression for NLP, including the benchmarks, metrics and methodology.\nWe outline the current obstacles and future research directions.\n"],"author":[{"name":["Canwen Xu"]},{"name":["Julian McAuley"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2202.07105v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2202.07105v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2205.03559v1"],"updated":["2022-05-07T05:22:43Z"],"published":["2022-05-07T05:22:43Z"],"title":["Number Entity Recognition"],"summary":["  Numbers are essential components of text, like any other word tokens, from\nwhich natural language processing (NLP) models are built and deployed. Though\nnumbers are typically not accounted for distinctly in most NLP tasks, there is\nstill an underlying amount of numeracy already exhibited by NLP models. In this\nwork, we attempt to tap this potential of state-of-the-art NLP models and\ntransfer their ability to boost performance in related tasks. Our proposed\nclassification of numbers into entities helps NLP models perform well on\nseveral tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on\nquestion answering using joint embeddings, outperforming the BERT and RoBERTa\nbaseline classification.\n"],"author":[{"name":["Dhanasekar Sundararaman"]},{"name":["Vivek Subramanian"]},{"name":["Guoyin Wang"]},{"name":["Liyan Xu"]},{"name":["Lawrence Carin"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2205.03559v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2205.03559v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.02359v2"],"updated":["2021-07-24T19:26:38Z"],"published":["2021-06-04T09:17:15Z"],"title":["How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social\n  Impact"],"summary":["  Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.\n"],"author":[{"name":["Zhijing Jin"]},{"name":["Geeticka Chauhan"]},{"name":["Brian Tse"]},{"name":["Mrinmaya Sachan"]},{"name":["Rada Mihalcea"]}],"arxiv:comment":[{"_":"Findings of ACL 2021; also accepted at the NLP for Positive Impact\n  workshop@ACL 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.02359v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.02359v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2107.06785v2"],"updated":["2021-07-15T02:04:08Z"],"published":["2021-07-14T15:42:15Z"],"title":["Large-Scale News Classification using BERT Language Model: Spark NLP\n  Approach"],"summary":["  The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.\n"],"author":[{"name":["Kuncahyo Setyo Nugroho"]},{"name":["Anantha Yullian Sukmadewa"]},{"name":["Novanto Yudistira"]}],"arxiv:doi":[{"_":"10.1145/3479645.3479658","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1145/3479645.3479658","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2107.06785v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2107.06785v2","rel":"related","type":"application/pdf"}}],"arxiv:journal_ref":[{"_":"SIET '21: 6th International Conference on Sustainable Information\n  Engineering and Technology 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CC","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2007.09134v1"],"updated":["2020-07-17T17:50:50Z"],"published":["2020-07-17T17:50:50Z"],"title":["A Systematic Review of Natural Language Processing for Knowledge\n  Management in Healthcare"],"summary":["  Driven by the visions of Data Science, recent years have seen a paradigm\nshift in Natural Language Processing (NLP). NLP has set the milestone in text\nprocessing and proved to be the preferred choice for researchers in the\nhealthcare domain. The objective of this paper is to identify the potential of\nNLP, especially, how NLP is used to support the knowledge management process in\nthe healthcare domain, making data a critical and trusted component in\nimproving the health outcomes. This paper provides a comprehensive survey of\nthe state-of-the-art NLP research with a particular focus on how knowledge is\ncreated, captured, shared, and applied in the healthcare domain. Our findings\nsuggest, first, the techniques of NLP those supporting knowledge management\nextraction and knowledge capture processes in healthcare. Second, we propose a\nconceptual model for the knowledge extraction process through NLP. Finally, we\ndiscuss a set of issues, challenges, and proposed future research areas.\n"],"author":[{"name":["Ganga Prasad Basyal"]},{"name":["Bhaskar P. Rimal"]},{"name":["David Zeng"]}],"arxiv:doi":[{"_":"10.5121/csit.2020.100921","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.5121/csit.2020.100921","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2007.09134v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2007.09134v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.00895v1"],"updated":["2021-05-03T14:30:32Z"],"published":["2021-05-03T14:30:32Z"],"title":["Teaching NLP outside Linguistics and Computer Science classrooms: Some\n  challenges and some opportunities"],"summary":["  NLP's sphere of influence went much beyond computer science research and the\ndevelopment of software applications in the past decade. We see people using\nNLP methods in a range of academic disciplines from Asian Studies to Clinical\nOncology. We also notice the presence of NLP as a module in most of the data\nscience curricula within and outside of regular university setups. These\ncourses are taken by students from very diverse backgrounds. This paper takes a\ncloser look at some issues related to teaching NLP to these diverse audiences\nbased on my classroom experiences, and identifies some challenges the\ninstructors face, particularly when there is no ecosystem of related courses\nfor the students. In this process, it also identifies a few challenge areas for\nboth NLP researchers and tool developers.\n"],"author":[{"name":["Sowmya Vajjala"]}],"arxiv:comment":[{"_":"To appear in the Teaching NLP workshop at NAACL 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.00895v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.00895v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.06090v1"],"updated":["2021-06-10T23:59:26Z"],"published":["2021-06-10T23:59:26Z"],"title":["Graph Neural Networks for Natural Language Processing: A Survey"],"summary":["  Deep learning has become the dominant approach in coping with various tasks\nin Natural LanguageProcessing (NLP). Although text inputs are typically\nrepresented as a sequence of tokens, there isa rich variety of NLP problems\nthat can be best expressed with a graph structure. As a result, thereis a surge\nof interests in developing new deep learning techniques on graphs for a large\nnumberof NLP tasks. In this survey, we present a comprehensive overview onGraph\nNeural Networks(GNNs) for Natural Language Processing. We propose a new\ntaxonomy of GNNs for NLP, whichsystematically organizes existing research of\nGNNs for NLP along three axes: graph construction,graph representation\nlearning, and graph based encoder-decoder models. We further introducea large\nnumber of NLP applications that are exploiting the power of GNNs and summarize\nthecorresponding benchmark datasets, evaluation metrics, and open-source codes.\nFinally, we discussvarious outstanding challenges for making the full use of\nGNNs for NLP as well as future researchdirections. To the best of our\nknowledge, this is the first comprehensive overview of Graph NeuralNetworks for\nNatural Language Processing.\n"],"author":[{"name":["Lingfei Wu"]},{"name":["Yu Chen"]},{"name":["Kai Shen"]},{"name":["Xiaojie Guo"]},{"name":["Hanning Gao"]},{"name":["Shucheng Li"]},{"name":["Jian Pei"]},{"name":["Bo Long"]}],"arxiv:comment":[{"_":"127 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.06090v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.06090v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.07410v1"],"updated":["2021-06-14T13:23:20Z"],"published":["2021-06-14T13:23:20Z"],"title":["Model Explainability in Deep Learning Based Natural Language Processing"],"summary":["  Machine learning (ML) model explainability has received growing attention,\nespecially in the area related to model risk and regulations. In this paper, we\nreviewed and compared some popular ML model explainability methodologies,\nespecially those related to Natural Language Processing (NLP) models. We then\napplied one of the NLP explainability methods Layer-wise Relevance Propagation\n(LRP) to a NLP classification model. We used the LRP method to derive a\nrelevance score for each word in an instance, which is a local explainability.\nThe relevance scores are then aggregated together to achieve global variable\nimportance of the model. Through the case study, we also demonstrated how to\napply the local explainability method to false positive and false negative\ninstances to discover the weakness of a NLP model. These analysis can help us\nto understand NLP models better and reduce the risk due to the black-box nature\nof NLP models. We also identified some common issues due to the special natures\nof NLP models and discussed how explainability analysis can act as a control to\ndetect these issues after the model has been trained.\n"],"author":[{"name":["Shafie Gholizadeh"]},{"name":["Nengfeng Zhou"]}],"arxiv:comment":[{"_":"12 pages, 8 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.07410v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.07410v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.07499v1"],"updated":["2021-06-14T15:27:22Z"],"published":["2021-06-14T15:27:22Z"],"title":["An Empirical Survey of Data Augmentation for Limited Data Learning in\n  NLP"],"summary":["  NLP has achieved great progress in the past decade through the use of neural\nmodels and large labeled datasets. The dependence on abundant data prevents NLP\nmodels from being applied to low-resource settings or novel tasks where\nsignificant time, money, or expertise is required to label massive amounts of\ntextual data. Recently, data augmentation methods have been explored as a means\nof improving data efficiency in NLP. To date, there has been no systematic\nempirical overview of data augmentation for NLP in the limited labeled data\nsetting, making it difficult to understand which methods work in which\nsettings. In this paper, we provide an empirical survey of recent progress on\ndata augmentation for NLP in the limited labeled data setting, summarizing the\nlandscape of methods (including token-level augmentations, sentence-level\naugmentations, adversarial augmentations, and hidden-space augmentations) and\ncarrying out experiments on 11 datasets covering topics/news classification,\ninference tasks, paraphrasing tasks, and single-sentence tasks. Based on the\nresults, we draw several conclusions to help practitioners choose appropriate\naugmentations in different settings and discuss the current challenges and\nfuture directions for limited data learning in NLP.\n"],"author":[{"name":["Jiaao Chen"]},{"name":["Derek Tam"]},{"name":["Colin Raffel"]},{"name":["Mohit Bansal"]},{"name":["Diyi Yang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.07499v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.07499v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.00544v2"],"updated":["2021-09-11T09:16:05Z"],"published":["2021-09-01T17:14:26Z"],"title":["Towards Improving Adversarial Training of NLP Models"],"summary":["  Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model's robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models'\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .\n"],"author":[{"name":["Jin Yong Yoo"]},{"name":["Yanjun Qi"]}],"arxiv:comment":[{"_":"EMNLP Findings 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.00544v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.00544v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2110.02467v1"],"updated":["2021-10-06T02:48:58Z"],"published":["2021-10-06T02:48:58Z"],"title":["BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation\n  Models"],"summary":["  Pre-trained Natural Language Processing (NLP) models can be easily adapted to\na variety of downstream language tasks. This significantly accelerates the\ndevelopment of language models. However, NLP models have been shown to be\nvulnerable to backdoor attacks, where a pre-defined trigger word in the input\ntext causes model misprediction. Previous NLP backdoor attacks mainly focus on\nsome specific tasks. This makes those attacks less general and applicable to\nother kinds of NLP models and tasks. In this work, we propose \\Name, the first\ntask-agnostic backdoor attack against the pre-trained NLP models. The key\nfeature of our attack is that the adversary does not need prior information\nabout the downstream tasks when implanting the backdoor to the pre-trained\nmodel. When this malicious model is released, any downstream models transferred\nfrom it will also inherit the backdoor, even after the extensive transfer\nlearning process. We further design a simple yet effective strategy to bypass a\nstate-of-the-art defense. Experimental results indicate that our approach can\ncompromise a wide range of downstream NLP tasks in an effective and stealthy\nway.\n"],"author":[{"name":["Kangjie Chen"]},{"name":["Yuxian Meng"]},{"name":["Xiaofei Sun"]},{"name":["Shangwei Guo"]},{"name":["Tianwei Zhang"]},{"name":["Jiwei Li"]},{"name":["Chun Fan"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2110.02467v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2110.02467v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2110.09132v1"],"updated":["2021-10-18T09:35:40Z"],"published":["2021-10-18T09:35:40Z"],"title":["EmbRace: Accelerating Sparse Communication for Distributed Training of\n  NLP Neural Networks"],"summary":["  Distributed data-parallel training has been widely used for natural language\nprocessing (NLP) neural network models. However, the embedding tables in NLP\nmodels, holding a large portion of parameters and bringing dramatic sparsity in\ncommunication, make it a big challenge to efficiently scale the distributed\ntraining. Current distributed training frameworks mainly concentrate on dense\nmodels but neglect the sparsity of NLP models, resulting in significant\ncommunication overhead and relatively poor scalability.\n  In this paper, we propose EmbRace, an efficient communication framework\ndesigned to accelerate sparse communication of distributed NLP model training.\nEmbRace introduces Sparsity-aware Hybrid Communication, which combines AlltoAll\nand AllReduce to optimize the communication overhead for sparse and dense data\nin NLP models. EmbRace further introduces a 2D Communication Scheduling\napproach to thoroughly overlap communication with computation by optimizing\nmodel computation procedure, relaxing the dependency of embeddings, and\nscheduling communication with a priority queue.\n  We implement EmbRace based on PyTorch and Horovod, and conduct comprehensive\nevaluations with four representative NLP models on two high-performance GPU\nclusters. Experimental results show that EmbRace achieves up to 30.66X speedup\non 16 GPUs clusters among four popular distributed training baselines.\n"],"author":[{"name":["Shengwei Li"]},{"name":["Zhiquan Lai"]},{"name":["Dongsheng Li"]},{"name":["Xiangyu Ye"]},{"name":["Yabo Duan"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2110.09132v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2110.09132v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.MA","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2205.01500v1"],"updated":["2022-05-03T13:58:38Z"],"published":["2022-05-03T13:58:38Z"],"title":["Meta Learning for Natural Language Processing: A Survey"],"summary":["  Deep learning has been the mainstream technique in natural language\nprocessing (NLP) area. However, the techniques require many labeled data and\nare less generalizable across domains. Meta-learning is an arising field in\nmachine learning studying approaches to learn better learning algorithms.\nApproaches aim at improving algorithms in various aspects, including data\nefficiency and generalizability. Efficacy of approaches has been shown in many\nNLP tasks, but there is no systematic survey of these approaches in NLP, which\nhinders more researchers from joining the field. Our goal with this survey\npaper is to offer researchers pointers to relevant meta-learning works in NLP\nand attract more attention from the NLP community to drive future innovation.\nThis paper first introduces the general concepts of meta-learning and the\ncommon approaches. Then we summarize task construction settings and application\nof meta-learning for various NLP problems and review the development of\nmeta-learning in NLP community.\n"],"author":[{"name":["Hung-yi Lee"]},{"name":["Shang-Wen Li"]},{"name":["Ngoc Thang Vu"]}],"arxiv:comment":[{"_":"Accepted by NAACL 2022","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2205.01500v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2205.01500v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2110.03353v1"],"updated":["2021-10-07T11:45:31Z"],"published":["2021-10-07T11:45:31Z"],"title":["Noisy Text Data: Achilles' Heel of popular transformer based NLP models"],"summary":["  In the last few years, the ML community has created a number of new NLP\nmodels based on transformer architecture. These models have shown great\nperformance for various NLP tasks on benchmark datasets, often surpassing SOTA\nresults. Buoyed with this success, one often finds industry practitioners\nactively experimenting with fine-tuning these models to build NLP applications\nfor industry use cases. However, for most datasets that are used by\npractitioners to build industrial NLP applications, it is hard to guarantee the\npresence of any noise in the data. While most transformer based NLP models have\nperformed exceedingly well in transferring the learnings from one dataset to\nanother, it remains unclear how these models perform when fine-tuned on noisy\ntext. We address the open question by Kumar et al. (2020) to explore the\nsensitivity of popular transformer based NLP models to noise in the text data.\nWe continue working with the noise as defined by them -- spelling mistakes &\ntypos (which are the most commonly occurring noise). We show (via experimental\nresults) that these models perform badly on most common NLP tasks namely text\nclassification, textual similarity, NER, question answering, text summarization\non benchmark datasets. We further show that as the noise in data increases, the\nperformance degrades. Our findings suggest that one must be vary of the\npresence of noise in their datasets while fine-tuning popular transformer based\nNLP models.\n"],"author":[{"name":["Kartikay Bagla"]},{"name":["Ankit Kumar"]},{"name":["Shivam Gupta"]},{"name":["Anuj Gupta"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2110.03353v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2110.03353v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2111.08408v1"],"updated":["2021-11-16T12:20:47Z"],"published":["2021-11-16T12:20:47Z"],"title":["STAMP 4 NLP -- An Agile Framework for Rapid Quality-Driven NLP\n  Applications Development"],"summary":["  The progress in natural language processing (NLP) research over the last\nyears, offers novel business opportunities for companies, as automated user\ninteraction or improved data analysis. Building sophisticated NLP applications\nrequires dealing with modern machine learning (ML) technologies, which impedes\nenterprises from establishing successful NLP projects. Our experience in\napplied NLP research projects shows that the continuous integration of research\nprototypes in production-like environments with quality assurance builds trust\nin the software and shows convenience and usefulness regarding the business\ngoal. We introduce STAMP 4 NLP as an iterative and incremental process model\nfor developing NLP applications. With STAMP 4 NLP, we merge software\nengineering principles with best practices from data science. Instantiating our\nprocess model allows efficiently creating prototypes by utilizing templates,\nconventions, and implementations, enabling developers and data scientists to\nfocus on the business goals. Due to our iterative-incremental approach,\nbusinesses can deploy an enhanced version of the prototype to their software\nenvironment after every iteration, maximizing potential business value and\ntrust early and avoiding the cost of successful yet never deployed experiments.\n"],"author":[{"name":["Philipp Kohl"]},{"name":["Oliver Schmidts"]},{"name":["Lars Klöser"]},{"name":["Henri Werth"]},{"name":["Bodo Kraft"]},{"name":["Albert Zündorf"]}],"arxiv:doi":[{"_":"10.1007/978-3-030-85347-1_12","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/978-3-030-85347-1_12","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2111.08408v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2111.08408v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Preprint of short paper for QUATIC 2021 conference","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Quality of Information and Communications Technology, 2021, p.\n  156-166","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1809.01448v1"],"updated":["2018-09-05T11:55:05Z"],"published":["2018-09-05T11:55:05Z"],"title":["Appendix - Recommended Statistical Significance Tests for NLP Tasks"],"summary":["  Statistical significance testing plays an important role when drawing\nconclusions from experimental results in NLP papers. Particularly, it is a\nvaluable tool when one would like to establish the superiority of one algorithm\nover another. This appendix complements the guide for testing statistical\nsignificance in NLP presented in \\cite{dror2018hitchhiker} by proposing valid\nstatistical tests for the common tasks and evaluation measures in the field.\n"],"author":[{"name":["Rotem Dror"]},{"name":["Roi Reichart"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1809.01448v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1809.01448v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.03732v1"],"updated":["2021-02-07T07:37:07Z"],"published":["2021-02-07T07:37:07Z"],"title":["Representation Learning for Natural Language Processing"],"summary":["  This book aims to review and present the recent advances of distributed\nrepresentation learning for NLP, including why representation learning can\nimprove NLP, how representation learning takes part in various important topics\nof NLP, and what challenges are still not well addressed by distributed\nrepresentation.\n"],"author":[{"name":["Zhiyuan Liu"]},{"name":["Yankai Lin"]},{"name":["Maosong Sun"]}],"arxiv:doi":[{"_":"10.1007/978-981-15-5573-2","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/978-981-15-5573-2","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2102.03732v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.03732v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Published in Springer","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.02756v1"],"updated":["2021-04-06T19:34:36Z"],"published":["2021-04-06T19:34:36Z"],"title":["Efficient transfer learning for NLP with ELECTRA"],"summary":["  Clark et al. [2020] claims that the ELECTRA approach is highly efficient in\nNLP performances relative to computation budget. As such, this reproducibility\nstudy focus on this claim, summarized by the following question: Can we use\nELECTRA to achieve close to SOTA performances for NLP in low-resource settings,\nin term of compute cost?\n"],"author":[{"name":["François Mercier"]}],"arxiv:comment":[{"_":"Submission for ML Reproducibility Challenge 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Machine Learning Reproducibility Challenge 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.02756v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.02756v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.12405v2"],"updated":["2021-05-14T07:59:50Z"],"published":["2021-04-26T09:00:56Z"],"title":["A dissemination workshop for introducing young Italian students to NLP"],"summary":["  We describe and make available the game-based material developed for a\nlaboratory run at several Italian science festivals to popularize NLP among\nyoung students.\n"],"author":[{"name":["Lucio Messina"]},{"name":["Lucia Busso"]},{"name":["Claudia Roberta Combei"]},{"name":["Ludovica Pannitto"]},{"name":["Alessio Miaschi"]},{"name":["Gabriele Sarti"]},{"name":["Malvina Nissim"]}],"arxiv:comment":[{"_":"3 pages, 4 figures, accepted at Teaching NLP 2021 workshop","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.12405v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.12405v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.01052v1"],"updated":["2021-05-03T17:51:17Z"],"published":["2021-05-03T17:51:17Z"],"title":["Applied Language Technology: NLP for the Humanities"],"summary":["  This contribution describes a two-course module that seeks to provide\nhumanities majors with a basic understanding of language technology and its\napplications using Python. The learning materials consist of interactive\nJupyter Notebooks and accompanying YouTube videos, which are openly available\nwith a Creative Commons licence.\n"],"author":[{"name":["Tuomo Hiippala"]}],"arxiv:doi":[{"_":"10.18653/v1/2021.teachingnlp-1.5","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.18653/v1/2021.teachingnlp-1.5","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2105.01052v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.01052v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings of the Fifth Workshop on Teaching NLP, 2021, pp. 46-48","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1702.01923v1"],"updated":["2017-02-07T08:33:35Z"],"published":["2017-02-07T08:33:35Z"],"title":["Comparative Study of CNN and RNN for Natural Language Processing"],"summary":["  Deep neural networks (DNN) have revolutionized the field of natural language\nprocessing (NLP). Convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), the two main types of DNN architectures, are widely explored to\nhandle various NLP tasks. CNN is supposed to be good at extracting\nposition-invariant features and RNN at modeling units in sequence. The state of\nthe art on many NLP tasks often switches due to the battle between CNNs and\nRNNs. This work is the first systematic comparison of CNN and RNN on a wide\nrange of representative NLP tasks, aiming to give basic guidance for DNN\nselection.\n"],"author":[{"name":["Wenpeng Yin"]},{"name":["Katharina Kann"]},{"name":["Mo Yu"]},{"name":["Hinrich Schütze"]}],"arxiv:comment":[{"_":"7 pages, 11 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1702.01923v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1702.01923v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1801.06422v3"],"updated":["2019-05-06T12:28:05Z"],"published":["2018-01-19T14:41:45Z"],"title":["Evaluating neural network explanation methods using hybrid documents and\n  morphological agreement"],"summary":["  The behavior of deep neural networks (DNNs) is hard to understand. This makes\nit necessary to explore post hoc explanation methods. We conduct the first\ncomprehensive evaluation of explanation methods for NLP. To this end, we design\ntwo novel evaluation paradigms that cover two important classes of NLP\nproblems: small context and large context problems. Both paradigms require no\nmanual annotation and are therefore broadly applicable. We also introduce\nLIMSSE, an explanation method inspired by LIME that is designed for NLP. We\nshow empirically that LIMSSE, LRP and DeepLIFT are the most effective\nexplanation methods and recommend them for explaining DNNs in NLP.\n"],"author":[{"name":["Nina Poerner"]},{"name":["Benjamin Roth"]},{"name":["Hinrich Schütze"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1801.06422v3","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1801.06422v3","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1708.05148v1"],"updated":["2017-08-17T06:42:03Z"],"published":["2017-08-17T06:42:03Z"],"title":["Natural Language Processing: State of The Art, Current Trends and\n  Challenges"],"summary":["  Natural language processing (NLP) has recently gained much attention for\nrepresenting and analysing human language computationally. It has spread its\napplications in various fields such as machine translation, email spam\ndetection, information extraction, summarization, medical, and question\nanswering etc. The paper distinguishes four phases by discussing different\nlevels of NLP and components of Natural Language Generation (NLG) followed by\npresenting the history and evolution of NLP, state of the art presenting the\nvarious applications of NLP and current trends and challenges.\n"],"author":[{"name":["Diksha Khurana"]},{"name":["Aditya Koli"]},{"name":["Kiran Khatter"]},{"name":["Sukhdev Singh"]}],"arxiv:comment":[{"_":"25 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1708.05148v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1708.05148v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1711.01505v1"],"updated":["2017-11-04T22:46:54Z"],"published":["2017-11-04T22:46:54Z"],"title":["Towards Linguistically Generalizable NLP Systems: A Workshop and Shared\n  Task"],"summary":["  This paper presents a summary of the first Workshop on Building\nLinguistically Generalizable Natural Language Processing Systems, and the\nassociated Build It Break It, The Language Edition shared task. The goal of\nthis workshop was to bring together researchers in NLP and linguistics with a\nshared task aimed at testing the generalizability of NLP systems beyond the\ndistributions of their training data. We describe the motivation, setup, and\nparticipation of the shared task, provide discussion of some highlighted\nresults, and discuss lessons learned.\n"],"author":[{"name":["Allyson Ettinger"]},{"name":["Sudha Rao"]},{"name":["Hal Daumé III"]},{"name":["Emily M. Bender"]}],"arxiv:comment":[{"_":"Updated version of the EMNLP Workshop and Shared Task description\n  paper, Proceedings of the First Workshop on Building Linguistically\n  Generalizable NLP Systems. 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1711.01505v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1711.01505v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2002.03056v1"],"updated":["2020-02-08T00:42:21Z"],"published":["2020-02-08T00:42:21Z"],"title":["autoNLP: NLP Feature Recommendations for Text Analytics Applications"],"summary":["  While designing machine learning based text analytics applications, often,\nNLP data scientists manually determine which NLP features to use based upon\ntheir knowledge and experience with related problems. This results in increased\nefforts during feature engineering process and renders automated reuse of\nfeatures across semantically related applications inherently difficult. In this\npaper, we argue for standardization in feature specification by outlining\nstructure of a language for specifying NLP features and present an approach for\ntheir reuse across applications to increase likelihood of identifying optimal\nfeatures.\n"],"author":[{"name":["Janardan Misra"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2002.03056v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2002.03056v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.13461v1"],"updated":["2021-02-26T13:37:10Z"],"published":["2021-02-26T13:37:10Z"],"title":["Methods for the Design and Evaluation of HCI+NLP Systems"],"summary":["  HCI and NLP traditionally focus on different evaluation methods. While HCI\ninvolves a small number of people directly and deeply, NLP traditionally relies\non standardized benchmark evaluations that involve a larger number of people\nindirectly. We present five methodological proposals at the intersection of HCI\nand NLP and situate them in the context of ML-based NLP models. Our goal is to\nfoster interdisciplinary collaboration and progress in both fields by\nemphasizing what the fields can learn from each other.\n"],"author":[{"name":["Hendrik Heuer"]},{"name":["Daniel Buschek"]}],"arxiv:comment":[{"_":"Accepted at the EACL 2021 Workshop on Bridging Human-Computer\n  Interaction and Natural Language Processing (HCI+NLP Workshop)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2102.13461v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.13461v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.HC","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.02746v1"],"updated":["2021-05-06T15:15:54Z"],"published":["2021-05-06T15:15:54Z"],"title":["Introducing Information Retrieval for Biomedical Informatics Students"],"summary":["  Introducing biomedical informatics (BMI) students to natural language\nprocessing (NLP) requires balancing technical depth with practical know-how to\naddress application-focused needs. We developed a set of three activities\nintroducing introductory BMI students to information retrieval with NLP,\ncovering document representation strategies and language models from TF-IDF to\nBERT. These activities provide students with hands-on experience targeted\ntowards common use cases, and introduce fundamental components of NLP workflows\nfor a wide variety of applications.\n"],"author":[{"name":["Sanya B. Taneja"]},{"name":["Richard D. Boyce"]},{"name":["William T. Reynolds"]},{"name":["Denis Newman-Griffis"]}],"arxiv:comment":[{"_":"To appear in the Proceedings of the Fifth Workshop on Teaching NLP @\n  NAACL","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.02746v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.02746v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.01167v1"],"updated":["2021-06-02T14:03:06Z"],"published":["2021-06-02T14:03:06Z"],"title":["End-to-End NLP Knowledge Graph Construction"],"summary":["  This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)\nfrom scientific papers. We focus on extracting four types of relations:\nevaluatedOn between tasks and datasets, evaluatedBy between tasks and\nevaluation metrics, as well as coreferent and related relations between the\nsame type of entities. For instance, F1-score is coreferent with F-measure. We\nintroduce novel methods for each of these relation types and apply our final\nframework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a\nlarge-scale KG, which can facilitate automatically constructing scientific\nleaderboards for the NLP community. The results of our experiments indicate\nthat the resulting KG contains high-quality information.\n"],"author":[{"name":["Ishani Mondal"]},{"name":["Yufang Hou"]},{"name":["Charles Jochim"]}],"arxiv:comment":[{"_":"Accepted in ACL 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.01167v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.01167v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.01211v1"],"updated":["2021-09-02T21:00:17Z"],"published":["2021-09-02T21:00:17Z"],"title":["Quantifying Reproducibility in NLP and ML"],"summary":["  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n"],"author":[{"name":["Anya Belz"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.01211v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.01211v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2201.08066v1"],"updated":["2022-01-20T09:05:34Z"],"published":["2022-01-20T09:05:34Z"],"title":["NLP Methods in Host-based Intrusion Detection Systems: A Systematic\n  Review and Future Directions"],"summary":["  The Host-Based Intrusion Detection Systems (HIDS) are widely used for\ndefending against cybersecurity attacks. An increasing number of HIDS have\nstarted leveraging the advances in Natural Language Processing (NLP)\ntechnologies that have shown promising results in precisely detecting low\nfootprint, zero-day attacks and predict attacker's next steps. We conduct a\nsystematic review of the literature on NLP-based HIDS in order to build a\nsystematized body of knowledge. We develop an NLP-based HIDS taxonomy for\ncomparing the features, techniques, attacks, datasets, and metrics found from\nthe reviewed papers. We highlight the prevalent practices and the future\nresearch areas.\n"],"author":[{"name":["Zarrin Tasnim Sworna"]},{"name":["Zahra Mousavi"]},{"name":["Muhammad Ali Babar"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2201.08066v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2201.08066v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2111.08529v1"],"updated":["2021-11-16T14:58:05Z"],"published":["2021-11-16T14:58:05Z"],"title":["Improving the robustness and accuracy of biomedical language models\n  through adversarial training"],"summary":["  Deep transformer neural network models have improved the predictive accuracy\nof intelligent text processing systems in the biomedical domain. They have\nobtained state-of-the-art performance scores on a wide variety of biomedical\nand clinical Natural Language Processing (NLP) benchmarks. However, the\nrobustness and reliability of these models has been less explored so far.\nNeural NLP models can be easily fooled by adversarial samples, i.e. minor\nchanges to input that preserve the meaning and understandability of the text\nbut force the NLP system to make erroneous decisions. This raises serious\nconcerns about the security and trust-worthiness of biomedical NLP systems,\nespecially when they are intended to be deployed in real-world use cases. We\ninvestigated the robustness of several transformer neural language models, i.e.\nBioBERT, SciBERT, BioMed-RoBERTa, and Bio-ClinicalBERT, on a wide range of\nbiomedical and clinical text processing tasks. We implemented various\nadversarial attack methods to test the NLP systems in different attack\nscenarios. Experimental results showed that the biomedical NLP models are\nsensitive to adversarial samples; their performance dropped in average by 21\nand 18.9 absolute percent on character-level and word-level adversarial noise,\nrespectively. Conducting extensive adversarial training experiments, we\nfine-tuned the NLP models on a mixture of clean samples and adversarial inputs.\nResults showed that adversarial training is an effective defense mechanism\nagainst adversarial noise; the models robustness improved in average by 11.3\nabsolute percent. In addition, the models performance on clean data increased\nin average by 2.4 absolute present, demonstrating that adversarial training can\nboost generalization abilities of biomedical NLP systems.\n"],"author":[{"name":["Milad Moradi"]},{"name":["Matthias Samwald"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2111.08529v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2111.08529v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1401.0569v2"],"updated":["2014-01-08T19:00:04Z"],"published":["2014-01-03T00:57:13Z"],"title":["Natural Language Processing in Biomedicine: A Unified System\n  Architecture Overview"],"summary":["  In modern electronic medical records (EMR) much of the clinically important\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\nprovided in structured data fields, but rather are encoded in clinician\ngenerated narrative text. Natural language processing (NLP) provides a means of\n\"unlocking\" this important data source for applications in clinical decision\nsupport, quality assurance, and public health. This chapter provides an\noverview of representative NLP systems in biomedicine based on a unified\narchitectural view. A general architecture in an NLP system consists of two\nmain components: background knowledge that includes biomedical knowledge\nresources and a framework that integrates NLP tools to process text. Systems\ndiffer in both components, which we will review briefly. Additionally,\nchallenges facing current research efforts in biomedical NLP include the\npaucity of large, publicly available annotated corpora, although initiatives\nthat facilitate data sharing, system evaluation, and collaborative work between\nresearchers in clinical NLP are starting to emerge.\n"],"author":[{"name":["Son Doan"]},{"name":["Mike Conway"]},{"name":["Tu Minh Phuong"]},{"name":["Lucila Ohno-Machado"]}],"arxiv:doi":[{"_":"10.1007/978-1-4939-0847-9_16","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/978-1-4939-0847-9_16","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1401.0569v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1401.0569v2","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"25 pages, 5 figures, book chapter in Clinical Bioinformatics, 2014,\n  edited by Ronand Trent","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1503.00168v1"],"updated":["2015-02-28T19:46:50Z"],"published":["2015-02-28T19:46:50Z"],"title":["The NLP Engine: A Universal Turing Machine for NLP"],"summary":["  It is commonly accepted that machine translation is a more complex task than\npart of speech tagging. But how much more complex? In this paper we make an\nattempt to develop a general framework and methodology for computing the\ninformational and/or processing complexity of NLP applications and tasks. We\ndefine a universal framework akin to a Turning Machine that attempts to fit\n(most) NLP tasks into one paradigm. We calculate the complexities of various\nNLP tasks using measures of Shannon Entropy, and compare `simple' ones such as\npart of speech tagging to `complex' ones such as machine translation. This\npaper provides a first, though far from perfect, attempt to quantify NLP tasks\nunder a uniform paradigm. We point out current deficiencies and suggest some\navenues for fruitful research.\n"],"author":[{"name":["Jiwei Li"]},{"name":["Eduard Hovy"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1503.00168v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1503.00168v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1707.04244v1"],"updated":["2017-07-13T17:52:51Z"],"published":["2017-07-13T17:52:51Z"],"title":["Lithium NLP: A System for Rich Information Extraction from Noisy User\n  Generated Text on Social Media"],"summary":["  In this paper, we describe the Lithium Natural Language Processing (NLP)\nsystem - a resource-constrained, high- throughput and language-agnostic system\nfor information extraction from noisy user generated text on social media.\nLithium NLP extracts a rich set of information including entities, topics,\nhashtags and sentiment from text. We discuss several real world applications of\nthe system currently incorporated in Lithium products. We also compare our\nsystem with existing commercial and academic NLP systems in terms of\nperformance, information extracted and languages supported. We show that\nLithium NLP is at par with and in some cases, outperforms state- of-the-art\ncommercial NLP systems.\n"],"author":[{"name":["Preeti Bhargava"]},{"name":["Nemanja Spasojevic"]},{"name":["Guoning Hu"]}],"arxiv:comment":[{"_":"9 pages, 6 figures, 2 tables, EMNLP 2017 Workshop on Noisy User\n  Generated Text WNUT 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1707.04244v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1707.04244v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1807.00571v1"],"updated":["2018-07-02T09:53:50Z"],"published":["2018-07-02T09:53:50Z"],"title":["The Interplay between Lexical Resources and Natural Language Processing"],"summary":["  Incorporating linguistic, world and common sense knowledge into AI/NLP\nsystems is currently an important research area, with several open problems and\nchallenges. At the same time, processing and storing this knowledge in lexical\nresources is not a straightforward task. This tutorial proposes to address\nthese complementary goals from two methodological perspectives: the use of NLP\nmethods to help the process of constructing and enriching lexical resources and\nthe use of lexical resources for improving NLP applications. Two main types of\naudience can benefit from this tutorial: those working on language resources\nwho are interested in becoming acquainted with automatic NLP techniques, with\nthe end goal of speeding and/or easing up the process of resource curation; and\non the other hand, researchers in NLP who would like to benefit from the\nknowledge of lexical resources to improve their systems and models. The slides\nof the tutorial are available at https://bitbucket.org/luisespinosa/lr-nlp/\n"],"author":[{"name":["Jose Camacho-Collados"]},{"name":["Luis Espinosa-Anke"]},{"name":["Mohammad Taher Pilehvar"]}],"arxiv:comment":[{"_":"NAACL 2018 Tutorial","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings of the 2018 Conference of the North American Chapter\n  of the Association for Computational Linguistics: Tutorial Abstracts","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1807.00571v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1807.00571v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1905.08741v1"],"updated":["2019-05-21T16:37:36Z"],"published":["2019-05-21T16:37:36Z"],"title":["Next-to-leading power threshold effects for inclusive and exclusive\n  processes with final state jets"],"summary":["  It is well known that cross-sections in perturbative QCD receive large\ncorrections from soft and collinear radiation, whose properties must be\nresummed to all orders in the coupling. Whether or not the universal properties\nof this radiation can be extended to next-to-leading power (NLP) in the\nthreshold expansion has been the subject of much recent study. In this paper,\nwe consider two types of NLP effects: the interplay of next-to-soft and\ncollinear radiation in processes with final state jets and the NLP\ncontributions stemming from soft quarks. We derive an NLP amplitude for soft\ngluons and quarks, valid for an arbitrary number of coloured or colourless\nmassless final state particles. We show explicitly that this framework can be\nused to correctly obtain the dominant NLP effects in three different types of\nprocesses at next-to-leading order: deep-inelastic scattering, hadroproduction\nvia electron-positron annihilation and prompt photon production. Our results\nprovide an important ingredient for developing a universal resummation\nformalism for NLP effects.\n"],"author":[{"name":["Melissa van Beekveld"]},{"name":["Wim Beenakker"]},{"name":["Eric Laenen"]},{"name":["Chris D. White"]}],"arxiv:doi":[{"_":"10.1007/JHEP03(2020)106","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/JHEP03(2020)106","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1905.08741v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1905.08741v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2007.05872v1"],"updated":["2020-07-11T22:56:37Z"],"published":["2020-07-11T22:56:37Z"],"title":["Is Machine Learning Speaking my Language? A Critical Look at the\n  NLP-Pipeline Across 8 Human Languages"],"summary":["  Natural Language Processing (NLP) is increasingly used as a key ingredient in\ncritical decision-making systems such as resume parsers used in sorting a list\nof job candidates. NLP systems often ingest large corpora of human text,\nattempting to learn from past human behavior and decisions in order to produce\nsystems that will make recommendations about our future world. Over 7000 human\nlanguages are being spoken today and the typical NLP pipeline underrepresents\nspeakers of most of them while amplifying the voices of speakers of other\nlanguages. In this paper, a team including speakers of 8 languages - English,\nChinese, Urdu, Farsi, Arabic, French, Spanish, and Wolof - takes a critical\nlook at the typical NLP pipeline and how even when a language is technically\nsupported, substantial caveats remain to prevent full participation. Despite\nhuge and admirable investments in multilingual support in many tools and\nresources, we are still making NLP-guided decisions that systematically and\ndramatically underrepresent the voices of much of the world.\n"],"author":[{"name":["Esma Wali"]},{"name":["Yan Chen"]},{"name":["Christopher Mahoney"]},{"name":["Thomas Middleton"]},{"name":["Marzieh Babaeianjelodar"]},{"name":["Mariama Njie"]},{"name":["Jeanna Neefe Matthews"]}],"arxiv:comment":[{"_":"Participatory Approaches to Machine Learning Workshop, 37th\n  International Conference on Machine Learning","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2007.05872v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2007.05872v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"I.2.7","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2010.01724v1"],"updated":["2020-10-05T00:23:00Z"],"published":["2020-10-05T00:23:00Z"],"title":["TextAttack: Lessons learned in designing Python frameworks for NLP"],"summary":["  TextAttack is an open-source Python toolkit for adversarial attacks,\nadversarial training, and data augmentation in NLP. TextAttack unites 15+\npapers from the NLP adversarial attack literature into a single framework, with\nmany components reused across attacks. This framework allows both researchers\nand developers to test and study the weaknesses of their NLP models. To build\nsuch an open-source NLP toolkit requires solving some common problems: How do\nwe enable users to supply models from different deep learning frameworks? How\ncan we build tools to support as many different datasets as possible? We share\nour insights into developing a well-written, well-documented NLP Python\nframework in hope that they can aid future development of similar packages.\n"],"author":[{"name":["John X. Morris"]},{"name":["Jin Yong Yoo"]},{"name":["Yanjun Qi"]}],"arxiv:comment":[{"_":"4 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2010.01724v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2010.01724v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2011.04372v1"],"updated":["2020-11-09T12:13:55Z"],"published":["2020-11-09T12:13:55Z"],"title":["Low-Resource Adaptation of Neural NLP Models"],"summary":["  Real-world applications of natural language processing (NLP) are challenging.\nNLP models rely heavily on supervised machine learning and require large\namounts of annotated data. These resources are often based on language data\navailable in large quantities, such as English newswire. However, in real-world\napplications of NLP, the textual resources vary across several dimensions, such\nas language, dialect, topic, and genre. It is challenging to find annotated\ndata of sufficient amount and quality. The objective of this thesis is to\ninvestigate methods for dealing with such low-resource scenarios in information\nextraction and natural language understanding. To this end, we study distant\nsupervision and sequential transfer learning in various low-resource settings.\nWe develop and adapt neural NLP models to explore a number of research\nquestions concerning NLP tasks with minimal or no training data.\n"],"author":[{"name":["Farhad Nooralahzadeh"]}],"arxiv:comment":[{"_":"Thesis submitted for the degree of Philosophiae Doctor. Department of\n  Informatics, University of Oslo.\n  https://www.mn.uio.no/ifi/forskning/aktuelt/arrangementer/disputaser/2020/nooralahzadeh.html","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2011.04372v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2011.04372v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2101.07270v1"],"updated":["2021-01-18T19:00:01Z"],"published":["2021-01-18T19:00:01Z"],"title":["Next-to-leading power threshold corrections for finite order and\n  resummed colour-singlet cross sections"],"summary":["  We study next-to-leading-power (NLP) threshold corrections in colour-singlet\nproduction processes, with particular emphasis on Drell-Yan (DY) and\nsingle-Higgs production. We assess the quality of the partonic and hadronic\nthreshold expansions for each process up to NNLO. We determine numerically the\nNLP leading-logarithmic (LL) resummed contribution in addition to the\nleading-power next-to-next-to-leading logarithmic (LP NNLL) resummed DY and\nHiggs cross sections, matched to NNLO. We find that the inclusion of NLP\nlogarithms is numerically more relevant than increasing the precision to\nN$^3$LL at LP for these processes. We also perform an analytical and numerical\ncomparison of LP NNLL + NLP LL resummation in soft-collinear effective theory\nand direct QCD, where we achieve excellent analytical and numerical agreement\nonce the NLP LL terms are included in both formalisms. Our results underline\nthe phenomenological importance of understanding the NLP structure of QCD cross\nsections.\n"],"author":[{"name":["Melissa van Beekveld"]},{"name":["Eric Laenen"]},{"name":["Jort Sinninghe Damsté"]},{"name":["Leonardo Vernazza"]}],"arxiv:doi":[{"_":"10.1007/JHEP05(2021)114","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/JHEP05(2021)114","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2101.07270v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2101.07270v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Code available at github.com/melli1992/resummation","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2103.04044v1"],"updated":["2021-03-06T06:26:00Z"],"published":["2021-03-06T06:26:00Z"],"title":["Putting Humans in the Natural Language Processing Loop: A Survey"],"summary":["  How can we design Natural Language Processing (NLP) systems that learn from\nhuman feedback? There is a growing research body of Human-in-the-loop (HITL)\nNLP frameworks that continuously integrate human feedback to improve the model\nitself. HITL NLP research is nascent but multifarious -- solving various NLP\nproblems, collecting diverse feedback from different people, and applying\ndifferent methods to learn from collected feedback. We present a survey of HITL\nNLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI)\ncommunities that highlights its short yet inspiring history, and thoroughly\nsummarize recent frameworks focusing on their tasks, goals, human interactions,\nand feedback learning methods. Finally, we discuss future directions for\nintegrating human feedback in the NLP development loop.\n"],"author":[{"name":["Zijie J. Wang"]},{"name":["Dongjin Choi"]},{"name":["Shenyu Xu"]},{"name":["Diyi Yang"]}],"arxiv:comment":[{"_":"The paper is accepted to the HCI+NLP workshop at EACL 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2103.04044v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2103.04044v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.HC","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2103.06944v2"],"updated":["2021-03-23T22:02:14Z"],"published":["2021-03-11T20:44:31Z"],"title":["Preregistering NLP Research"],"summary":["  Preregistration refers to the practice of specifying what you are going to\ndo, and what you expect to find in your study, before carrying out the study.\nThis practice is increasingly common in medicine and psychology, but is rarely\ndiscussed in NLP. This paper discusses preregistration in more detail, explores\nhow NLP researchers could preregister their work, and presents several\npreregistration questions for different kinds of studies. Finally, we argue in\nfavour of registered reports, which could provide firmer grounds for slow\nscience in NLP research. The goal of this paper is to elicit a discussion in\nthe NLP community, which we hope to synthesise into a general NLP\npreregistration form in future research.\n"],"author":[{"name":["Emiel van Miltenburg"]},{"name":["Chris van der Lee"]},{"name":["Emiel Krahmer"]}],"arxiv:comment":[{"_":"Accepted at NAACL2021; pre-final draft, comments welcome","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2103.06944v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2103.06944v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2203.06414v2"],"updated":["2022-04-12T06:43:05Z"],"published":["2022-03-12T11:37:17Z"],"title":["A survey in Adversarial Defences and Robustness in NLP"],"summary":["  In recent years, it has been seen that deep neural networks are lacking\nrobustness and are likely to break in case of adversarial perturbations in\ninput data. Strong adversarial attacks are proposed by various authors for\ncomputer vision and Natural Language Processing (NLP). As a counter-effort,\nseveral defense mechanisms are also proposed to save these networks from\nfailing. In contrast with image data, generating adversarial attacks and\ndefending these models is not easy in NLP because of the discrete nature of the\ntext data. However, numerous methods for adversarial defense are proposed of\nlate, for different NLP tasks such as text classification, named entity\nrecognition, natural language inferencing, etc. These methods are not just used\nfor defending neural networks from adversarial attacks, but also used as a\nregularization mechanism during training, saving the model from overfitting.\nThe proposed survey is an attempt to review different methods proposed for\nadversarial defenses in NLP in the recent past by proposing a novel taxonomy.\nThis survey also highlights the fragility of the advanced deep neural networks\nin NLP and the challenges in defending them.\n"],"author":[{"name":["Shreya Goyal"]},{"name":["Sumanth Doddapaneni"]},{"name":["Mitesh M. Khapra"]},{"name":["Balaraman Ravindran"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2203.06414v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2203.06414v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2203.13357v1"],"updated":["2022-03-24T22:07:22Z"],"published":["2022-03-24T22:07:22Z"],"title":["One Country, 700+ Languages: NLP Challenges for Underrepresented\n  Languages and Dialects in Indonesia"],"summary":["  NLP research is impeded by a lack of resources and awareness of the\nchallenges presented by underrepresented languages and dialects. Focusing on\nthe languages spoken in Indonesia, the second most linguistically diverse and\nthe fourth most populous nation of the world, we provide an overview of the\ncurrent state of NLP research for Indonesia's 700+ languages. We highlight\nchallenges in Indonesian NLP and how these affect the performance of current\nNLP systems. Finally, we provide general recommendations to help develop NLP\ntechnology not only for languages of Indonesia but also other underrepresented\nlanguages.\n"],"author":[{"name":["Alham Fikri Aji"]},{"name":["Genta Indra Winata"]},{"name":["Fajri Koto"]},{"name":["Samuel Cahyawijaya"]},{"name":["Ade Romadhony"]},{"name":["Rahmad Mahendra"]},{"name":["Kemal Kurniawan"]},{"name":["David Moeljadi"]},{"name":["Radityo Eko Prasojo"]},{"name":["Timothy Baldwin"]},{"name":["Jey Han Lau"]},{"name":["Sebastian Ruder"]}],"arxiv:comment":[{"_":"Accepted in ACL 2022","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2203.13357v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2203.13357v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2205.05071v2"],"updated":["2022-05-11T18:18:41Z"],"published":["2022-05-10T17:56:23Z"],"title":["Towards Climate Awareness in NLP Research"],"summary":["  The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.\n"],"author":[{"name":["Daniel Hershcovich"]},{"name":["Nicolas Webersinke"]},{"name":["Mathias Kraus"]},{"name":["Julia Anna Bingler"]},{"name":["Markus Leippold"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2205.05071v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2205.05071v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1810.01048v1"],"updated":["2018-10-02T03:11:55Z"],"published":["2018-10-02T03:11:55Z"],"title":["Privacy-Preserving Outsourcing of Large-Scale Nonlinear Programming to\n  the Cloud"],"summary":["  The increasing massive data generated by various sources has given birth to\nbig data analytics. Solving large-scale nonlinear programming problems (NLPs)\nis one important big data analytics task that has applications in many domains\nsuch as transport and logistics. However, NLPs are usually too computationally\nexpensive for resource-constrained users. Fortunately, cloud computing provides\nan alternative and economical service for resource-constrained users to\noutsource their computation tasks to the cloud. However, one major concern with\noutsourcing NLPs is the leakage of user's private information contained in NLP\nformulations and results. Although much work has been done on\nprivacy-preserving outsourcing of computation tasks, little attention has been\npaid to NLPs. In this paper, we for the first time investigate secure\noutsourcing of general large-scale NLPs with nonlinear constraints. A secure\nand efficient transformation scheme at the user side is proposed to protect\nuser's private information; at the cloud side, generalized reduced gradient\nmethod is applied to effectively solve the transformed large-scale NLPs. The\nproposed protocol is implemented on a cloud computing testbed. Experimental\nevaluations demonstrate that significant time can be saved for users and the\nproposed mechanism has the potential for practical use.\n"],"author":[{"name":["Ang Li"]},{"name":["Wei Du"]},{"name":["Qinghua Li"]}],"arxiv:doi":[{"_":"10.1007/978-3-030-01701-9_31","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/978-3-030-01701-9_31","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1810.01048v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1810.01048v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"Ang Li and Wei Du equally contributed to this work. This work was\n  done when Wei Du was at the University of Arkansas. 2018 EAI International\n  Conference on Security and Privacy in Communication Networks (SecureComm)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CR","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CR","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2004.08333v2"],"updated":["2020-04-26T18:54:36Z"],"published":["2020-04-17T16:25:36Z"],"title":["Natural Language Processing with Deep Learning for Medical Adverse Event\n  Detection from Free-Text Medical Narratives: A Case Study of Detecting Total\n  Hip Replacement Dislocation"],"summary":["  Accurate and timely detection of medical adverse events (AEs) from free-text\nmedical narratives is challenging. Natural language processing (NLP) with deep\nlearning has already shown great potential for analyzing free-text data, but\nits application for medical AE detection has been limited. In this study we\nproposed deep learning based NLP (DL-NLP) models for efficient and accurate hip\ndislocation AE detection following total hip replacement from standard\n(radiology notes) and non-standard (follow-up telephone notes) free-text\nmedical narratives. We benchmarked these proposed models with a wide variety of\ntraditional machine learning based NLP (ML-NLP) models, and also assessed the\naccuracy of International Classification of Diseases (ICD) and Current\nProcedural Terminology (CPT) codes in capturing these hip dislocation AEs in a\nmulti-center orthopaedic registry. All DL-NLP models out-performed all of the\nML-NLP models, with a convolutional neural network (CNN) model achieving the\nbest overall performance (Kappa = 0.97 for radiology notes, and Kappa = 1.00\nfor follow-up telephone notes). On the other hand, the ICD/CPT codes of the\npatients who sustained a hip dislocation AE were only 75.24% accurate, showing\nthe potential of the proposed model to be used in largescale orthopaedic\nregistries for accurate and efficient hip dislocation AE detection to improve\nthe quality of care and patient outcome.\n"],"author":[{"name":["Alireza Borjali"]},{"name":["Martin Magneli"]},{"name":["David Shin"]},{"name":["Henrik Malchau"]},{"name":["Orhun K. Muratoglu"]},{"name":["Kartik M. Varadarajan"]}],"arxiv:doi":[{"_":"10.1016/j.compbiomed.2020.104140","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1016/j.compbiomed.2020.104140","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2004.08333v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2004.08333v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2012.00633v1"],"updated":["2020-12-01T16:58:01Z"],"published":["2020-12-01T16:58:01Z"],"title":["Meta-Embeddings for Natural Language Inference and Semantic Similarity\n  tasks"],"summary":["  Word Representations form the core component for almost all advanced Natural\nLanguage Processing (NLP) applications such as text mining, question-answering,\nand text summarization, etc. Over the last two decades, immense research is\nconducted to come up with one single model to solve all major NLP tasks. The\nmajor problem currently is that there are a plethora of choices for different\nNLP tasks. Thus for NLP practitioners, the task of choosing the right model to\nbe used itself becomes a challenge. Thus combining multiple pre-trained word\nembeddings and forming meta embeddings has become a viable approach to improve\ntackle NLP tasks. Meta embedding learning is a process of producing a single\nword embedding from a given set of pre-trained input word embeddings. In this\npaper, we propose to use Meta Embedding derived from few State-of-the-Art\n(SOTA) models to efficiently tackle mainstream NLP tasks like classification,\nsemantic relatedness, and text similarity. We have compared both ensemble and\ndynamic variants to identify an efficient approach. The results obtained show\nthat even the best State-of-the-Art models can be bettered. Thus showing us\nthat meta-embeddings can be used for several NLP tasks by harnessing the power\nof several individual representations.\n"],"author":[{"name":["Shree Charran R"],"arxiv:affiliation":[{"_":"Senior Member IEEE","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Rahul Kumar Dubey"],"arxiv:affiliation":[{"_":"Senior Member IEEE","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"link":[{"$":{"href":"http://arxiv.org/abs/2012.00633v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2012.00633v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.IR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.12982v1"],"updated":["2021-02-25T16:35:07Z"],"published":["2021-02-25T16:35:07Z"],"title":["A Primer on Contrastive Pretraining in Language Processing: Methods,\n  Lessons Learned and Perspectives"],"summary":["  Modern natural language processing (NLP) methods employ self-supervised\npretraining objectives such as masked language modeling to boost the\nperformance of various application tasks. These pretraining methods are\nfrequently extended with recurrence, adversarial or linguistic property\nmasking, and more recently with contrastive learning objectives. Contrastive\nself-supervised training objectives enabled recent successes in image\nrepresentation pretraining by learning to contrast input-input pairs of\naugmented images as either similar or dissimilar. However, in NLP, automated\ncreation of text input augmentations is still very challenging because a single\ntoken can invert the meaning of a sentence. For this reason, some contrastive\nNLP pretraining methods contrast over input-label pairs, rather than over\ninput-input pairs, using methods from Metric Learning and Energy Based Models.\nIn this survey, we summarize recent self-supervised and supervised contrastive\nNLP pretraining methods and describe where they are used to improve language\nmodeling, few or zero-shot learning, pretraining data-efficiency and specific\nNLP end-tasks. We introduce key contrastive learning concepts with lessons\nlearned from prior research and structure works by applications and cross-field\nrelations. Finally, we point to open challenges and future directions for\ncontrastive NLP to encourage bringing contrastive NLP pretraining closer to\nrecent successes in image representation pretraining.\n"],"author":[{"name":["Nils Rethmeier"]},{"name":["Isabelle Augenstein"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2102.12982v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.12982v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CV","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2103.01834v4"],"updated":["2021-09-02T03:10:43Z"],"published":["2021-03-02T16:19:44Z"],"title":["A Data-Centric Framework for Composable NLP Workflows"],"summary":["  Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n"],"author":[{"name":["Zhengzhong Liu"]},{"name":["Guanxiong Ding"]},{"name":["Avinash Bukkittu"]},{"name":["Mansi Gupta"]},{"name":["Pengzhi Gao"]},{"name":["Atif Ahmed"]},{"name":["Shikun Zhang"]},{"name":["Xin Gao"]},{"name":["Swapnil Singhavi"]},{"name":["Linwei Li"]},{"name":["Wei Wei"]},{"name":["Zecong Hu"]},{"name":["Haoran Shi"]},{"name":["Haoying Zhang"]},{"name":["Xiaodan Liang"]},{"name":["Teruko Mitamura"]},{"name":["Eric P. Xing"]},{"name":["Zhiting Hu"]}],"arxiv:comment":[{"_":"8 pages, 4 figures, EMNLP 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2103.01834v4","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2103.01834v4","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.10640v3"],"updated":["2021-04-24T17:31:46Z"],"published":["2021-03-23T22:38:20Z"],"title":["The NLP Cookbook: Modern Recipes for Transformer based Deep Learning\n  Architectures"],"summary":["  In recent years, Natural Language Processing (NLP) models have achieved\nphenomenal success in linguistic and semantic tasks like text classification,\nmachine translation, cognitive dialogue systems, information retrieval via\nNatural Language Understanding (NLU), and Natural Language Generation (NLG).\nThis feat is primarily attributed due to the seminal Transformer architecture,\nleading to designs such as BERT, GPT (I, II, III), etc. Although these\nlarge-size models have achieved unprecedented performances, they come at high\ncomputational costs. Consequently, some of the recent NLP architectures have\nutilized concepts of transfer learning, pruning, quantization, and knowledge\ndistillation to achieve moderate model sizes while keeping nearly similar\nperformances as achieved by their predecessors. Additionally, to mitigate the\ndata size challenge raised by language models from a knowledge extraction\nperspective, Knowledge Retrievers have been built to extricate explicit data\ndocuments from a large corpus of databases with greater efficiency and\naccuracy. Recent research has also focused on superior inference by providing\nefficient attention to longer input sequences. In this paper, we summarize and\nexamine the current state-of-the-art (SOTA) NLP models that have been employed\nfor numerous NLP tasks for optimal performance and efficiency. We provide a\ndetailed understanding and functioning of the different architectures, a\ntaxonomy of NLP designs, comparative evaluations, and future directions in NLP.\n"],"author":[{"name":["Sushant Singh"]},{"name":["Ausif Mahmood"]}],"arxiv:doi":[{"_":"10.1109/ACCESS.2021.3077350","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1109/ACCESS.2021.3077350","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/2104.10640v3","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.10640v3","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"27 pages and 29 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"IEEE Access (Volume: 9), 2021","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.07869v1"],"updated":["2021-12-15T04:20:35Z"],"published":["2021-12-15T04:20:35Z"],"title":["Fine-Tuning Large Neural Language Models for Biomedical Natural Language\n  Processing"],"summary":["  Motivation: A perennial challenge for biomedical researchers and clinical\npractitioners is to stay abreast with the rapid growth of publications and\nmedical notes. Natural language processing (NLP) has emerged as a promising\ndirection for taming information overload. In particular, large neural language\nmodels facilitate transfer learning by pretraining on unlabeled text, as\nexemplified by the successes of BERT models in various NLP applications.\nHowever, fine-tuning such models for an end task remains challenging,\nespecially with small labeled datasets, which are common in biomedical NLP.\n  Results: We conduct a systematic study on fine-tuning stability in biomedical\nNLP. We show that finetuning performance may be sensitive to pretraining\nsettings, especially in low-resource domains. Large models have potential to\nattain better performance, but increasing model size also exacerbates\nfinetuning instability. We thus conduct a comprehensive exploration of\ntechniques for addressing fine-tuning instability. We show that these\ntechniques can substantially improve fine-tuning performance for lowresource\nbiomedical NLP applications. Specifically, freezing lower layers is helpful for\nstandard BERT-BASE models, while layerwise decay is more effective for\nBERT-LARGE and ELECTRA models. For low-resource text similarity tasks such as\nBIOSSES, reinitializing the top layer is the optimal strategy. Overall,\ndomainspecific vocabulary and pretraining facilitate more robust models for\nfine-tuning. Based on these findings, we establish new state of the art on a\nwide range of biomedical NLP applications.\n  Availability and implementation: To facilitate progress in biomedical NLP, we\nrelease our state-of-the-art pretrained and fine-tuned models:\nhttps://aka.ms/BLURB.\n"],"author":[{"name":["Robert Tinn"]},{"name":["Hao Cheng"]},{"name":["Yu Gu"]},{"name":["Naoto Usuyama"]},{"name":["Xiaodong Liu"]},{"name":["Tristan Naumann"]},{"name":["Jianfeng Gao"]},{"name":["Hoifung Poon"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.07869v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.07869v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2201.07281v2"],"updated":["2022-05-10T17:07:24Z"],"published":["2022-01-18T19:34:23Z"],"title":["Annotating the Tweebank Corpus on Named Entity Recognition and Building\n  NLP Models for Social Media Analysis"],"summary":["  Social media data such as Twitter messages (\"tweets\") pose a particular\nchallenge to NLP systems because of their short, noisy, and colloquial nature.\nTasks such as Named Entity Recognition (NER) and syntactic parsing require\nhighly domain-matched training data for good performance. To date, there is no\ncomplete training corpus for both NER and syntactic analysis (e.g., part of\nspeech tagging, dependency parsing) of tweets. While there are some publicly\navailable annotated NLP datasets of tweets, they are only designed for\nindividual tasks. In this study, we aim to create Tweebank-NER, an English NER\ncorpus based on Tweebank V2 (TB2), train state-of-the-art (SOTA) Tweet NLP\nmodels on TB2, and release an NLP pipeline called Twitter-Stanza. We annotate\nnamed entities in TB2 using Amazon Mechanical Turk and measure the quality of\nour annotations. We train the Stanza pipeline on TB2 and compare with\nalternative NLP frameworks (e.g., FLAIR, spaCy) and transformer-based models.\nThe Stanza tokenizer and lemmatizer achieve SOTA performance on TB2, while the\nStanza NER tagger, part-of-speech (POS) tagger, and dependency parser achieve\ncompetitive performance against non-transformer models. The transformer-based\nmodels establish a strong baseline in Tweebank-NER and achieve the new SOTA\nperformance in POS tagging and dependency parsing on TB2. We release the\ndataset and make both the Stanza pipeline and BERTweet-based models available\n\"off-the-shelf\" for use in future Tweet NLP research. Our source code, data,\nand pre-trained models are available at:\n\\url{https://github.com/social-machines/TweebankNLP}.\n"],"author":[{"name":["Hang Jiang"]},{"name":["Yining Hua"]},{"name":["Doug Beeferman"]},{"name":["Deb Roy"]}],"arxiv:comment":[{"_":"Accepted at LREC 2022 (Long Papers)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2201.07281v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2201.07281v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2204.11909v1"],"updated":["2022-04-25T18:25:57Z"],"published":["2022-04-25T18:25:57Z"],"title":["How can NLP Help Revitalize Endangered Languages? A Case Study and\n  Roadmap for the Cherokee Language"],"summary":["  More than 43% of the languages spoken in the world are endangered, and\nlanguage loss currently occurs at an accelerated rate because of globalization\nand neocolonialism. Saving and revitalizing endangered languages has become\nvery important for maintaining the cultural diversity on our planet. In this\nwork, we focus on discussing how NLP can help revitalize endangered languages.\nWe first suggest three principles that may help NLP practitioners to foster\nmutual understanding and collaboration with language communities, and we\ndiscuss three ways in which NLP can potentially assist in language education.\nWe then take Cherokee, a severely-endangered Native American language, as a\ncase study. After reviewing the language's history, linguistic features, and\nexisting resources, we (in collaboration with Cherokee community members)\narrive at a few meaningful ways NLP practitioners can collaborate with\ncommunity partners. We suggest two approaches to enrich the Cherokee language's\nresources with machine-in-the-loop processing, and discuss several NLP tools\nthat people from the Cherokee community have shown interest in. We hope that\nour work serves not only to inform the NLP community about Cherokee, but also\nto provide inspiration for future work on endangered languages in general. Our\ncode and data will be open-sourced at\nhttps://github.com/ZhangShiyue/RevitalizeCherokee\n"],"author":[{"name":["Shiyue Zhang"]},{"name":["Ben Frey"]},{"name":["Mohit Bansal"]}],"arxiv:comment":[{"_":"ACL 2022","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2204.11909v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2204.11909v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.05780v1"],"updated":["2021-12-07T22:49:58Z"],"published":["2021-12-07T22:49:58Z"],"title":["A Scoping Review of Publicly Available Language Tasks in Clinical\n  Natural Language Processing"],"summary":["  Objective: to provide a scoping review of papers on clinical natural language\nprocessing (NLP) tasks that use publicly available electronic health record\ndata from a cohort of patients. Materials and Methods: We searched six\ndatabases, including biomedical research and computer science literature\ndatabase. A round of title/abstract screening and full-text screening were\nconducted by two reviewers. Our method followed the Preferred Reporting Items\nfor Systematic Reviews and Meta-Analysis (PRISMA) guidelines. Results: A total\nof 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and\n2021. We categorized the tasks by the type of NLP problems, including name\nentity recognition, summarization, and other NLP tasks. Some tasks were\nintroduced with a topic of clinical decision support applications, such as\nsubstance abuse, phenotyping, cohort selection for clinical trial. We\nsummarized the tasks by publication and dataset information. Discussion: The\nbreadth of clinical NLP tasks keeps growing as the field of NLP evolves with\nadvancements in language systems. However, gaps exist in divergent interests\nbetween general domain NLP community and clinical informatics community, and in\ngeneralizability of the data sources. We also identified issues in data\nselection and preparation including the lack of time-sensitive data, and\ninvalidity of problem size and evaluation. Conclusions: The existing clinical\nNLP tasks cover a wide range of topics and the field will continue to grow and\nattract more attention from both general domain NLP and clinical informatics\ncommunity. We encourage future work to incorporate multi-disciplinary\ncollaboration, reporting transparency, and standardization in data preparation.\n"],"author":[{"name":["Yanjun Gao"]},{"name":["Dmitriy Dligach"]},{"name":["Leslie Christensen"]},{"name":["Samuel Tesch"]},{"name":["Ryan Laffin"]},{"name":["Dongfang Xu"]},{"name":["Timothy Miller"]},{"name":["Ozlem Uzuner"]},{"name":["Matthew M Churpek"]},{"name":["Majid Afshar"]}],"arxiv:comment":[{"_":"Paper submitted to Journal of American Medical Informatics\n  Association (JAMIA)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.05780v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.05780v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1401.0923v2"],"updated":["2014-11-10T14:29:01Z"],"published":["2014-01-05T18:28:58Z"],"title":["Heavy Quarkonium Production at Collider Energies: Factorization and\n  Evolution"],"summary":["  We present a factorization formalism for inclusive production of heavy\nquarkonia of large transverse momentum, $p_T$ at collider energies, including\nboth leading power (LP) and next-to-leading power (NLP) behavior in $p_T$. We\ndemonstrate that both LP and NLP contributions can be factorized in terms of\nperturbatively calculable short-distance partonic coefficient functions and\nuniversal non-perturbative fragmentation functions, and derive the evolution\nequations that are implied by the factorization. We identify projection\noperators for all channels of the factorized LP and NLP infrared safe\nshort-distance partonic hard parts, and corresponding operator definitions of\nfragmentation functions. For the NLP, we focus on the contributions involving\nthe production of a heavy quark pair, a necessary condition for producing a\nheavy quarkonium. We evaluate the first non-trivial order of evolution kernels\nfor all relevant fragmentation functions, and discuss the role of NLP\ncontributions.\n"],"author":[{"name":["Zhong-Bo Kang"]},{"name":["Yan-Qing Ma"]},{"name":["Jian-Wei Qiu"]},{"name":["George Sterman"]}],"arxiv:doi":[{"_":"10.1103/PhysRevD.90.034006","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1103/PhysRevD.90.034006","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1401.0923v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1401.0923v2","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"70 pages and 26 figures, the published version","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Phys. Rev. D 90, 034006 (2014)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"nucl-th","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1508.06044v1"],"updated":["2015-08-25T06:34:00Z"],"published":["2015-08-25T06:34:00Z"],"title":["Visualizing NLP annotations for Crowdsourcing"],"summary":["  Visualizing NLP annotation is useful for the collection of training data for\nthe statistical NLP approaches. Existing toolkits either provide limited visual\naid, or introduce comprehensive operators to realize sophisticated linguistic\nrules. Workers must be well trained to use them. Their audience thus can hardly\nbe scaled to large amounts of non-expert crowdsourced workers. In this paper,\nwe present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to\nannotate two general categories of NLP problems: clustering and parsing.\nWorkers can finish the tasks with simplified operators in an interactive\ninterface, and fix errors conveniently. User studies show our toolkit is very\nfriendly to NLP non-experts, and allow them to produce high quality labels for\nseveral sophisticated problems. We release our source code and toolkit to spur\nfuture research.\n"],"author":[{"name":["Hanchuan Li"]},{"name":["Haichen Shen"]},{"name":["Shengliang Xu"]},{"name":["Congle Zhang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1508.06044v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1508.06044v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1610.06842v2"],"updated":["2018-01-11T16:07:26Z"],"published":["2016-10-21T16:23:31Z"],"title":["Non-abelian factorisation for next-to-leading-power threshold logarithms"],"summary":["  Soft and collinear radiation is responsible for large corrections to many\nhadronic cross sections, near thresholds for the production of heavy final\nstates. There is much interest in extending our understanding of this radiation\nto next-to-leading power (NLP) in the threshold expansion. In this paper, we\ngeneralise a previously proposed all-order NLP factorisation formula to include\nnon-abelian corrections. We define a non-abelian radiative jet function,\norganising collinear enhancements at NLP, and compute it for quark jets at one\nloop. We discuss in detail the issue of double counting between soft and\ncollinear regions. Finally, we verify our prescription by reproducing all NLP\nlogarithms in Drell-Yan production up to NNLO, including those associated with\ndouble real emission. Our results constitute an important step in the\ndevelopment of a fully general resummation formalism for NLP threshold effects.\n"],"author":[{"name":["D. Bonocore"]},{"name":["E. Laenen"]},{"name":["L. Magnea"]},{"name":["L. Vernazza"]},{"name":["C. D. White"]}],"arxiv:doi":[{"_":"10.1007/JHEP12(2016)121","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.1007/JHEP12(2016)121","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1610.06842v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1610.06842v2","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"17 pages, 4 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"hep-ph","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"hep-th","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1710.06632v1"],"updated":["2017-10-18T09:13:06Z"],"published":["2017-10-18T09:13:06Z"],"title":["Towards a Seamless Integration of Word Senses into Downstream NLP\n  Applications"],"summary":["  Lexical ambiguity can impede NLP systems from accurate understanding of\nsemantics. Despite its potential benefits, the integration of sense-level\ninformation into NLP systems has remained understudied. By incorporating a\nnovel disambiguation algorithm into a state-of-the-art classification model, we\ncreate a pipeline to integrate sense-level information into downstream NLP\napplications. We show that a simple disambiguation of the input text can lead\nto consistent performance improvement on multiple topic categorization and\npolarity detection datasets, particularly when the fine granularity of the\nunderlying sense inventory is reduced and the document is sufficiently large.\nOur results also point to the need for sense representation research to focus\nmore on in vivo evaluations which target the performance in downstream NLP\napplications rather than artificial benchmarks.\n"],"author":[{"name":["Mohammad Taher Pilehvar"]},{"name":["Jose Camacho-Collados"]},{"name":["Roberto Navigli"]},{"name":["Nigel Collier"]}],"arxiv:doi":[{"_":"10.18653/v1/P17-1170","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.18653/v1/P17-1170","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1710.06632v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1710.06632v1","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"ACL 2017","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada (2017),\n  pages 1857-1869","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1805.11824v1"],"updated":["2018-05-30T06:49:15Z"],"published":["2018-05-30T06:49:15Z"],"title":["Anaphora and Coreference Resolution: A Review"],"summary":["  Entity resolution aims at resolving repeated references to an entity in a\ndocument and forms a core component of natural language processing (NLP)\nresearch. This field possesses immense potential to improve the performance of\nother NLP fields like machine translation, sentiment analysis, paraphrase\ndetection, summarization, etc. The area of entity resolution in NLP has seen\nproliferation of research in two separate sub-areas namely: anaphora resolution\nand coreference resolution. Through this review article, we aim at clarifying\nthe scope of these two tasks in entity resolution. We also carry out a detailed\nanalysis of the datasets, evaluation metrics and research methods that have\nbeen adopted to tackle this NLP problem. This survey is motivated with the aim\nof providing the reader with a clear understanding of what constitutes this NLP\nproblem and the issues that require attention.\n"],"author":[{"name":["Rhea Sukthanker"]},{"name":["Soujanya Poria"]},{"name":["Erik Cambria"]},{"name":["Ramkumar Thirunavukarasu"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1805.11824v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1805.11824v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1811.06179v1"],"updated":["2018-11-15T04:58:21Z"],"published":["2018-11-15T04:58:21Z"],"title":["Implementing a Portable Clinical NLP System with a Common Data Model - a\n  Lisp Perspective"],"summary":["  This paper presents a Lisp architecture for a portable NLP system, termed\nLAPNLP, for processing clinical notes. LAPNLP integrates multiple standard,\ncustomized and in-house developed NLP tools. Our system facilitates portability\nacross different institutions and data systems by incorporating an enriched\nCommon Data Model (CDM) to standardize necessary data elements. It utilizes\nUMLS to perform domain adaptation when integrating generic domain NLP tools. It\nalso features stand-off annotations that are specified by positional reference\nto the original document. We built an interval tree based search engine to\nefficiently query and retrieve the stand-off annotations by specifying\npositional requirements. We also developed a utility to convert an inline\nannotation format to stand-off annotations to enable the reuse of clinical text\ndatasets with inline annotations. We experimented with our system on several\nNLP facilitated tasks including computational phenotyping for lymphoma patients\nand semantic relation extraction for clinical notes. These experiments\nshowcased the broader applicability and utility of LAPNLP.\n"],"author":[{"name":["Yuan Luo"]},{"name":["Peter Szolovits"]}],"arxiv:comment":[{"_":"6 pages, accepted by IEEE BIBM 2018 as regular paper","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1811.06179v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1811.06179v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1302.4814v1"],"updated":["2013-02-20T05:47:44Z"],"published":["2013-02-20T05:47:44Z"],"title":["NLP and CALL: integration is working"],"summary":["  In the first part of this article, we explore the background of\ncomputer-assisted learning from its beginnings in the early XIXth century and\nthe first teaching machines, founded on theories of learning, at the start of\nthe XXth century. With the arrival of the computer, it became possible to offer\nlanguage learners different types of language activities such as comprehension\ntasks, simulations, etc. However, these have limits that cannot be overcome\nwithout some contribution from the field of natural language processing (NLP).\nIn what follows, we examine the challenges faced and the issues raised by\nintegrating NLP into CALL. We hope to demonstrate that the key to success in\nintegrating NLP into CALL is to be found in multidisciplinary work between\ncomputer experts, linguists, language teachers, didacticians and NLP\nspecialists.\n"],"author":[{"name":["Georges Antoniadis"],"arxiv:affiliation":[{"_":"LIDILEM","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Sylviane Granger"],"arxiv:affiliation":[{"_":"LIDILEM","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Olivier Kraif"],"arxiv:affiliation":[{"_":"LIDILEM","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Claude Ponton"],"arxiv:affiliation":[{"_":"LIDILEM","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Virginie Zampa"],"arxiv:affiliation":[{"_":"LIDILEM","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]}],"arxiv:journal_ref":[{"_":"TaLC7, France (2006)","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1302.4814v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1302.4814v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1902.00679v1"],"updated":["2019-02-02T09:30:26Z"],"published":["2019-02-02T09:30:26Z"],"title":["Natural Language Processing, Sentiment Analysis and Clinical Analytics"],"summary":["  Recent advances in Big Data has prompted health care practitioners to utilize\nthe data available on social media to discern sentiment and emotions\nexpression. Health Informatics and Clinical Analytics depend heavily on\ninformation gathered from diverse sources. Traditionally, a healthcare\npractitioner will ask a patient to fill out a questionnaire that will form the\nbasis of diagnosing the medical condition. However, medical practitioners have\naccess to many sources of data including the patients writings on various\nmedia. Natural Language Processing (NLP) allows researchers to gather such data\nand analyze it to glean the underlying meaning of such writings. The field of\nsentiment analysis (applied to many other domains) depend heavily on techniques\nutilized by NLP. This work will look into various prevalent theories underlying\nthe NLP field and how they can be leveraged to gather users sentiments on\nsocial media. Such sentiments can be culled over a period of time thus\nminimizing the errors introduced by data input and other stressors.\nFurthermore, we look at some applications of sentiment analysis and application\nof NLP to mental health. The reader will also learn about the NLTK toolkit that\nimplements various NLP theories and how they can make the data scavenging\nprocess a lot easier.\n"],"author":[{"name":["Adil Rajput"]}],"link":[{"$":{"href":"http://arxiv.org/abs/1902.00679v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1902.00679v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1904.09535v3"],"updated":["2019-10-18T12:17:27Z"],"published":["2019-04-21T03:11:24Z"],"title":["NeuronBlocks: Building Your NLP DNN Models Like Playing Lego"],"summary":["  Deep Neural Networks (DNN) have been widely employed in industry to address\nvarious Natural Language Processing (NLP) tasks. However, many engineers find\nit a big overhead when they have to choose from multiple frameworks, compare\ndifferent types of models, and understand various optimization mechanisms. An\nNLP toolkit for DNN models with both generality and flexibility can greatly\nimprove the productivity of engineers by saving their learning cost and guiding\nthem to find optimal solutions to their tasks. In this paper, we introduce\nNeuronBlocks\\footnote{Code: \\url{https://github.com/Microsoft/NeuronBlocks}}\n\\footnote{Demo: \\url{https://youtu.be/x6cOpVSZcdo}}, a toolkit encapsulating a\nsuite of neural network modules as building blocks to construct various DNN\nmodels with complex architecture. This toolkit empowers engineers to build,\ntrain, and test various NLP models through simple configuration of JSON files.\nThe experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003\ndemonstrate the effectiveness of NeuronBlocks.\n"],"author":[{"name":["Ming Gong"]},{"name":["Linjun Shou"]},{"name":["Wutao Lin"]},{"name":["Zhijie Sang"]},{"name":["Quanjia Yan"]},{"name":["Ze Yang"]},{"name":["Feixiang Cheng"]},{"name":["Daxin Jiang"]}],"arxiv:comment":[{"_":"6 pages, 3 figures","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"EMNLP 2019","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1904.09535v3","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1904.09535v3","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1906.02243v1"],"updated":["2019-06-05T18:40:53Z"],"published":["2019-06-05T18:40:53Z"],"title":["Energy and Policy Considerations for Deep Learning in NLP"],"summary":["  Recent progress in hardware and methodology for training neural networks has\nushered in a new generation of large networks trained on abundant data. These\nmodels have obtained notable gains in accuracy across many NLP tasks. However,\nthese accuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result these models are costly to train and develop, both\nfinancially, due to the cost of hardware and electricity or cloud compute time,\nand environmentally, due to the carbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring this issue to the attention of NLP\nresearchers by quantifying the approximate financial and environmental costs of\ntraining a variety of recently successful neural network models for NLP. Based\non these findings, we propose actionable recommendations to reduce costs and\nimprove equity in NLP research and practice.\n"],"author":[{"name":["Emma Strubell"]},{"name":["Ananya Ganesh"]},{"name":["Andrew McCallum"]}],"arxiv:comment":[{"_":"In the 57th Annual Meeting of the Association for Computational\n  Linguistics (ACL). Florence, Italy. July 2019","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1906.02243v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1906.02243v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1906.08976v1"],"updated":["2019-06-21T06:39:11Z"],"published":["2019-06-21T06:39:11Z"],"title":["Mitigating Gender Bias in Natural Language Processing: Literature Review"],"summary":["  As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in\npopularity, it becomes increasingly vital to recognize the role they play in\nshaping societal biases and stereotypes. Although NLP models have shown success\nin modeling various applications, they propagate and may even amplify gender\nbias found in text corpora. While the study of bias in artificial intelligence\nis not new, methods to mitigate gender bias in NLP are relatively nascent. In\nthis paper, we review contemporary studies on recognizing and mitigating gender\nbias in NLP. We discuss gender bias based on four forms of representation bias\nand analyze methods recognizing gender bias. Furthermore, we discuss the\nadvantages and drawbacks of existing gender debiasing methods. Finally, we\ndiscuss future studies for recognizing and mitigating gender bias in NLP.\n"],"author":[{"name":["Tony Sun"]},{"name":["Andrew Gaut"]},{"name":["Shirlyn Tang"]},{"name":["Yuxin Huang"]},{"name":["Mai ElSherief"]},{"name":["Jieyu Zhao"]},{"name":["Diba Mirza"]},{"name":["Elizabeth Belding"]},{"name":["Kai-Wei Chang"]},{"name":["William Yang Wang"]}],"arxiv:comment":[{"_":"Accepted to ACL 2019","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1906.08976v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1906.08976v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1907.09548v1"],"updated":["2019-07-22T19:54:20Z"],"published":["2019-07-22T19:54:20Z"],"title":["On the Equivalence Between Abstract Dialectical Frameworks and Logic\n  Programs"],"summary":["  Abstract Dialectical Frameworks (ADFs) are argumentation frameworks where\neach node is associated with an acceptance condition. This allows us to model\ndifferent types of dependencies as supports and attacks. Previous studies\nprovided a translation from Normal Logic Programs (NLPs) to ADFs and proved the\nstable models semantics for a normal logic program has an equivalent semantics\nto that of the corresponding ADF. However, these studies failed in identifying\na semantics for ADFs equivalent to a three-valued semantics (as partial stable\nmodels and well-founded models) for NLPs. In this work, we focus on a fragment\nof ADFs, called Attacking Dialectical Frameworks (ADF$^+$s), and provide a\ntranslation from NLPs to ADF$^+$s robust enough to guarantee the equivalence\nbetween partial stable models, well-founded models, regular models, stable\nmodels semantics for NLPs and respectively complete models, grounded models,\npreferred models, stable models for ADFs. In addition, we define a new\nsemantics for ADF$^+$s, called L-stable, and show it is equivalent to the\nL-stable semantics for NLPs. This paper is under consideration for acceptance\nin TPLP.\n"],"author":[{"name":["João Alcântara"]},{"name":["Samy Sá"]},{"name":["Juan Acosta-Guadarrama"]}],"arxiv:comment":[{"_":"Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/1907.09548v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1907.09548v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LO","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/1912.11078v2"],"updated":["2020-09-12T19:56:10Z"],"published":["2019-11-09T23:53:19Z"],"title":["Predictive Biases in Natural Language Processing Models: A Conceptual\n  Framework and Overview"],"summary":["  An increasing number of works in natural language processing have addressed\nthe effect of bias on the predicted outcomes, introducing mitigation techniques\nthat act on different parts of the standard NLP pipeline (data and models).\nHowever, these works have been conducted in isolation, without a unifying\nframework to organize efforts within the field. This leads to repetitive\napproaches, and puts an undue focus on the effects of bias, rather than on\ntheir origins. Research focused on bias symptoms rather than the underlying\norigins could limit the development of effective countermeasures. In this\npaper, we propose a unifying conceptualization: the predictive bias framework\nfor NLP. We summarize the NLP literature and propose a general mathematical\ndefinition of predictive bias in NLP along with a conceptual framework,\ndifferentiating four main origins of biases: label bias, selection bias, model\noveramplification, and semantic bias. We discuss how past work has countered\neach bias origin. Our framework serves to guide an introductory overview of\npredictive bias in NLP, integrating existing work into a single structure and\nopening avenues for future research.\n"],"author":[{"name":["Deven Shah"]},{"name":["H. Andrew Schwartz"]},{"name":["Dirk Hovy"]}],"arxiv:doi":[{"_":"10.18653/v1/2020.acl-main.468","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"title":"doi","href":"http://dx.doi.org/10.18653/v1/2020.acl-main.468","rel":"related"}},{"$":{"href":"http://arxiv.org/abs/1912.11078v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/1912.11078v2","rel":"related","type":"application/pdf"}}],"arxiv:comment":[{"_":"9 pages excluding references, 1 figure, 3 pages for appendix","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Association for Computational Linguistics. (2020) 5248--5264","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2004.09890v1"],"updated":["2020-04-21T10:37:44Z"],"published":["2020-04-21T10:37:44Z"],"title":["Considering Likelihood in NLP Classification Explanations with Occlusion\n  and Language Modeling"],"summary":["  Recently, state-of-the-art NLP models gained an increasing syntactic and\nsemantic understanding of language, and explanation methods are crucial to\nunderstand their decisions. Occlusion is a well established method that\nprovides explanations on discrete language data, e.g. by removing a language\nunit from an input and measuring the impact on a model's decision. We argue\nthat current occlusion-based methods often produce invalid or syntactically\nincorrect language data, neglecting the improved abilities of recent NLP\nmodels. Furthermore, gradient-based explanation methods disregard the discrete\ndistribution of data in NLP. Thus, we propose OLM: a novel explanation method\nthat combines occlusion and language models to sample valid and syntactically\ncorrect replacements with high likelihood, given the context of the original\ninput. We lay out a theoretical foundation that alleviates these weaknesses of\nother explanation methods in NLP and provide results that underline the\nimportance of considering data likelihood in occlusion-based explanation.\n"],"author":[{"name":["David Harbecke"]},{"name":["Christoph Alt"]}],"arxiv:comment":[{"_":"ACL 2020 Student Research Workshop","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2004.09890v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2004.09890v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2005.13012v2"],"updated":["2021-01-12T15:48:52Z"],"published":["2020-05-26T20:14:39Z"],"title":["Comparing BERT against traditional machine learning text classification"],"summary":["  The BERT model has arisen as a popular state-of-the-art machine learning\nmodel in the recent years that is able to cope with multiple NLP tasks such as\nsupervised text classification without human supervision. Its flexibility to\ncope with any type of corpus delivering great results has make this approach\nvery popular not only in academia but also in the industry. Although, there are\nlots of different approaches that have been used throughout the years with\nsuccess. In this work, we first present BERT and include a little review on\nclassical NLP approaches. Then, we empirically test with a suite of experiments\ndealing different scenarios the behaviour of BERT against the traditional\nTF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work\nis to add empirical evidence to support or refuse the use of BERT as a default\non NLP tasks. Experiments show the superiority of BERT and its independence of\nfeatures of the NLP problem such as the language of the text adding empirical\nevidence to use BERT as a default technique to be used in NLP problems.\n"],"author":[{"name":["Santiago González-Carvajal"]},{"name":["Eduardo C. Garrido-Merchán"]}],"arxiv:comment":[{"_":"12 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2005.13012v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2005.13012v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"stat.ML","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2005.14050v2"],"updated":["2020-05-29T16:44:18Z"],"published":["2020-05-28T14:32:08Z"],"title":["Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP"],"summary":["  We survey 146 papers analyzing \"bias\" in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing \"bias\" is an inherently normative process. We\nfurther find that these papers' proposed quantitative techniques for measuring\nor mitigating \"bias\" are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing \"bias\" in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of \"bias\"---i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements---and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.\n"],"author":[{"name":["Su Lin Blodgett"]},{"name":["Solon Barocas"]},{"name":["Hal Daumé III"]},{"name":["Hanna Wallach"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2005.14050v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2005.14050v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2010.01526v1"],"updated":["2020-10-04T09:47:40Z"],"published":["2020-10-04T09:47:40Z"],"title":["NLP Service APIs and Models for Efficient Registration of New Clients"],"summary":["  State-of-the-art NLP inference uses enormous neural architectures and models\ntrained for GPU-months, well beyond the reach of most consumers of NLP. This\nhas led to one-size-fits-all public API-based NLP service models by major AI\ncompanies, serving large numbers of clients. Neither (hardware deficient)\nclients nor (heavily subscribed) servers can afford traditional fine tuning.\nMany clients own little or no labeled data. We initiate a study of adaptation\nof centralized NLP services to clients, and present one practical and\nlightweight approach. Each client uses an unsupervised, corpus-based sketch to\nregister to the service. The server uses an auxiliary network to map the sketch\nto an abstract vector representation, which then informs the main labeling\nnetwork. When a new client registers with its sketch, it gets immediate\naccuracy benefits. We demonstrate the success of the proposed architecture\nusing sentiment labeling, NER, and predictive language modeling\n"],"author":[{"name":["Sahil Shah"]},{"name":["Vihari Piratla"]},{"name":["Soumen Chakrabarti"]},{"name":["Sunita Sarawagi"]}],"arxiv:comment":[{"_":"Accepted to Findings of EMNLP, 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2010.01526v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2010.01526v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2011.05911v1"],"updated":["2020-11-11T17:04:55Z"],"published":["2020-11-11T17:04:55Z"],"title":["Situated Data, Situated Systems: A Methodology to Engage with Power\n  Relations in Natural Language Processing Research"],"summary":["  We propose a bias-aware methodology to engage with power relations in natural\nlanguage processing (NLP) research. NLP research rarely engages with bias in\nsocial contexts, limiting its ability to mitigate bias. While researchers have\nrecommended actions, technical methods, and documentation practices, no\nmethodology exists to integrate critical reflections on bias with technical NLP\nmethods. In this paper, after an extensive and interdisciplinary literature\nreview, we contribute a bias-aware methodology for NLP research. We also\ncontribute a definition of biased text, a discussion of the implications of\nbiased NLP systems, and a case study demonstrating how we are executing the\nbias-aware methodology in research on archival metadata descriptions.\n"],"author":[{"name":["Lucy Havens"]},{"name":["Melissa Terras"]},{"name":["Benjamin Bach"]},{"name":["Beatrice Alex"]}],"arxiv:comment":[{"_":"Accepted to the 2nd Workshop on Gender Bias in Natural Language\n  Processing at COLING 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2011.05911v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2011.05911v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2011.13231v1"],"updated":["2020-11-26T10:59:23Z"],"published":["2020-11-26T10:59:23Z"],"title":["NLPStatTest: A Toolkit for Comparing NLP System Performance"],"summary":["  Statistical significance testing centered on p-values is commonly used to\ncompare NLP system performance, but p-values alone are insufficient because\nstatistical significance differs from practical significance. The latter can be\nmeasured by estimating effect size. In this paper, we propose a three-stage\nprocedure for comparing NLP system performance and provide a toolkit,\nNLPStatTest, that automates the process. Users can upload NLP system evaluation\nscores and the toolkit will analyze these scores, run appropriate significance\ntests, estimate effect size, and conduct power analysis to estimate Type II\nerror. The toolkit provides a convenient and systematic way to compare NLP\nsystem performance that goes beyond statistical significance testing\n"],"author":[{"name":["Haotian Zhu"]},{"name":["Denise Mak"]},{"name":["Jesse Gioannini"]},{"name":["Fei Xia"]}],"arxiv:comment":[{"_":"Will appear in AACL-IJCNLP 2020","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"arxiv:journal_ref":[{"_":"Proceedings of the 1st Conference of the Asia-Pacific Chapter of\n  the Association for Computational Linguistics and the 10th International\n  Joint Conference on Natural Language Processing: System Demonstrations (2020)\n  40-46","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2011.13231v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2011.13231v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"stat.AP","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.00214v1"],"updated":["2021-01-30T11:53:56Z"],"published":["2021-01-30T11:53:56Z"],"title":["Taxonomic survey of Hindi Language NLP systems"],"summary":["  Natural Language processing (NLP) represents the task of automatic handling\nof natural human language by machines.There is large spectrum of possible\napplications of NLP which help in automating tasks like translating text from\none language to other, retrieving and summarizing data from very huge\nrepositories, spam email filtering, identifying fake news in digital media,\nfind sentiment and feedback of people, find political opinions and views of\npeople on various government policies, provide effective medical assistance\nbased on past history records of patient etc. Hindi is the official language of\nIndia with nearly 691 million users in India and 366 million in rest of world.\nAt present, a number of government and private sector projects and researchers\nin India and abroad, are working towards developing NLP applications and\nresources for Indian languages. This survey gives a report of the resources and\napplications available for Hindi language NLP.\n"],"author":[{"name":["Nikita P. Desai"],"arxiv:affiliation":[{"_":"Dr.","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":[" Prof."],"arxiv:affiliation":[{"_":"Dr.","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}]},{"name":["Vipul K. Dabhi"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2102.00214v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.00214v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2102.13136v1"],"updated":["2021-02-25T19:28:39Z"],"published":["2021-02-25T19:28:39Z"],"title":["Automated essay scoring using efficient transformer-based language\n  models"],"summary":["  Automated Essay Scoring (AES) is a cross-disciplinary effort involving\nEducation, Linguistics, and Natural Language Processing (NLP). The efficacy of\nan NLP model in AES tests it ability to evaluate long-term dependencies and\nextrapolate meaning even when text is poorly written. Large pretrained\ntransformer-based language models have dominated the current state-of-the-art\nin many NLP tasks, however, the computational requirements of these models make\nthem expensive to deploy in practice. The goal of this paper is to challenge\nthe paradigm in NLP that bigger is better when it comes to AES. To do this, we\nevaluate the performance of several fine-tuned pretrained NLP models with a\nmodest number of parameters on an AES dataset. By ensembling our models, we\nachieve excellent results with fewer parameters than most pretrained\ntransformer-based models.\n"],"author":[{"name":["Christopher M Ormerod"]},{"name":["Akanksha Malhotra"]},{"name":["Amir Jafari"]}],"arxiv:comment":[{"_":"11 pages, 1 figure, 3 tables","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2102.13136v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2102.13136v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.08835v2"],"updated":["2021-09-30T22:36:50Z"],"published":["2021-04-18T12:14:46Z"],"title":["CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in\n  NLP"],"summary":["  Humans can learn a new language task efficiently with only few examples, by\nleveraging their knowledge obtained when learning prior tasks. In this paper,\nwe explore whether and how such cross-task generalization ability can be\nacquired, and further applied to build better few-shot learners across diverse\nNLP tasks. We introduce CrossFit, a problem setup for studying cross-task\ngeneralization ability, which standardizes seen/unseen task partitions, data\naccess during different learning stages, and the evaluation protocols. To\ninstantiate different seen/unseen task partitions in CrossFit and facilitate\nin-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse\nfew-shot NLP tasks created from open-access NLP datasets and converted to a\nunified text-to-text format. Our analysis reveals that the few-shot learning\nability on unseen tasks can be improved via an upstream learning stage using a\nset of seen tasks. We also observe that the selection of upstream learning\ntasks can significantly influence few-shot performance on unseen tasks, asking\nfurther analysis on task similarity and transferability.\n"],"author":[{"name":["Qinyuan Ye"]},{"name":["Bill Yuchen Lin"]},{"name":["Xiang Ren"]}],"arxiv:comment":[{"_":"Accepted to EMNLP 2021. Camera-ready version. Code:\n  https://github.com/INK-USC/CrossFit","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.08835v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.08835v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2104.10097v1"],"updated":["2021-04-20T16:30:59Z"],"published":["2021-04-20T16:30:59Z"],"title":["Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing"],"summary":["  The use of crowdworkers in NLP research is growing rapidly, in tandem with\nthe exponential increase in research production in machine learning and AI.\nEthical discussion regarding the use of crowdworkers within the NLP research\ncommunity is typically confined in scope to issues related to labor conditions\nsuch as fair pay. We draw attention to the lack of ethical considerations\nrelated to the various tasks performed by workers, including labeling,\nevaluation, and production. We find that the Final Rule, the common ethical\nframework used by researchers, did not anticipate the use of online\ncrowdsourcing platforms for data collection, resulting in gaps between the\nspirit and practice of human-subjects ethics in NLP research. We enumerate\ncommon scenarios where crowdworkers performing NLP tasks are at risk of harm.\nWe thus recommend that researchers evaluate these risks by considering the\nthree ethical principles set up by the Belmont Report. We also clarify some\ncommon misconceptions regarding the Institutional Review Board (IRB)\napplication. We hope this paper will serve to reopen the discussion within our\ncommunity regarding the ethical use of crowdworkers.\n"],"author":[{"name":["Boaz Shmueli"]},{"name":["Jan Fell"]},{"name":["Soumya Ray"]},{"name":["Lun-Wei Ku"]}],"arxiv:comment":[{"_":"To be published in NAACL-HLT 2021. 12 pages, 1 figure, 3 tables","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2104.10097v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2104.10097v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CY","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2105.00823v1"],"updated":["2021-05-03T13:15:24Z"],"published":["2021-05-03T13:15:24Z"],"title":["Switching Contexts: Transportability Measures for NLP"],"summary":["  This paper explores the topic of transportability, as a sub-area of\ngeneralisability. By proposing the utilisation of metrics based on\nwell-established statistics, we are able to estimate the change in performance\nof NLP models in new contexts. Defining a new measure for transportability may\nallow for better estimation of NLP system performance in new domains, and is\ncrucial when assessing the performance of NLP systems in new tasks and domains.\nThrough several instances of increasing complexity, we demonstrate how\nlightweight domain similarity measures can be used as estimators for the\ntransportability in NLP applications. The proposed transportability measures\nare evaluated in the context of Named Entity Recognition and Natural Language\nInference tasks.\n"],"author":[{"name":["Guy Marshall"]},{"name":["Mokanarangan Thayaparan"]},{"name":["Philip Osborne"]},{"name":["Andre Freitas"]}],"arxiv:comment":[{"_":"10 pages, 4 figures. To appear in IWCS 2021 proceedings","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2105.00823v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2105.00823v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2106.02192v1"],"updated":["2021-06-04T00:40:59Z"],"published":["2021-06-04T00:40:59Z"],"title":["Grounding 'Grounding' in NLP"],"summary":["  The NLP community has seen substantial recent interest in grounding to\nfacilitate interaction between language technologies and the world. However, as\na community, we use the term broadly to reference any linking of text to data\nor non-textual modality. In contrast, Cognitive Science more formally defines\n\"grounding\" as the process of establishing what mutual information is required\nfor successful communication between two interlocutors -- a definition which\nmight implicitly capture the NLP usage but differs in intent and scope. We\ninvestigate the gap between these definitions and seek answers to the following\nquestions: (1) What aspects of grounding are missing from NLP tasks? Here we\npresent the dimensions of coordination, purviews and constraints. (2) How is\nthe term \"grounding\" used in the current research? We study the trends in\ndatasets, domains, and tasks introduced in recent NLP conferences. And finally,\n(3) How to advance our current definition to bridge the gap with Cognitive\nScience? We present ways to both create new tasks or repurpose existing ones to\nmake advancements towards achieving a more complete sense of grounding.\n"],"author":[{"name":["Khyathi Raghavi Chandu"]},{"name":["Yonatan Bisk"]},{"name":["Alan W Black"]}],"arxiv:comment":[{"_":"24 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2106.02192v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2106.02192v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2107.03072v1"],"updated":["2021-07-07T08:33:00Z"],"published":["2021-07-07T08:33:00Z"],"title":["Android Security using NLP Techniques: A Review"],"summary":["  Android is among the most targeted platform by attackers. While attackers are\nimproving their techniques, traditional solutions based on static and dynamic\nanalysis have been also evolving. In addition to the application code, Android\napplications have some metadata that could be useful for security analysis of\napplications. Unlike traditional application distribution mechanisms, Android\napplications are distributed centrally in mobile markets. Therefore, beside\napplication packages, such markets contain app information provided by app\ndevelopers and app users. The availability of such useful textual data together\nwith the advancement in Natural Language Processing (NLP) that is used to\nprocess and understand textual data has encouraged researchers to investigate\nthe use of NLP techniques in Android security. Especially, security solutions\nbased on NLP have accelerated in the last 5 years and proven to be useful. This\nstudy reviews these proposals and aim to explore possible research directions\nfor future studies by presenting state-of-the-art in this domain. We mainly\nfocus on NLP-based solutions under four categories: description-to-behaviour\nfidelity, description generation, privacy and malware detection.\n"],"author":[{"name":["Sevil Sen"]},{"name":["Burcu Can"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2107.03072v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2107.03072v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CR","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2107.04374v1"],"updated":["2021-07-09T11:47:13Z"],"published":["2021-07-09T11:47:13Z"],"title":["Benchmarking for Biomedical Natural Language Processing Tasks with a\n  Domain Specific ALBERT"],"summary":["  The availability of biomedical text data and advances in natural language\nprocessing (NLP) have made new applications in biomedical NLP possible.\nLanguage models trained or fine tuned using domain specific corpora can\noutperform general models, but work to date in biomedical NLP has been limited\nin terms of corpora and tasks. We present BioALBERT, a domain-specific\nadaptation of A Lite Bidirectional Encoder Representations from Transformers\n(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical\n(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark\ndatasets. Experiments show that BioALBERT outperforms the state of the art on\nnamed entity recognition (+11.09% BLURB score improvement), relation extraction\n(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document\nclassification (+0.62% F1-score), and question answering (+2.83% BLURB score).\nIt represents a new state of the art in 17 out of 20 benchmark datasets. By\nmaking BioALBERT models and data available, our aim is to help the biomedical\nNLP community avoid computational costs of training and establish a new set of\nbaselines for future efforts across a broad range of biomedical NLP tasks.\n"],"author":[{"name":["Usman Naseem"]},{"name":["Adam G. Dunn"]},{"name":["Matloob Khushi"]},{"name":["Jinman Kim"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2107.04374v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2107.04374v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2107.08262v1"],"updated":["2021-07-17T15:49:22Z"],"published":["2021-07-17T15:49:22Z"],"title":["Tea: Program Repair Using Neural Network Based on Program Information\n  Attention Matrix"],"summary":["  The advance in machine learning (ML)-driven natural language process (NLP)\npoints a promising direction for automatic bug fixing for software programs, as\nfixing a buggy program can be transformed to a translation task. While software\nprograms contain much richer information than one-dimensional natural language\ndocuments, pioneering work on using ML-driven NLP techniques for automatic\nprogram repair only considered a limited set of such information. We\nhypothesize that more comprehensive information of software programs, if\nappropriately utilized, can improve the effectiveness of ML-driven NLP\napproaches in repairing software programs. As the first step towards proving\nthis hypothesis, we propose a unified representation to capture the syntax,\ndata flow, and control flow aspects of software programs, and devise a method\nto use such a representation to guide the transformer model from NLP in better\nunderstanding and fixing buggy programs. Our preliminary experiment confirms\nthat the more comprehensive information of software programs used, the better\nML-driven NLP techniques can perform in fixing bugs in these programs.\n"],"author":[{"name":["Wenshuo Wang"]},{"name":["Chen Wu"]},{"name":["Liang Cheng"]},{"name":["Yang Zhang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2107.08262v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2107.08262v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.SE","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.02941v1"],"updated":["2021-09-07T08:48:13Z"],"published":["2021-09-07T08:48:13Z"],"title":["Countering Online Hate Speech: An NLP Perspective"],"summary":["  Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.\n"],"author":[{"name":["Mudit Chaudhary"]},{"name":["Chandni Saxena"]},{"name":["Helen Meng"]}],"arxiv:comment":[{"_":"12 pages","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.02941v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.02941v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.09138v1"],"updated":["2021-09-19T14:51:51Z"],"published":["2021-09-19T14:51:51Z"],"title":["Multi-Task Learning in Natural Language Processing: An Overview"],"summary":["  Deep learning approaches have achieved great success in the field of Natural\nLanguage Processing (NLP). However, deep neural models often suffer from\noverfitting and data scarcity problems that are pervasive in NLP tasks. In\nrecent years, Multi-Task Learning (MTL), which can leverage useful information\nof related tasks to achieve simultaneous performance improvement on multiple\nrelated tasks, has been used to handle these problems. In this paper, we give\nan overview of the use of MTL in NLP tasks. We first review MTL architectures\nused in NLP tasks and categorize them into four classes, including the parallel\narchitecture, hierarchical architecture, modular architecture, and generative\nadversarial architecture. Then we present optimization techniques on loss\nconstruction, data sampling, and task scheduling to properly train a multi-task\nmodel. After presenting applications of MTL in a variety of NLP tasks, we\nintroduce some benchmark datasets. Finally, we make a conclusion and discuss\nseveral possible research directions in this field.\n"],"author":[{"name":["Shijie Chen"]},{"name":["Yu Zhang"]},{"name":["Qiang Yang"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.09138v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.09138v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2109.12575v1"],"updated":["2021-09-26T11:55:23Z"],"published":["2021-09-26T11:55:23Z"],"title":["Paradigm Shift in Natural Language Processing"],"summary":["  In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.\n"],"author":[{"name":["Tianxiang Sun"]},{"name":["Xiangyang Liu"]},{"name":["Xipeng Qiu"]},{"name":["Xuanjing Huang"]}],"arxiv:comment":[{"_":"https://txsun1997.github.io/nlp-paradigm-shift","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2109.12575v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2109.12575v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2110.06733v1"],"updated":["2021-10-13T14:03:07Z"],"published":["2021-10-13T14:03:07Z"],"title":["Systematic Inequalities in Language Technology Performance across the\n  World's Languages"],"summary":["  Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.\n"],"author":[{"name":["Damián Blasi"]},{"name":["Antonios Anastasopoulos"]},{"name":["Graham Neubig"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2110.06733v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2110.06733v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2110.10470v2"],"updated":["2021-10-25T04:43:03Z"],"published":["2021-10-20T10:17:04Z"],"title":["Interpreting Deep Learning Models in Natural Language Processing: A\n  Review"],"summary":["  Neural network models have achieved state-of-the-art performances in a wide\nrange of natural language processing (NLP) tasks. However, a long-standing\ncriticism against neural network models is the lack of interpretability, which\nnot only reduces the reliability of neural NLP systems but also limits the\nscope of their applications in areas where interpretability is essential (e.g.,\nhealth care applications). In response, the increasing interest in interpreting\nneural NLP models has spurred a diverse array of interpretation methods over\nrecent years. In this survey, we provide a comprehensive review of various\ninterpretation methods for neural models in NLP. We first stretch out a\nhigh-level taxonomy for interpretation methods in NLP, i.e., training-based\napproaches, test-based approaches, and hybrid approaches. Next, we describe\nsub-categories in each category in detail, e.g., influence-function based\nmethods, KNN-based methods, attention-based models, saliency-based methods,\nperturbation-based methods, etc. We point out deficiencies of current methods\nand suggest some avenues for future research.\n"],"author":[{"name":["Xiaofei Sun"]},{"name":["Diyi Yang"]},{"name":["Xiaoya Li"]},{"name":["Tianwei Zhang"]},{"name":["Yuxian Meng"]},{"name":["Han Qiu"]},{"name":["Guoyin Wang"]},{"name":["Eduard Hovy"]},{"name":["Jiwei Li"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2110.10470v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2110.10470v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.08159v1"],"updated":["2021-12-15T14:31:32Z"],"published":["2021-12-15T14:31:32Z"],"title":["One size does not fit all: Investigating strategies for\n  differentially-private learning across NLP tasks"],"summary":["  Preserving privacy in training modern NLP models comes at a cost. We know\nthat stricter privacy guarantees in differentially-private stochastic gradient\ndescent (DP-SGD) generally degrade model performance. However, previous\nresearch on the efficiency of DP-SGD in NLP is inconclusive or even\ncounter-intuitive. In this short paper, we provide a thorough analysis of\ndifferent privacy preserving strategies on seven downstream datasets in five\ndifferent `typical' NLP tasks with varying complexity using modern neural\nmodels. We show that unlike standard non-private approaches to solving NLP\ntasks, where bigger is usually better, privacy-preserving strategies do not\nexhibit a winning pattern, and each task and privacy regime requires a special\ntreatment to achieve adequate performance.\n"],"author":[{"name":["Manuel Senge"]},{"name":["Timour Igamberdiev"]},{"name":["Ivan Habernal"]}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.08159v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.08159v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2112.08313v2"],"updated":["2022-05-09T21:36:47Z"],"published":["2021-12-15T18:02:04Z"],"title":["Measure and Improve Robustness in NLP Models: A Survey"],"summary":["  As NLP models achieved state-of-the-art performances over benchmarks and\ngained wide applications, it has been increasingly important to ensure the safe\ndeployment of these models in the real world, e.g., making sure the models are\nrobust against unseen or challenging scenarios. Despite robustness being an\nincreasingly studied topic, it has been separately explored in applications\nlike vision and NLP, with various definitions, evaluation and mitigation\nstrategies in multiple lines of research. In this paper, we aim to provide a\nunifying survey of how to define, measure and improve robustness in NLP. We\nfirst connect multiple definitions of robustness, then unify various lines of\nwork on identifying robustness failures and evaluating models' robustness.\nCorrespondingly, we present mitigation strategies that are data-driven,\nmodel-driven, and inductive-prior-based, with a more systematic view of how to\neffectively improve robustness in NLP models. Finally, we conclude by outlining\nopen challenges and future directions to motivate further research in this\narea.\n"],"author":[{"name":["Xuezhi Wang"]},{"name":["Haohan Wang"]},{"name":["Diyi Yang"]}],"arxiv:comment":[{"_":"Accepted by NAACL 2022 main conference (Long paper). Camera-ready\n  version","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2112.08313v2","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2112.08313v2","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]},{"id":["http://arxiv.org/abs/2201.00768v1"],"updated":["2022-01-03T17:17:11Z"],"published":["2022-01-03T17:17:11Z"],"title":["Robust Natural Language Processing: Recent Advances, Challenges, and\n  Future Directions"],"summary":["  Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.\n"],"author":[{"name":["Marwan Omar"]},{"name":["Soohyeon Choi"]},{"name":["DaeHun Nyang"]},{"name":["David Mohaisen"]}],"arxiv:comment":[{"_":"Survey; 2 figures, 4 tables","$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom"}}],"link":[{"$":{"href":"http://arxiv.org/abs/2201.00768v1","rel":"alternate","type":"text/html"}},{"$":{"title":"pdf","href":"http://arxiv.org/pdf/2201.00768v1","rel":"related","type":"application/pdf"}}],"arxiv:primary_category":[{"$":{"xmlns:arxiv":"http://arxiv.org/schemas/atom","term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}}],"category":[{"$":{"term":"cs.CL","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.AI","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.CR","scheme":"http://arxiv.org/schemas/atom"}},{"$":{"term":"cs.LG","scheme":"http://arxiv.org/schemas/atom"}}]}]